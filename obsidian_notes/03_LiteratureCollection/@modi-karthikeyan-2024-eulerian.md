---
title: "Eulerian at BioLaySumm: Preprocessing Over Abstract is All You Need"
authors: Satyam Modi, T Karthikeyan
year: 2024
database: ACL Anthology
citekey: modi-karthikeyan-2024-eulerian
tags:
  - BioLaySumm/2024
  - PLOS
  - eLife
  - ROUGE
  - BERTScore
  - FKGL
  - DCRS
  - CLI
  - LENS
  - AlignScore
  - SummaC
url: https://aclanthology.org/2024.bionlp-1.77/
file: "[[Eulerian at BioLaySumm - Preprocessing Over Abstract is All You Need.pdf]]"
---

>[!title]
Eulerian at BioLaySumm: Preprocessing Over Abstract is All You Need

>[!year]
2024

>[!author]
Satyam Modi, T Karthikeyan


------------------------------------

### Summary
1. 

------------------------------------

### Research question

How can large language models (LLMs) be adapted and fine-tuned to generate accurate, relevant, and readable lay summaries of biomedical research articles for non-expert readers?

------------------------------------

### Methodology
1. 

------------------------------------

### Findings
1. 

------------------------------------

### Discussion
1. 

------------------------------------

### Remarks & Limitations
1. 

------------------------------------

### Citation

```
@inproceedings{modi-karthikeyan-2024-eulerian,
    title = "Eulerian at {B}io{L}ay{S}umm: Preprocessing Over Abstract is All You Need",
    author = "Modi, Satyam  and
      Karthikeyan, T",
    editor = "Demner-Fushman, Dina  and
      Ananiadou, Sophia  and
      Miwa, Makoto  and
      Roberts, Kirk  and
      Tsujii, Junichi",
    booktitle = "Proceedings of the 23rd Workshop on Biomedical Natural Language Processing",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.bionlp-1.77",
    pages = "826--830",
    abstract = "In this paper, we present our approach to the BioLaySumm 2024 Shared Task on Lay Sum- marization of Biomedical Research Articles at BioNLP workshop 2024. The task aims to generate lay summaries from the abstract and main texts of biomedical research articles, making them understandable to lay audiences. We used some preprocessing techniques and finetuned FLAN-T5 models for the summarization task. Our method achieved an AlignScore of 0.9914 and a SummaC metric score of 0.944.",
}
```