---
title: Controllable Sentence Simplification with a Unified Text-to-Text Transfer Transformer
authors: Kim Cheng Sheang, Horacio Saggion
year: 2021
database: ACL Anthology
citekey: sheang-saggion-2021-controllable
tags:
  - text-simplification
  - language-complexity
  - readability
url: https://aclanthology.org/2021.inlg-1.38/
file: "[[Controllable Sentence Simplification with a Unified Text-to-Text Transfer Transformer.pdf]]"
---

>[!title]
Controllable Sentence Simplification with a Unified Text-to-Text Transfer Transformer

>[!year]
2021

>[!author]
Kim Cheng Sheang, Horacio Saggion


------------------------------------

### Summary


------------------------------------

### Research question


------------------------------------

### Context


------------------------------------

### Methodology


------------------------------------

### Findings


------------------------------------

### Discussion


------------------------------------

### Remarks & Limitations


------------------------------------

### Citation

```
@inproceedings{sheang-saggion-2021-controllable,
    title = "Controllable Sentence Simplification with a Unified Text-to-Text Transfer Transformer",
    author = "Sheang, Kim Cheng  and
      Saggion, Horacio",
    editor = "Belz, Anya  and
      Fan, Angela  and
      Reiter, Ehud  and
      Sripada, Yaji",
    booktitle = "Proceedings of the 14th International Conference on Natural Language Generation",
    month = aug,
    year = "2021",
    address = "Aberdeen, Scotland, UK",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.inlg-1.38/",
    doi = "10.18653/v1/2021.inlg-1.38",
    pages = "341--352",
    abstract = "Recently, a large pre-trained language model called T5 (A Unified Text-to-Text Transfer Transformer) has achieved state-of-the-art performance in many NLP tasks. However, no study has been found using this pre-trained model on Text Simplification. Therefore in this paper, we explore the use of T5 fine-tuning on Text Simplification combining with a controllable mechanism to regulate the system outputs that can help generate adapted text for different target audiences. Our experiments show that our model achieves remarkable results with gains of between +0.69 and +1.41 over the current state-of-the-art (BART+ACCESS). We argue that using a pre-trained model such as T5, trained on several tasks with large amounts of data, can help improve Text Simplification."
}
```