{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma',\n",
       " 'olivia',\n",
       " 'ava',\n",
       " 'isabella',\n",
       " 'sophia',\n",
       " 'charlotte',\n",
       " 'mia',\n",
       " 'amelia',\n",
       " 'harper',\n",
       " 'evelyn']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()\n",
    "\n",
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = sorted(list(set(''.join(words))))\n",
    "\n",
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the special character that will be used to pad the words\n",
    "SPECIAL = '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 0,\n",
       " 'b': 1,\n",
       " 'c': 2,\n",
       " 'd': 3,\n",
       " 'e': 4,\n",
       " 'f': 5,\n",
       " 'g': 6,\n",
       " 'h': 7,\n",
       " 'i': 8,\n",
       " 'j': 9,\n",
       " 'k': 10,\n",
       " 'l': 11,\n",
       " 'm': 12,\n",
       " 'n': 13,\n",
       " 'o': 14,\n",
       " 'p': 15,\n",
       " 'q': 16,\n",
       " 'r': 17,\n",
       " 's': 18,\n",
       " 't': 19,\n",
       " 'u': 20,\n",
       " 'v': 21,\n",
       " 'w': 22,\n",
       " 'x': 23,\n",
       " 'y': 24,\n",
       " 'z': 25,\n",
       " '.': 26}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the vocabulary of characters and the mapping from character to index\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "stoi[SPECIAL] = N\n",
    "\n",
    "stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'a',\n",
       " 1: 'b',\n",
       " 2: 'c',\n",
       " 3: 'd',\n",
       " 4: 'e',\n",
       " 5: 'f',\n",
       " 6: 'g',\n",
       " 7: 'h',\n",
       " 8: 'i',\n",
       " 9: 'j',\n",
       " 10: 'k',\n",
       " 11: 'l',\n",
       " 12: 'm',\n",
       " 13: 'n',\n",
       " 14: 'o',\n",
       " 15: 'p',\n",
       " 16: 'q',\n",
       " 17: 'r',\n",
       " 18: 's',\n",
       " 19: 't',\n",
       " 20: 'u',\n",
       " 21: 'v',\n",
       " 22: 'w',\n",
       " 23: 'x',\n",
       " 24: 'y',\n",
       " 25: 'z',\n",
       " 26: '.'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the inverse mapping from index to character\n",
    "itos = {i: ch for ch, i in stoi.items()}\n",
    "\n",
    "itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the context window size for the model\n",
    "# This time we are not using bigrams like in the previous notebook\n",
    "# We are using a context window of 3 characters to predict the next character\n",
    "# This is what we call a higher-order Markov model (that we learned in TAI 2024)\n",
    "K = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... -> e\n",
      "..e -> m\n",
      ".em -> m\n",
      "emm -> a\n",
      "mma -> .\n",
      "... -> o\n",
      "..o -> l\n",
      ".ol -> i\n",
      "oli -> v\n",
      "liv -> i\n",
      "ivi -> a\n",
      "via -> .\n",
      "... -> a\n",
      "..a -> v\n",
      ".av -> a\n",
      "ava -> .\n",
      "... -> i\n",
      "..i -> s\n",
      ".is -> a\n",
      "isa -> b\n",
      "sab -> e\n",
      "abe -> l\n",
      "bel -> l\n",
      "ell -> a\n",
      "lla -> .\n",
      "... -> s\n",
      "..s -> o\n",
      ".so -> p\n",
      "sop -> h\n",
      "oph -> i\n",
      "phi -> a\n",
      "hia -> .\n"
     ]
    }
   ],
   "source": [
    "# Build the dataset\n",
    "\n",
    "def create_dataset(words, stoi, K=3):\n",
    "    xs, ys = [], []\n",
    "    \n",
    "    # Loop through each word in the dataset\n",
    "    for word in words:\n",
    "        # Add special characters to the beginning and end of the word\n",
    "        word = SPECIAL * K + word + SPECIAL\n",
    "\n",
    "        # Get the length of the word\n",
    "        L = len(word)\n",
    "\n",
    "        # Slide a window of size K over the word\n",
    "        for i in range(L - K):\n",
    "            # Get the input and output characters\n",
    "            x = word[i:i + K]\n",
    "            y = word[i + K]\n",
    "\n",
    "            # Print the input and output characters\n",
    "            print(''.join(x), '->', y)\n",
    "\n",
    "            # Convert the characters to indices and add them to the dataset\n",
    "            xs.append([stoi[ch] for ch in x])\n",
    "            ys.append(stoi[y])\n",
    "            \n",
    "    return torch.tensor(xs), torch.tensor(ys)\n",
    "\n",
    "xs, ys = create_dataset(words[:5], stoi, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[26, 26, 26],\n",
       "         [26, 26,  4],\n",
       "         [26,  4, 12],\n",
       "         [ 4, 12, 12],\n",
       "         [12, 12,  0],\n",
       "         [26, 26, 26],\n",
       "         [26, 26, 14],\n",
       "         [26, 14, 11],\n",
       "         [14, 11,  8],\n",
       "         [11,  8, 21],\n",
       "         [ 8, 21,  8],\n",
       "         [21,  8,  0],\n",
       "         [26, 26, 26],\n",
       "         [26, 26,  0],\n",
       "         [26,  0, 21],\n",
       "         [ 0, 21,  0],\n",
       "         [26, 26, 26],\n",
       "         [26, 26,  8],\n",
       "         [26,  8, 18],\n",
       "         [ 8, 18,  0],\n",
       "         [18,  0,  1],\n",
       "         [ 0,  1,  4],\n",
       "         [ 1,  4, 11],\n",
       "         [ 4, 11, 11],\n",
       "         [11, 11,  0],\n",
       "         [26, 26, 26],\n",
       "         [26, 26, 18],\n",
       "         [26, 18, 14],\n",
       "         [18, 14, 15],\n",
       "         [14, 15,  7],\n",
       "         [15,  7,  8],\n",
       "         [ 7,  8,  0]]),\n",
       " tensor([ 4, 12, 12,  0, 26, 14, 11,  8, 21,  8,  0, 26,  0, 21,  0, 26,  8, 18,\n",
       "          0,  1,  4, 11, 11,  0, 26, 18, 14, 15,  7,  8,  0, 26]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3]), torch.int64, torch.Size([32]), torch.int64)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs.shape, xs.dtype, ys.shape, ys.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7570,  2.7416],\n",
       "        [ 0.1253, -1.3804],\n",
       "        [-0.0276,  1.8062],\n",
       "        [ 1.1212, -0.5921],\n",
       "        [ 0.6326, -1.1594],\n",
       "        [ 0.0355, -0.6052],\n",
       "        [-2.8315,  0.0387],\n",
       "        [-0.2734, -0.2890],\n",
       "        [ 0.3583, -1.1933],\n",
       "        [-0.4516, -2.1629],\n",
       "        [-0.2730, -0.3183],\n",
       "        [-0.4963, -1.1389],\n",
       "        [ 1.1615,  0.2430],\n",
       "        [-0.0942, -1.2870],\n",
       "        [-0.9104, -0.9098],\n",
       "        [-0.9621,  0.1775],\n",
       "        [-1.7263, -2.1575],\n",
       "        [-0.8047, -0.6147],\n",
       "        [ 1.2772, -0.5029],\n",
       "        [ 1.1255,  0.1676],\n",
       "        [ 0.3237, -0.4461],\n",
       "        [-1.0830, -0.2872],\n",
       "        [-0.1616, -1.6518],\n",
       "        [-0.5570, -0.4926],\n",
       "        [ 1.6354,  1.1351],\n",
       "        [-0.7456, -1.4554],\n",
       "        [-0.6175,  2.6224]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now, we will create a neural network model that takes the indices of the characters in the context window as input and predicts the index of the next character.\n",
    "# First, we will build the embedding lookup table that maps the character indices to their embeddings.\n",
    "# So, we have N + 1 characters (including the special character) and we will map them to an embedding of size D.\n",
    "# Since we have a few number of characters, we will use a small embedding size of D = 2 for now.\n",
    "\n",
    "# Define the number of characters and the embedding size\n",
    "D = 2\n",
    "\n",
    "# Create the embedding lookup table with N + 1 characters and D dimensions and initialize it randomly\n",
    "C = torch.randn((N + 1, D))\n",
    "\n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.7570,  2.7416])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now, before we embed all the input characters using the embedding lookup table, let's first try to embed a single individual integer to \n",
    "# get a sense of how the embedding lookup table works.\n",
    "\n",
    "# One way to do this is just take the lookup table and index it with the integer value.\n",
    "# For example, if we want to embed the character 'a' which has an index of 0, we can just index the lookup table with 0 to get the embedding of 'a'.\n",
    "C[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The other way presented in the previous notebook is to use one-hot encoding to convert the integer to a one-hot vector and \n",
    "# then perform a matrix multiplication with the embedding lookup table.\n",
    "\n",
    "# Note that we need to convert the integer to a tensor before performing the one-hot encoding, because the one_hot function only works with tensors.\n",
    "# We also need to convert the one-hot vector to a float tensor before performing the matrix multiplication, because PyTorch is dumb and doesn't do this automatically.\n",
    "# This will just give us the same embedding as before.\n",
    "# It plucks out the row corresponding to the index of the character in the embedding lookup table.\n",
    "# This tells us that we can interpret the embedding of an integer as the integer indexing into the embedding lookup table, but equivalently, \n",
    "# we can interpret it as a first layer of the neural network, with no non-linearity, that is just a linear layer with the weight matrix \n",
    "# being the embedding lookup table.\n",
    "# We are just going to index because it is much faster and so we will discard that interpretation of one-hot inputs into neural networks.\n",
    "F.one_hot(torch.tensor([0]), N + 1).float() @ C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7570,  2.7416],\n",
       "        [-0.7570,  2.7416],\n",
       "        [ 0.1253, -1.3804],\n",
       "        [ 0.1253, -1.3804]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now, embedding a single integer is easy enough, we can just ask the embedding lookup table for the embedding of that integer.\n",
    "# But, how do we simultaneously embed all of the k-order contexts present in the dataset?\n",
    "# Luckily, PyTorch is quite flexible and not only allows us to index the embedding lookup table with a single integer, but also with list or tensor of integers.\n",
    "# We can even index the same row multiple times.\n",
    "C[torch.tensor([0, 0, 1, 1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "makemore",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
