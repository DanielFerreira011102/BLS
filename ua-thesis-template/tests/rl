Reinforcement learning (RL) is another approach to training language models for controlled text generation. 
RL-based methods formulate the generation process as a sequential decision-making problem, where the model learns to take actions that maximize a reward function aligned with the desired control attributes \cite{ziegler2019finetuninglanguage, zhang-zhang-2022-quasicontrollable}.
In the RL framework, the language model is treated as an agent that interacts with an environment (the generated text) by taking actions (generating tokens) based on a policy $\pi$. The policy is parameterized by the model's weights $\theta$ and maps states (input context and previously generated tokens) to a probability distribution over actions (next token to generate). 
The agent receives a reward $r$ based on how well the generated text aligns with the desired control attributes.
The objective of RL-based methods is to learn an optimal policy $\pi^*$ that maximizes the expected cumulative reward:
\begin{equation}
\label{eq:rl-objective}
\pi^* = \argmax_\pi \mathbb{E}{\tau \sim \pi} \left[ \sum{t=1}^T r(s_t, a_t) \right]
\end{equation}
where $\tau$ represents a trajectory (sequence of states and actions) sampled from the policy $\pi$, $s_t$ and $a_t$ are the state and action at time step $t$, respectively, and $T$ is the total number of time steps.

Ziegler et al. \cite{ziegler2019finetuninglanguage} proposed a reinforcement learning approach to fine-tune language models using human preferences as rewards. The authors collected a dataset of human preferences between pairs of generated summaries and used this data to train a reward model that predicts the human preference score given a generated summary. The language model is then fine-tuned using the learned reward model to generate summaries that align with human preferences.

Zhang and Zhang \cite{zhang-zhang-2022-quasicontrollable} introduced a quasi-controllable language generation method based on reinforcement learning. Their approach uses a discriminator to provide rewards based on the alignment between the generated text and the desired control attributes. The language model is trained using the Proximal Policy Optimization (PPO) algorithm to maximize the expected reward while maintaining the fluency and diversity of the generated text.

RL-based methods offer a flexible framework for integrating various control attributes into the generation process. However, these methods can be computationally expensive and require careful design of the reward function to ensure that the generated text aligns with the desired attributes while maintaining language quality. In the context of medical question answering, defining a suitable reward function that captures the nuances of language complexity and medical accuracy may be challenging and require domain expertise.

\subsection{Inference-Phase Methods}
Inference-phase methods aim to steer the generation process at inference time without modifying the language model's parameters. 
These approaches focus on manipulating the model's output probabilities or embedding space to align the generated text with the desired attributes. 
Inference-phase methods offer greater flexibility and adaptability compared to training-based methods, as they can be applied to pre-trained language models without the need for attribute-specific fine-tuning.