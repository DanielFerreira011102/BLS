\chapter{Background}
\label{c2}

This chapter presents some basic tips and a few examples on how to use \LaTeX.


\section{Language Models}
\label{c2:s:language-models}



\section{Controlled Text Generation}
\label{c2:s:controlled-text-generation}

\gls{ctg} is a growing area of \gls{nlp} research that focuses on developing techniques to guide language models in producing outputs with specific desired attributes or properties \cite{liang2024controllabletextgenerationlarge, 10.1145/3617680, keskar2019ctrlconditionaltransformerlanguage, dathathri2020plugplaylanguagemodels}.
The goal is to enable fine-grained control over various aspects of the generated text, such as style, sentiment, topic, or complexity level, without compromising the fluency, coherence, and relevance of the output. 
This is in contrast to traditional language generation where the model has little control over the nature of the generated text beyond the input prompt.

Formally, \gls{ctg} can be defined as the task of generating a sequence of tokens $Y = \{y_1, y_2, ..., y_m\}$ that maximizes the conditional probability distribution $P(Y|X, A)$, given an input context $X$ and a set of control attributes $A = \{a_1, a_2, ..., a_k\}$. 
Mathematically, this can be expressed as:

\begin{equation}
    \label{eq:controlled-generation}
    \argmax_Y P(Y | X, A) = \argmax_Y \prod_{i = 1}^{m} P(y_i | y_{<i}, X, A)
\end{equation}

where $y_{<i}$ represents the tokens generated before position $i$. 
The control attributes $A$ can represent any definable text property, and each attribute can be assigned different values based on the specific generation task and desired level of granularity.

The primary challenge in \gls{ctg} lies in effectively integrating the control attributes into the generation process while preserving the quality of the output. 
Current approaches can be broadly categorized into training-phase and inference-phase methods \cite{liang2024controllabletextgenerationlarge, he-etal-2022-ctrlsum}, which will be explored in detail in the following sections.

\subsection{Training-Phase Methods}
Training-phase methods focus on adapting the language model's architecture, training objective, or training data to incorporate the desired control attributes. 
These methods aim to directly encode the control attributes into the model parameters during the training process, which often results in better control and output quality compared to inference-phase methods while requiring no additional computation at runtime.
However, they may require substantial amounts of labeled data and can be less flexible in adapting to new control attributes.

\subsubsection{Retrain/Refactoring}
Retraining or refactoring methods involve either training a new model from scratch or fundamentally modifying its architecture to better accommodate specific control conditions. 
This approach is typically used when existing pre-trained models fail to meet new, more stringent control requirements or when the desired attributes are too complex to be effectively integrated using other methods.
Retraining ensures that the model intrinsically adapts at both the architectural and parameter levels to generate text that conforms to the desired control attributes, but it can be computationally expensive and time-consuming, especially for large-scale models like GPT-3 or T5.

Keskar et al. \cite{keskar2019ctrlconditionaltransformerlanguage} introduced \gls{ctrl}, a 1.63 billion parameter conditional transformer language model trained on 140GB of text data with over 50 control codes that can be used to condition the generation process on attributes such as the source domain (e.g., Wikipedia, books), website URLs, and even specific Reddit communities (e.g., r/science, r/jokes).
Each text sequence in the training data is prepended with a control code that serves as an explicit metadata tag, allowing the model to learn the association between the text and specific attributes through natural co-occurrence during training.
For instance, a control code like ``Wikipedia'' indicates that the subsequent text is in the style of a Wikipedia article, while ``Reviews Rating: 5.0'' prompts the model to generate a positive product review.
While conceptually simple and effective, training a model like \gls{ctrl} requires massive amounts of data and computational resources. 
Collecting datasets with explicit control codes for every desired attribute is often unfeasible.
CTRL may also struggle with more nuanced control, such as fine-grained sentiment or complexity levels.

Building upon the idea of CTRL, Chan et al. \cite{chan2022coconselfsupervisedapproachcontrolled} developed Content-Conditioner (CoCon), a self-supervised framework for fine-grained controllable text generation. 
CoCon adds a content-conditioning layer to GPT-2, allowing it to generate text that incorporates specific content inputs at the word and phrase level. 
The CoCon layer is trained using a self-reconstruction objective, where the model learns to reconstruct text sequences by conditioning on content extracted from the target sequence itself, rather than relying on external control codes.
Thus, removing the need for large-scale labeled datasets.
CoCon demonstrates strong performance in controlling semantic attributes (e.g., sentiment, topic) and integrating content-level constraints.
At the same time, by keeping the original language model intact and only adding a lightweight content-conditioning module, CoCon largely preserves the diversity and fluency of the pre-trained model.

Zhang et al. \cite{zhang-etal-2020-pointer} proposed PrOgressive INsertion-based TransformER (POINTER), an insertion-based transformer architecture for hard-constrained text generation.
Unlike traditional left-to-right language models, POINTER generates text by progressively inserting new tokens between existing ones. 
This allows for precise control over the inclusion of specified keywords or phrases in the output. 
The model is trained from scratch on a large corpus using an insertion-based objective function. 
During inference, POINTER first inserts the target keywords into the input sequence and then fills in the remaining tokens to produce a coherent output.
POINTER offers a promising solution for generating medical text with strict lexical constraints, such as including specific medical terms or phrases. 
However, the model's reliance on keyword insertion limits its ability to control more abstract attributes like text complexity, which often depends on factors beyond vocabulary choice.

% FAST

\subsubsection{Fine-tuning}
Fine-tuning is another common training-phase method, where a pre-trained language model is further trained on a smaller, task-specific dataset to learn the desired control attributes. 
Fine-tuning makes slight adjustments to the model's parameters to better align with the target task or control conditions, while aiming to preserve the general knowledge learned during pre-training. 
Compared to retraining from scratch, fine-tuning is more computationally efficient as it requires less data and computation.
However, if the dataset is too small or narrow in scope, the model may overfit and lose some of the general knowledge acquired during pre-training.

Zeldes et al. \cite{zeldes2020technicalreportauxiliarytuning} proposed Auxiliary Tuning, where an auxiliary model is trained to augment a frozen pre-trained language model.
The auxiliary model takes the same input as the pre-trained model, along with additional task-specific information or control codes. It produces a set of logits that are added to the logits of the pre-trained model before applying the softmax function to obtain the final output probabilities.
Intuitively, the auxiliary model learns the residual logits needed to shift the probability distribution of the pre-trained model towards the target distribution.
To further improve training efficiency, the lower layers of the pre-trained model can also be used as a feature extractor for the auxiliary model inputs. 
There are no constraints on the auxiliary model architecture, except that it must output logits over the same vocabulary as the original model.
It is typically much smaller than the pre-trained model (e.g., a few Transformer layers) and can be trained on a small amount of data.
In experiments on keyword-conditioned generation, auxiliary tuning achieved similar performance to full fine-tuning, while being more parameter-efficient and avoiding the risk of catastrophic forgetting.

More recently, Shi et al. \cite{shi2024lifilightweightcontrolledtext} introduced Lightweight Fine-grained Control (LiFi). 
Instead of using discrete, categorical control codes, LiFi uses continuous, relative control signals that come from an attribute classifier. 
For example, instead of simply labeling a movie review as positive or negative, LiFi captures the degree of positivity or negativity on a continuous scale.
The attribute classifier is initially trained on a small labeled dataset and later fine-tuned with unlabeled data to improve its performance. 
It outputs continuous control vectors that represent attribute strengths. 
These control vectors guide the generation process through lightweight neural modules called adapters, which are integrated into each layer of the pre-trained language model. 
The adapters use a bottleneck architecture with a reduction factor of 16 for feedforward modules and 4 for multi-head attention, balancing control strength and computational efficiency.
A major advantage of LiFi is that it only needs to train these small adapter modules, leaving the base pre-trained model frozen. 
This significantly reduces computational demands, as the adapters constitute only about 0.04\% of the model's parameters. 
During generation, LiFi combines multiple adapters using a position-specific fusion mechanism, enabling smooth transitions between attributes and precise control over the generated content.
% For instance, a generated movie review could have a specific level of positivity while maintaining a particular writing style.
Shi et al. evaluated LiFi on sentiment control and topic control, alongside a newly proposed stylistic novel-writing task. 
They used GPT-2 Medium as the base model and a variety of automated and human evaluation metrics to assess control strength, fluency, and diversity. 
LiFi achieved superior performance in control tasks, notably excelling in polarity reversal scenarios (e.g., positive to negative sentiment) and maintaining fluency.

% FLAN
% InstructCTG

\subsubsection{Reinforcement Learning}
\gls{rl} offers a more flexible and dynamic approach to \gls{ctg}, where the language model learns to optimize its output based on feedback or reward signals that indicate the quality and alignment of the generated text with the desired attributes. 
It is particularly well-suited for scenarios where the control attributes are complex, subjective, or difficult to explicitly define, such as generating text with specific styles, tones, or linguistic properties. 
\gls{rl}-based methods formulate the generation process as a sequential decision-making problem, where the model learns to take actions (i.e., generate tokens) that maximize the expected reward, using algorithms such as REINFORCE \cite{10.1007/BF00992696}, Q-Learning \cite{Watkins1992-pa}, 
%Advantage Actor-Critic (A2C) \cite{mnih2016asynchronousmethodsdeepreinforcement}, 
and Proximal Policy Optimization (PPO) \cite{schulman2017proximalpolicyoptimizationalgorithms}.

Stiennon et al. \Cite{stiennon2022learningsummarizehumanfeedback} showed how \gls{rlhf} could be used to train language models to generate better summaries.
Their method consisted of three main steps: collecting human feedback, training a reward model, and optimizing a policy using reinforcement learning.
The process began by collecting a large dataset of human preferences between pairs of summaries. 
Human evaluators compared two summaries and indicated which one better represented the original text. To ensure high-quality feedback, the researchers implemented careful quality control procedures and maintained close communication with evaluators through a dedicated website and chat system.
Using this human preference data, they trained a reward model to predict which summary humans would prefer. 
The reward model builds on a supervised baseline and adds a scalar output head that predicts the log odds of one summary being preferred over another.
The model was trained using a cross-entropy loss function and was normalized so that reference summaries achieved a mean score of 0.
They then used reinforcement learning, specifically the PPO algorithm, to fine-tune a policy (language model) to maximize the reward predicted by the reward model. 
To prevent the policy from diverging too far from the initial supervised model, they included a KL penalty term in the reward function. 
Importantly, they used separate networks for the policy and value function to prevent value function updates from degrading the pretrained policy early in training.
The results showed that the 1.3B parameter model trained with human feedback outperformed standard supervised models up to 10 times larger, and their 6.7B parameter model produced summaries that humans preferred over the original reference summaries. 
The method also demonstrated strong transfer learning capabilities, performing well on news article summarization without specific training on news data.

Building on Stiennon's work, Dai et al. \cite{dai2023saferlhfsafereinforcement} proposed Safe \gls{rlhf} to address a fundamental tension in language model alignment: balancing helpfulness with harmlessness. 
Traditional \gls{rlhf} approaches often struggle when these objectives conflict, as a model being maximally helpful (e.g., providing detailed information about dangerous topics) might compromise safety.
Safe \gls{rlhf}'s key innovation is explicitly decoupling these two objectives during both data collection and training. 
During annotation, crowdworkers separately evaluate responses for helpfulness and harmlessness, rather than providing a single preference score. 
This separation helps avoid annotator confusion and produces cleaner training signals. 
For harmlessness evaluation, responses are additionally labeled across 14 predefined harm categories, with a response considered ``safe'' only if it poses no risks across all categories.
The method trains two separate preference models: a Reward Model (RM) for helpfulness and a Cost Model (CM) for harmlessness. 
The CM combines binary safety classification with preference learning, allowing it to both rank responses by relative safety and identify harmful content. 
The training process formulates safety alignment as a constrained optimization problem, where the objective is to maximize the expected reward (helpfulness) while keeping the expected cost (potential harm) below a threshold. 
This is solved using the Lagrangian method, which introduces a multiplier $\lambda$ that dynamically adjusts the balance between objectives. 
When the model becomes less safe, $\lambda$ increases to prioritize harmlessness, and when safety improves, $\lambda$ decreases to focus more on helpfulness.
Through three rounds of iterative training, each incorporating red-team testing to identify vulnerabilities, Safe \gls{rlhf} demonstrated superior results compared to standard \gls{rlhf} and reward shaping approaches. 
Using Alpaca-7B as the base model, they achieved significant improvements in both helpfulness and harmlessness according to human evaluations, reducing harmful outputs from 53.08\% to 2.45\% while maintaining or improving helpful behavior.

Phatak et al. \cite{info:doi/10.2196/38095} developed TESLEA to address the challenge of making complex medical literature more accessible to a broader audience. 
Their method focuses specifically on medical text simplification, where the goal is to generate simplified versions of complex medical paragraphs while preserving the original meaning. 
To achieve this, the authors first fine-tuned a BART-based language model using the standard maximum likelihood estimation (MLE) on a dataset from the Cochrane Database of Scientific Reviews, which contains paired complex medical paragraphs and their simplified versions written by medical professionals.
This adaptation step helps the model learn the general characteristics of the simplification task and the medical domain, including terminology and writing patterns. 
The fine-tuned model then undergoes further training using reinforcement learning guided by three custom rewards designed to measure different aspects of text simplification quality: a relevance reward ($R_{cosine}$) that measures semantic similarity using BioSentVec embeddings \cite{Chen_2019}, a \gls{fkgl} reward ($R_{Flesch}$) that optimizes readability, and a lexical simplicity reward ($R_{lexical}$) based on word frequency distributions following Zipf's law. 
In each \gls{rl} step, the model generates two sets of simplified outputs, one using greedy decoding and another using multinomial sampling. 
The rewards are computed for both sets of outputs, and the \gls{rl} loss is calculated using the Self-Critical Sequence Training (SCST) algorithm \cite{8099614}, which uses the greedily decoded outputs as baselines to stabilize learning and reduce variance.
To prevent the model from deviating too far from the initial fine-tuned parameters, the final loss is a weighted sum of the RL loss and the MLE loss.
TESLEA demonstrated significant improvements in readability, reducing the FKGL by approximately 2.5 points compared to the original text, while maintaining comparable ROUGE and SARI scores with existing baselines. Human evaluations showed strong agreement (>70\%) among domain experts on the quality of generated simplifications across multiple dimensions including fluency, coherence, and factual accuracy.
The outputs from TESLEA were consistently shorter than those produced by baseline models and the input paragraphs themselves, suggesting it learned to remove unnecessary technical details while retaining the most relevant content.

% Efficient RL for Unsupervised Controlled Text Generation
% Token-level Direct Preference Optimization
% Quark

\subsection{Inference-Phase Methods}
Inference-phase methods aim to steer the generation process at inference time without changing the language model's parameters.
They dynamically manipulate the model's output probabilities or embedding space in real-time to align the generated text with the desired attributes or control conditions.
Compared to training-phase approaches, inference-phase methods are more flexible, avoiding the need for retraining or fine-tuning the model.
This makes them an alternative or complementary solution, especially for applications requiring on-the-fly output adjustments.

\subsubsection{Prompt Engineering}
Prompt engineering refers to the practice of designing input prompts that guide the language model to generate text with specific attributes or properties. 
The idea is to provide the model with explicit instructions or cues that signal the desired control conditions, such as keywords, phrases, or templates, which the model can then use to adjust its output accordingly.
Prompt engineering is an effective and easy way to control the output of language models without having to change the model architecture or training data.
The primary challenge lies in designing informative yet unambiguous prompts that the model can accurately interpret and follow.
To find the right prompts, researchers often rely on human intuition, trial and error, and, more recently, automated methods.

% Prompt engineering uses either hard prompts (natural language instructions) or soft prompts (trainable vectors) to guide the model's outputs.

AutoPrompt \cite{shin-etal-2020-autoprompt} provides an automated way to find effective discrete prompts for controlling language models. 
Instead of manually writing prompts, which can be time-consuming and error-prone, AutoPrompt uses a gradient-based algorithm to automatically discover effective trigger tokens. 
The method starts with a template containing [MASK] tokens and iteratively replaces them with words from the vocabulary by computing which tokens would maximize the probability of desired outputs. 
To prevent selecting irrelevant tokens, AutoPrompt filters candidates using token statistics and limits the search to a subset of the vocabulary. 
While originally developed for probing tasks like sentiment analysis, AutoPrompt showed that automatically discovered discrete prompts could effectively control language model behavior without modifying the model itself.

Ramirez et al. \cite{ramirez-etal-2023-controllable} tackled the challenge of controlling dialogue generation through Dialogue Acts (DAs).
When generating conversational responses, we often want to control not just what is said, but how it is said, whether it's a question, a statement, or a suggestion. 
However, traditional methods struggle to maintain both the desired dialogue style and semantic accuracy when working with limited examples. 
Their solution generates multiple candidate responses for a given input using few-shot prompting, and uses a set of six carefully designed ranking functions to select the response that best matches both the intended dialogue act and semantic content. 
For instance, when aiming to generate a question about a specific topic, their method ensures the response is both properly formed as a question and contains accurate information about the topic.

Zhang et al. \cite{zhang-etal-2023-pcfg} proposed Probabilistic Context-Free Grammar (PCFG) to address an important limitation in \gls{ctg}, specifically the inability to handle new types of controls that weren't seen during training.
Most systems can only handle a fixed set of control options (e.g., positive or negative sentiment), but PCFG uses a flexible grammar system to understand and process new control instructions. 
The grammar can combine basic elements in various ways to create diverse natural language commands. 
For example, from basic elements about style and topic, it can understand new combinations like ``write a formal email about climate change'' even if that exact combination wasn't seen before. 
The system has three steps: it creates a basic template, fills in the specific control details, and ensures the final command is grammatically correct.

For soft prompts, Prefix-Tuning \cite{li-liang-2021-prefix} adds trainable vectors called ``prefixes'' at every layer of a language model while keeping the model's original parameters frozen. 
For each layer, the prefix vectors are added before the input sequence, allowing them to influence how that layer processes the text. 
To improve training stability, the prefixes are generated through a small feedforward neural network rather than being optimized directly. 
When tested on table-to-text generation and summarization tasks, Prefix-Tuning achieved similar performance to fine-tuning the entire model while using only 0.1\%-2\% of the parameters. 
The method worked particularly well with limited training data and showed better ability to generalize to new topics compared to standard fine-tuning.

Prompt Tuning \cite{lester-etal-2021-power} takes a simpler approach by only adding trainable vectors at the input layer of the model. 
These vectors are prepended to the input embeddings and trained through backpropagation to adapt to different tasks. 
The authors found that with smaller language models, this method performed worse than fine-tuning. 
However, as model size increased, the performance gap disappeared. 
At 10 billion parameters, Prompt Tuning matched fine-tuning performance across multiple tasks. 
This finding is particularly useful for deploying large language models, as it means we can use a single frozen model for many tasks by just storing small sets of trained prompt vectors.

P-tuning \cite{liu-etal-2022-p} improves soft prompting by using a bi-directional LSTM network to convert discrete prompt tokens into trainable embeddings. 
Instead of directly optimizing the embeddings, which can be unstable, P-tuning first processes them through the LSTM network. 
The method combines continuous prompts at both the embedding layer and middle layers of the model. 
P-tuning demonstrated strong performance across various tasks, particularly excelling in few-shot scenarios.
Its ability to handle both classification and generation tasks effectively proved that continuous prompts can be as powerful as discrete prompts, with the added benefit of being more stable and easier to optimize through backpropagation.

% like AutoPrompt \cite{shin-etal-2020-autoprompt} and P-tuning \cite{liu-etal-2022-p}.

\subsubsection{Guided Decoding}
Guided decoding is a technique used during the decoding process of a language model to bias the generated text towards the desired control attributes. 
It works by manipulating the logits or probability distribution of the model to favor tokens that align with the control conditions.
This is often done using classifiers or reward models that provide additional supervision signals to guide the generation process.
Guided decoding approaches are often plug-and-play, meaning they can be easily integrated into existing language models without requiring extensive retraining or fine-tuning.
However, the use of external guiding models can slow down the generation process and often makes the text sound less natural and coherent.

Dathathri et al. (2020) introduced Plug and Play Language Models (PPLM), which enable controlled text generation through gradient updates in a language model's latent space. 
The authors demonstrated that we can achieve control over topic and sentiment by combining GPT-2 with lightweight attribute controllers, either bag-of-words or simple discriminators. 
Since the optimization is done retroactively in the activation space, we wouldn't need to retrain or fine-tune our base model, saving us significant computing resources. 
The method also allows multiple controllers to work together and provides fine-grained control through a strength parameter $\alpha$ that can be tuned to achieve different levels of attribute control.
To keep the text sounding natural, PPLM also includes a KL-divergence term that prevents the hidden states from changing too drastically. 
The authors tested PPLM on several control tasks. It could generate text about specific topics like science or politics, with positive or negative sentiment, or without toxic content. 
While PPLM worked well for controlling these attributes, it is slow because it needs multiple forward and backward passes through both models at each generation step.

GeDi by Krause et al. [65] made guided decoding more efficient by using small class-conditional language models instead of classifiers. 
The authors first took a small language model (GPT-2 Medium) and fine-tuned separate versions of it for each attribute.
For example, one model trained on positive text with a ``<positive>'' control code and another on negative text with a ``<negative>'' code. 
During generation, GeDi uses both models to calculate the probability that each potential next word will lead to text with the desired attribute. 
It does this by comparing how likely the text sequence is under both the positive and negative models using Bayes' rule. 
GeDi then uses these probabilities to adjust how likely each word is to be chosen next by the main language model. 
The method includes a weighting parameter that controls how much these probabilities influence the final word selection. 
By comparing the predictions of models trained on opposite attributes, GeDi can effectively control generation while being much faster than PPLM. 
However, it requires training new conditional models for each attribute you want to control.

DExperts by Liu et al. [85] simplified this idea further. 
Instead of computing classification probabilities, DExperts directly combines the predictions from three models: a base language model, an "expert" model trained on text with the desired attribute (like non-toxic text), and an "anti-expert" model trained on text with the undesired attribute (like toxic text). 
At each step, DExperts adds the expert's prediction scores to the base model's scores while subtracting the anti-expert's scores. 
A control parameter determines how strongly these expert models influence the generation. 
The authors showed this simple approach was very effective at making GPT-2 and GPT-3's text less toxic while staying fluent and natural. 
They also found that small expert models (125M parameters) could successfully guide much larger base models (1.5B parameters). 
Like GeDi, DExperts needs separate expert models for each attribute, but these can be much smaller than the base model.

XXX
PPLM
GeDi
DExperts
Arithmetic
MixAndMatch
LM-Steer
XXX

\subsubsection{Latent Space Manipulation}
Latent space manipulation is a technique for controlling the attributes of machine-generated text by strategically adjusting the hidden representations within a pre-trained language model. 
These hidden representations exist in a high-dimensional space called the latent space, which can be thought of as a compressed representation of the input text, capturing its essential features and characteristics.
Similar words, phrases, and concepts are clustered together in this space, allowing for semantic relationships to be encoded in the form of geometric distances and directions between points.
% continue this text, including the term "guiding vector"
By identifying specific directions or vectors in this latent space, we can steer the model's hidden representations during text generation, effectively pushing the output toward or away from certain attributes.
The magnitude of these guiding vectors determines how strongly the attribute is expressed in the text, allowing for fine-grained and continuous control over the output.
Common approaches to find these guiding vectors include ...

XXX
GENhance
ICV
MIRACLE
XXX

There are also other methods for \gls{ctg} that do not fit into any of the aforementioned categories.

\section{Language Complexity}
\label{c2:s:linguistic-complexity}

% The complexity of biomedical terminology represents a unique challenge for language complexity measurement. 
% Medical terms often combine elements from multiple languages, primarily Greek and Latin, creating systematic patterns that affect both structural and cognitive complexity. 
% These patterns follow predictable rules but can create significant processing challenges for non-expert readers.
% 
% The relationship between professional and lay terminology adds another layer of complexity. 
% Many medical concepts can be expressed through either technical or lay terms (e.g., ``myocardial infarction'' vs. ``heart attack''), creating parallel vocabularies that must be considered in complexity measurement. 

% The title should be either "Statistical Readability Metrics" or "Traditional Readability Metrics"
\subsection{Traditional Readability Metrics}

Readability formulas are designed to estimate the difficulty of a text based on surface-level features such as average sentence length, average word length, and the proportion of difficult or unfamiliar words. While originally developed for general-domain texts, some of these metrics have been applied to biomedical literature to assess the accessibility and suitability of patient education materials, consent forms, and other health-related documents. For example, researchers have used these metrics to evaluate the readability of online patient education materials for conditions such as nocturnal enuresis \cite{Fung2024-uh}, bariatric surgery \cite{Lucy2023-zi}, and female pelvic floor disorders \cite{Varli2023-ma}. Moreover, these metrics have been used to assess the readability of discharge instructions for heart failure patients \cite{Tuan2023-wc} and to analyze the quality of information provided by intensive and critical care societies \cite{Hanci2024-wv}.

The most popular readability metric is the \gls{fkgl}. This formula estimates the U.S. grade level required to understand a given text, calculated as follows:
\begin{equation}
    \label{eq:fkgl}
    FKGL = 0.39 \times \left(\frac{\text{words}}{\text{sentences}}\right) + 11.8 \times \left(\frac{\text{syllables}}{\text{words}}\right) - 15.59
\end{equation}
\gls{fkgl} scores typically vary from 0 to 18, with higher scores indicating more difficult text. For example, a score of 9.2 would suggest that the text is suitable for an average 9th-grade student. Text that scores above 12 suggest college-level or domain-specific expertise.

Another commonly used readability formula is the \gls{smog}. 
This metric estimates the years of education needed to understand a piece of writing based on the number of polysyllabic words (i.e., those with three or more syllables) in a sample of 30 sentences, using the following formula:
\begin{equation}
    \label{eq:smog}
    SMOG = 1.0430 \times \sqrt{\text{polysyllables} \times \left(\frac{30}{\text{sentences}}\right)} + 3.1291
\end{equation}
Like \gls{fkgl}, \gls{smog} scores correspond to U.S. grade levels. 
The \gls{smog} formula has gained popularity in healthcare settings due to its relative ease of use and focus on vocabulary complexity. Its widespread use in healthcare is supported by a 2010 study published in the Journal of the Royal College of Physicians of Edinburgh, which recommended \gls{smog} as the preferred measure for evaluating consumer-oriented healthcare material \cite{Fitzsimmons2010-mq}.

The \gls{dcrs} is another well-known readability metric that assesses text difficulty based on the average sentence length and the percentage of ``difficult'' words not found on a list of 3,000 familiar words. The original \gls{dcrs} formula is:
\begin{equation}
    \label{eq:dcrs}
    DCRS = 0.1579 \times \left(\frac{\text{difficult words}}{\text{total words}} \times 100\right) + 0.0496 \times \left(\frac{\text{total words}}{\text{total sentences}}\right)
\end{equation}
If the percentage of difficult words exceeds 5\%, an additional constant of 3.6365 is added to the raw score to get the final \gls{dcrs}. Scores range from 4.9 or below for easily understood text to 10 or above for very challenging text.

In addition to \gls{fkgl}, \gls{smog}, and \gls{dcrs}, there are many other readability formulas, such as the \gls{ari}, \gls{cli}, \gls{gfi}, and Linsear Write Formula. While the specific calculations differ, they all aim to estimate text difficulty based on factors like word length, sentence length, and syllable counts.
Although traditional readability formulas are still widely used, they have several notable limitations when applied to biomedical texts. 
First, these formulas rely on surface-level features like word and sentence length, which may not adequately capture the conceptual complexity of medical information \cite{Crossley2022, WANG2013503, Singh2024}.
They treat the text as a ``bag of words'', ignoring higher-level discourse structures that influence comprehension, such as information density, organization, coherence, and syntax.  
Second, readability formulas often treat all words equally based on length or syllable count, without considering the cognitive load imposed by specific terms \cite{Swanson2024}. 
Polysyllabic medical terms may be familiar to experts but challenging for lay readers. Conversely, short words like ``apnea'' or ``polyp'' may also be difficult for average adults to understand.
Third, these formulas were developed using general reading materials, often geared towards children's education levels. 
The assumptions and thresholds used in these formulas may not generalize well to the specialized language and adult literacy levels of biomedical texts \cite{Crossley2022}.
Despite these drawbacks, readability formulas provide an objective, quantitative measure of text complexity that can serve as a starting point for improving the clarity and accessibility of health-related materials. 
However, experts generally recommend using these formulas as part of a more holistic approach, combining multiple metrics with expert evaluation and user testing to ensure that health information is truly accessible and understandable for the intended audience \cite{Ko2024-dd, tanprasert-kauchak-2021-flesch}.

\subsection{Reference-Based Evaluation Metrics}

Reference-based evaluation metrics compare machine-generated text simplifications to one or more predefined gold-standard references.
The core idea is to measure how closely the generated text aligns with the references in terms of content, structure, and style.

The most widely adopted metric in the text simplification community might be the \gls{sari} \cite{xu-etal-2016-optimizing}. 
\gls{sari} evaluates the quality of a simplified text by comparing it to multiple reference simplifications and the original complex sentence. 
It computes the arithmetic mean of three sub-scores: 1) the F1 score for n-grams present in the output and the references but not in the input (addition), 2) the F1 score for n-grams in the output and input but not the references (copying), and 3) the F1 score for n-grams in the input and references but not the output (deletion). 
Considering these three operations, \gls{sari} rewards simplifications that add simplified n-grams, preserve important n-grams from the input, and delete unimportant n-grams. 
In the biomedical domain, \gls{sari} has been shown to be a more reliable metric for assessing text quality than other n-gram-based metrics like BLEU and ROUGE \cite{li2024largelanguagemodelsbiomedical}.
However, since it was originally designed to evaluate lexical (word-level) changes, it may not adequately measure how well a system preserves meaning or reduces complexity when evaluating multi-operation simplifications, such as rephrasing sentences or altering sentence structure \cite{alva-manchego-etal-2021-un}.
It also struggles with correctly identifying and rewarding appropriate lexical replacements in such scenarios.

BERTScore \cite{zhang2020bertscoreevaluatingtextgeneration} is another popular metric that uses contextualized word embeddings from the BERT language model to compute semantic similarity between the candidate and reference simplifications. 
It aligns each token in the candidate text with the most similar token in the reference, and then sums these cosine similarities to calculate precision and recall, with the F1 score computed as their harmonic mean. 
BERTScore has demonstrated good performance in various text generation tasks, including machine translation \cite{Vetrov2022ANA, Tang2024ImprovingBF}. 
In the context of text simplification, BERTScore has shown the highest correlations with human judgments of simplicity and meaning preservation compared to other metrics, particularly when evaluating the outputs of neural sequence-to-sequence models \cite{alva-manchego-etal-2021-un}, though it is unclear how well it generalizes to the biomedical domain. 
It does not explicitly measure readability aspects such as lexical or syntactic complexity \cite{li2024largelanguagemodelsbiomedical}. 
Furthermore, BERTScore's greedy alignment approach may allow system outputs to receive excess credit relative to the reference \cite{jin-gildea-2022-rewarding}.

\gls{rouge} \cite{xu-etal-2016-optimizing}, a family of metrics originally developed for text summarization, has also been adapted for text simplification.
\gls{rouge} measures the n-gram overlap between the simplified text and the references, with higher scores indicating greater lexical similarity.
Different \gls{rouge} variants focus on various aspects of n-gram matching:
\begin{itemize}
    \item \gls{rouge}-N calculates the recall based on the ratio of matching n-grams (e.g., unigrams for \gls{rouge}-1, bigrams for \gls{rouge}-2) to the total reference n-grams, but it does not account for the order or contiguity of the matches. 
    \item \gls{rouge}-L addresses this by considering the precision, recall, and F-measure of the largest common subsequence, which preserves order but allows gaps. 
    \item \gls{rouge}-S offers a more flexible matching by measuring the overlap of skip-bigrams, capturing both local and long-range sentence structure similarities. 
    \item \gls{rouge}-W is a weighted version of \gls{rouge}-L that assigns higher importance to consecutive matches, favoring longer common subsequences.
\end{itemize}
These \gls{rouge} variants can capture lexical and structural similarities at different granularities, but they do not explicitly measure semantic similarity, potentially missing differences in meaning between the original and simplified texts, and do not account for readability factors such as vocabulary or syntax, which are crucial for text simplification \cite{xu2024reasoningcomparisonllmenhancedsemantic}.
Ganesan \cite{ganesan2018rouge20updatedimproved} addressed some of these shortcomings in \gls{rouge} 2.0, which improves upon the original metric by incorporating WordNet-based synonym matching and allowing evaluation of specific topics through \gls{pos} based filtering. Similarly, Zhang et al. \cite{ZHANG2024121364} proposed \gls{rouge}-SEM, which combines \gls{rouge} with a Siamese-BERT network to measure semantic similarity and uses back-translation to rewrite texts that are semantically similar but lexically different.

Other reference-based metrics that have been used for text simplification include the \gls{bleu}, which calculates the precision of n-gram matches between the candidate and reference, with a brevity penalty \cite{papineni-etal-2002-bleu}; \gls{meteor}, which computes a weighted F-score based on exact, stem, synonym, and paraphrase matches between the candidate and reference \cite{banerjee-lavie-2005-meteor}; and \gls{ter}, which measures the minimum number of edits (insertions, deletions, substitutions, and shifts) required to transform the candidate text into the reference \cite{snover-etal-2006-study}.
All reference-based metrics require high-quality reference simplifications, which may be difficult and expensive to obtain, especially for large biomedical corpora \cite{alva-manchego-etal-2021-un, 94205144dd7945cc99b5a6544451b668, deutsch-etal-2022-limitations}. 
They also assume that the references represent the only valid simplifications, penalizing alternative paraphrases that may be equally fluent and meaningful \cite{sottana2023evaluationmetricseragpt4, huang-kochmar-2024-referee, lyu2024scigispynovelmetricbiomedical}. 
Furthermore, these metrics do not directly measure the readability or accessibility of the simplified text for the target audience, only its similarity to the references.

\subsection{Reference-Free Evaluation Metrics}

Recently, there has been growing interest in developing reference-free (context-based) evaluation metrics for text simplification that can assess output quality without relying on human-written reference simplifications. 
Reference-free metrics are appealing because collecting high-quality human reference simplifications is time-consuming, expensive, and may not always be feasible.

One such metric is \gls{samsa} \cite{sulem-etal-2018-semantic} that uses the semantic annotations in the \gls{ucca} framework to measure the structural simplicity of a text. 
It stipulates that an optimal simplification should assign each event in the input sentence to its own output sentence. 
The metric then computes the extent to which this is achieved by comparing the semantic parse of the original and simplified texts. 
By focusing on meaning-preserving structural changes, \gls{samsa} addresses some of the limitations of surface-level readability formulas and n-gram based metrics. 
However, it is designed primarily for sentence splitting and does not assess other simplification operations like lexical substitution or paraphrasing.

More recently, several metrics have been proposed that leverage large pre-trained language models to evaluate simplification quality. 
Maddela et al. \cite{maddela-etal-2023-lens} introduced \gls{lens}, a metric trained on human ratings of simplification to predict overall quality. 
It uses the RoBERTa model to encode the original and simplified sentences, and a new component to weigh the similarity of the simplified sentence to different possible references. 
\gls{lens} has shown high correlation with human judgments on both overall and aspect-level simplification quality.
Building on this work, Huang and Kochmar \cite{huang-kochmar-2024-referee} proposed REFeREE, which incorporates a curriculum learning approach with synthetic data to further improve performance.

\gls{siera} \cite{yamanaka-tokunaga-2024-siera} is another reference-free metric specifically designed for sentence simplification. 
It trains a ranking model to determine the relative simplicity between sentence pairs and uses a data augmentation technique to expand the training data with simplification edit operations. 
In experiments, \gls{siera} outperformed reference-based and traditional readability metrics on evaluating sentence simplification.

\gls{salsa} \cite{heineman-etal-2023-dancing} takes a different approach by combining error and quality evaluation into an edit-based reference-free framework. 
It defines 21 edit types covering lexical, syntactic and conceptual simplification operations, and uses human annotation to train a model to predict sentence-level quality from these fine-grained edits. 
\gls{salsa} provides an interpretable way to identify specific strengths and weaknesses of simplification systems. 
The authors also used the \gls{salsa} annotations to train an improved version of \gls{lens} called \gls{lens}-\gls{salsa} that can predict both overall and fine-grained edit-level quality.

Other reference-free metrics include QuestEval \cite{scialom-etal-2021-rethinking}, which uses a question-answering approach to measure information preservation, and \gls{sle} \cite{kriz-etal-2020-simple} which evaluates relative readability improvements using contrastive texts.

Other reference-free approaches include using pre-trained language models to estimate the probability or perplexity of the simplified text \cite{ke-etal-2022-ctrleval}, calculating readability scores based on lexical and syntactic features \cite{Truica2023-au}, and measuring the text's similarity to a corpus of simple or complex language \cite{Truica2023-au}.
However, reference-free metrics also have some limitations. They may not capture all aspects of simplification quality, such as information preservation or target audience appropriateness. They also require careful selection and validation of features or training data to ensure their relevance and generalizability to the biomedical domain.