\chapter{Background}
\label{c2}

This chapter presents the foundational concepts and current research in controlled text generation. We begin with an overview of large language models and their text generation capabilities, followed by a detailed examination of controlled text generation methods. The chapter concludes with an analysis of language complexity measurement approaches, which are essential for evaluating text simplification systems.

\section{Language Models}
\label{c2:s:language-models}

Language models are statistical models that learn to predict the probability distribution of words or tokens in a sequence. Modern neural language models, particularly those based on the Transformer architecture, process text through multiple layers of self-attention mechanisms that capture relationships between words at different positions in the sequence.

The text generation process in these models follows an autoregressive approach, where each token is predicted based on previously generated tokens. Given a sequence of tokens $X = \{x_1, x_2, ..., x_n\}$, the model computes the probability of the next token as:

\begin{equation}
    P(X) = P(x_1, x_2, ..., x_n) = \prod_{i=1}^n p(x_i|x_{<i})
\end{equation}

where $x_{<i}$ represents all tokens before position $i$. This formulation allows the model to generate text by iteratively sampling tokens from the predicted probability distribution.

Pre-trained language models learn these probability distributions through self-supervised training on large text corpora. During pre-training, models can acquire various types of knowledge:

\begin{itemize}
    \item Syntactic patterns and grammatical rules
    \item Semantic relationships between words
    \item Domain-specific terminology and conventions
    \item Common facts and world knowledge
\end{itemize}

However, the standard generation process provides limited control over the attributes of the generated text. The model generates tokens based solely on learned probabilities, making it difficult to ensure specific characteristics in the output, such as maintaining a consistent style, following a particular format, or adhering to domain-specific constraints.

\section{Controlled Text Generation}
\label{c2:s:controlled-text-generation}

Controlled text generation addresses the challenge of guiding language models to produce text with specific desired properties while maintaining fluency and coherence \cite{liang2024controllabletextgenerationlarge, 10.1145/3617680, keskar2019ctrlconditionaltransformerlanguage, dathathri2020plugplaylanguagemodels}. Unlike standard text generation, controlled generation explicitly incorporates additional constraints or conditions into the generation process.

The task involves generating text that satisfies specified control conditions $C$ while preserving the quality of the original language model output. When we incorporate these control conditions into the generation process, the probability distribution becomes:

\begin{equation}
    P(X|C) = P(x_1, x_2, ..., x_n|C) = \prod_{i=1}^n p(x_i|x_{<i}, C)
\end{equation}

Control conditions $C$ can represent any definable text property and take various forms depending on the specific task requirements and application domain.

\subsection{Control Conditions and Types}

Control conditions in text generation serve different purposes based on the application requirements. These conditions broadly fall into three categories: semantic control, structural control, and lexical control.

\subsubsection{Semantic Control}

Semantic control focuses on abstract properties of the generated text:

\begin{itemize}
   \item \textbf{Safety:} Text generation systems must avoid producing harmful, toxic, or biased content. This includes detecting and preventing discriminatory language, hate speech, or misleading information. For example, when generating dialogue responses, the system should avoid suggesting harmful actions or expressing biased views.
   
   \item \textbf{Sentiment:} Applications often need to control the emotional tone of generated text. A customer service chatbot might need to maintain a positive tone, while a news article generator should maintain neutral sentiment. The sentiment control extends beyond simple positive/negative classification to include fine-grained emotional states.
   
   \item \textbf{Topic:} Topic control ensures the generated text remains focused on specific subject matter. For instance, when generating scientific text, the system should maintain relevant terminology and concepts while avoiding unrelated topics. Topic control becomes particularly important in long-form text generation where maintaining thematic coherence is crucial.
\end{itemize}

\subsubsection{Structural Control}

Structural control manages the organization and format of generated text:

\begin{itemize}
   \item \textbf{Format Specifications:} Many applications require text to follow specific formats. This includes generating poetry with particular rhyme schemes and meter, creating structured documents like academic papers or technical reports, or producing code with specific syntax requirements. The system must understand and maintain these format constraints throughout the generation process.
   
   \item \textbf{Document Organization:} Long-form text often requires specific organizational structures. This includes section ordering, paragraph breaks, and hierarchical relationships between different parts of the text. For example, a scientific paper generator needs to maintain standard sections like introduction, methods, results, and discussion.
   
   \item \textbf{Length Control:} Applications may need to generate text of specific lengths, from concise summaries to detailed explanations. Length control involves more than simple truncation - it requires generating complete, coherent text that naturally fits within the specified length constraints.
\end{itemize}

\subsubsection{Lexical Control}

Lexical control operates at the vocabulary level:

\begin{itemize}
   \item \textbf{Keyword Inclusion:} Some applications require the generated text to incorporate specific keywords or phrases. This is common in search engine optimization, technical documentation, or domain-specific content generation. The challenge lies in naturally integrating these keywords while maintaining text fluency.
   
   \item \textbf{Vocabulary Constraints:} Text generation systems often need to restrict their vocabulary based on the target audience or domain. For example, text simplification systems must avoid complex terminology when generating content for general audiences. Similarly, technical writing might require using standardized terminology from a controlled vocabulary.
   
   \item \textbf{Term Consistency:} In technical or specialized writing, maintaining consistent terminology throughout the text is crucial. This includes using the same terms for specific concepts and avoiding synonyms that might cause confusion.
\end{itemize}

\subsection{Challenges in Control Integration}

Integrating control conditions into text generation presents several technical challenges:

\begin{itemize}
   \item \textbf{Control-Quality Trade-off:} Stronger control over text attributes often comes at the cost of reduced fluency or naturalness. Finding the right balance between control strength and text quality remains an ongoing challenge.
   
   \item \textbf{Multiple Constraint Interaction:} When multiple control conditions are applied simultaneously, they may conflict with each other. For example, maintaining a specific sentiment while including required keywords, or following a strict format while ensuring topic relevance.
   
   \item \textbf{Long-range Consistency:} As text length increases, maintaining consistent control becomes more difficult. Models may drift from the specified attributes or lose coherence over longer sequences.
   
   \item \textbf{Implicit Control Factors:} Some control aspects, like style or tone, are difficult to specify explicitly and may depend on subtle linguistic features that are hard to capture and manipulate.
\end{itemize}

\subsection{Methods Overview}

Methods for implementing controlled text generation can be divided into two main approaches: training-time methods and inference-time methods.
This division reflects when control mechanisms are integrated into the text generation process \cite{liang2024controllabletextgenerationlarge, he-etal-2022-ctrlsum}.

\begin{itemize}
   \item \textbf{Training-time Methods:} These methods modify the language model during the training phase through various techniques:
        \begin{itemize}
            \item Complete model retraining with control-specific objectives
            \item Fine-tuning to adapt existing models
            \item Training additional modules or adapters
            \item Reinforcement learning with feedback signals
        \end{itemize}
       
   \item \textbf{Inference-time Methods:} These methods guide text generation during inference without modifying the base model:
       \begin{itemize}
            \item Prompt design and engineering
            \item Guided decoding strategies
            \item Latent state manipulation
        \end{itemize}
\end{itemize}

\subsection{Training-time Methods}
\label{c2:s:training-time}

Training-time methods directly incorporate control mechanisms into the model's parameters or architecture. These methods aim to teach the model to recognize and respond to control signals during the training process.

\subsubsection{Model Retraining}

Complete model retraining is one of the earliest approaches to controlled text generation.
This method involves training a language model from scratch with control-specific architectures or objectives.

The CTRL model \cite{keskar2019ctrlconditionaltransformerlanguage} exemplifies this approach, introducing control codes to guide text generation. During training, each text segment in the dataset is paired with a control code that specifies attributes like style, domain, or topic. For example:

\begin{verbatim}
[Science] The study found significant correlations...
[Reviews Rating: 5.0] This product exceeded my expectations...
[Wikipedia] The Industrial Revolution began...
\end{verbatim}

CTRL learns to associate these codes with specific text characteristics through natural co-occurrence during training. 
The model processes control codes as special tokens that influence the entire generation process. 
The training data preparation involves:

\begin{itemize}
   \item Identifying relevant control attributes
   \item Creating consistent control code formats
   \item Collecting and labeling large-scale training data
   \item Ensuring balanced representation of different control codes
\end{itemize}

While CTRL demonstrates effective high-level control, the model cannot easily adapt to new control attributes without retraining, and the discrete nature of control codes limits the level of precision it can achieve.

The CoCon architecture \cite{chan2022coconselfsupervisedapproachcontrolled} addresses these limitations by introducing a more flexible content-conditioning mechanism. 
Instead of relying on discrete control codes, CoCon embeds control signals directly into the model's hidden states. 
The architecture combines a base language model with a dedicated content-conditioning module that processes control signals. 
Integration layers then combine the conditioned and base representations to influence the generation process.

To train this architecture effectively, CoCon implements multiple complementary loss functions:

\begin{itemize}
   \item \textbf{Primary Reconstruction Loss ($\mathcal{L}_{recon}$):} Ensures the model can accurately generate text that incorporates the given control signals.
   \item \textbf{Null Content Loss ($\mathcal{L}_{null}$):} Helps the model maintain robust performance even when control signals are absent, which is important for practical applications.
   \item \textbf{Cycle Reconstruction Loss ($\mathcal{L}_{cycle}$):} Promotes consistency in how control signals are applied. It ensures that if we extract control signals from a generated text and use them to condition another generation, we obtain similar results.
   \item \textbf{Adversarial Loss ($\mathcal{L}_{adv}$):} Improves the naturalness of the generated text by training the model to produce outputs that are indistinguishable from human-written text while maintaining the desired control attributes.
\end{itemize}

The total loss function for training the CoCon model is a weighted combination of these four components:

\begin{equation}
    \mathcal{L}_{total} = \mathcal{L}_{recon} + \lambda_1 \mathcal{L}_{null} + \lambda_2 \mathcal{L}_{cycle} + \lambda_3 \mathcal{L}_{adv}
\end{equation}

where the $\lambda$ parameters balance the contribution of each loss term.

This multi-loss training strategy ensures that the model learns to balance control requirements with text fluency and coherence.
Furthermore, by keeping the original language model intact and only adding a lightweight content-conditioning module, CoCon largely preserves the pre-trained knowledge and capabilities of the base model.

For scenarios requiring precise lexical control, the POINTER model \cite{zhang-etal-2020-pointer} introduces a fundamentally different approach based on insertion-based generation. 
Unlike traditional left-to-right generation models that predict one token at a time in sequence, POINTER can insert tokens at any position in the sequence. 
This makes it particularly well-suited for ensuring specific keywords or phrases appear in the generated text.

POINTER operates through a progressive refinement process. 
The generation begins with an initial sequence containing only the required lexical constraints (i.e., the keywords or phrases that must appear in the final text). The model then iteratively inserts new tokens between existing ones, gradually building up the complete text. At each step, a learned policy determines both which tokens to insert and where to place them, considering the surrounding context and the overall coherence of the sequence.

This insertion-based approach provides several advantages for lexically constrained generation. It guarantees that required keywords will appear in the output since they form the starting point of generation. The ability to insert tokens at any position allows for more flexible text construction compared to strict left-to-right generation. However, this flexibility comes with computational costs, as each insertion operation requires evaluating multiple possible positions and tokens. The process can become particularly intensive for longer sequences where many insertions are needed to complete the text.

\subsubsection{Fine-tuning}

Fine-tuning adapts a pre-trained language model to handle controlled text generation by adjusting its parameters using specialized datasets or training objectives. 
This approach preserves the general language generation capabilities of the pre-trained model while improving its ability to respond to specific control signals.

The fine-tuning process modifies the model parameters according to:

\begin{equation}
    \Theta^* = \Theta + \Delta\Theta
\end{equation}

where $\Theta$ represents the original parameters of the pre-trained model, and $\Delta\Theta$ represents the updates learned during fine-tuning. The parameter updates are computed by minimizing a task-specific loss:

\begin{equation}
    \Delta\Theta = \arg\min_{\Theta} \mathcal{L}(D_{control}, f(X; \Theta))
\end{equation}

where $D_{control}$ is a dataset designed for the control task, and $f(X; \Theta)$ represents the model's output for input $X$.

\paragraph{Adapter-Based Fine-tuning}
introduces specialized neural modules into the pre-trained model while keeping its original parameters frozen. This reduces the risk of catastrophic forgetting, where fine-tuning causes the model to lose previously learned knowledge.

Auxiliary Tuning \cite{zeldes2020technicalreportauxiliarytuning} adds a separate auxiliary model alongside the pre-trained language model. 
The auxiliary model processes both the input text and control signals, producing logits that are combined with the base model's output through a softmax operation:

\begin{equation}
    P(y|x,C) = \text{softmax}(f_{LM}(x) + f_{AUX}(x,C))
\end{equation}

where $f_{LM}$ represents the frozen pre-trained model and $f_{AUX}$ is the trainable auxiliary model. 
Intuitively, the auxiliary model learns the residual logits needed to shift the probability distribution of the pre-trained model towards the target distribution.
To further improve training efficiency, the lower layers of the pre-trained model can also be used as a feature extractor for the auxiliary model inputs. 
There are no constraints on the auxiliary model architecture, except that it must output logits over the same vocabulary as the original model.
It is typically much smaller than the pre-trained model (e.g., a few Transformer layers) and can be trained on a small amount of data.

DisCup \cite{zhang-song-2022-discup} combines discriminator guidance with prompt-tuning.
The training objective includes both likelihood and unlikelihood terms:

\begin{equation}
    \mathcal{L}_{total} = \sum_{i=1}^{|D|} \sum_{t=1}^{|x^{(i)}|} \mathcal{L}_{like}(x_t^{(i)}) + \mathcal{L}_{unlike}(x_t^{(i)})
\end{equation}

For each training step, the base model generates candidate tokens which the discriminator scores based on their alignment with desired attributes. 
$\mathcal{L}_{like}$ maximizes the probability of positively-scored tokens, while $\mathcal{L}_{unlike}$ minimizes the probability of negatively-scored ones. 
The unlikelihood training helps prevent the model from learning spurious correlations present in training data.
Furthermore, by operating on self-generated candidates rather than ground-truth tokens, DisCup maintains output diversity while improving attribute control.

LiFi \cite{shi2024lifilightweightcontrolledtext} proposes an different approach using lightweight adapters guided by attribute classifiers. The adapters are added to each transformer layer while keeping the base model frozen:

\begin{equation}
    h' = h + \text{Adapter}(h, c)
\end{equation}

where $h$ is the layer's hidden state and $c$ is the control signal from the classifier. The adapters use a bottleneck architecture with reduction factors of 16 and 4 for feedforward and attention modules respectively, adding only 0.04\% parameters to the base model.

During generation, multiple adapters can be combined through a weighted fusion mechanism:

\begin{equation}
    h'_t = h_t + \sum_{i=1}^k w_i \text{Adapter}_i(h_t, c_i)
\end{equation}

where $w_i$ are learnable weights balancing different control signals, and $k$ is the number of adapters. This allows LiFi to handle multiple control attributes while maintaining computational efficiency.

\paragraph{Data-Driven Fine-Tuning} 
focuses on creating better datasets to teach language models about control through fine-tuning.

FLAN \cite{wei2022finetunedlanguagemodelszeroshot} showed that turning NLP tasks into natural language instructions helps models learn new tasks without examples. 
For instance, instead of training a model to summarize text, FLAN trains it to follow instructions like ``Summarize the text'' or ``Write a positive review about the product''.
This helps the model understand what it needs to do through clear directions, enabling zero-shot generalization to new tasks.

Building on this idea, InstructCTG \cite{zhou2023controlledtextgenerationnatural} applies instruction-based fine-tuning specifically to controlled text generation. 
It turns control constraints into natural instructions and uses them to fine-tune language models. This makes the control process more intuitive, as the model learns to follow human-like directions rather than abstract control signals.

CHRT \cite{kumar-etal-2023-controlled} uses contrastive learning to control multiple attributes at once. 
It works by comparing texts that have different attributes (e.g., positive vs negative sentiment) and learning how these differences appear in the model's hidden layers. 
This helps the model understand how to change its output to match desired attributes without changing its basic structure.
CHRT combines two loss functions to balance attribute control and text quality:

\begin{itemize}
   \item \textbf{Contrastive Loss ($\mathcal{L}_c$):} Uses triplet loss to guide the transformed hidden representations towards a model fine-tuned on desired attributes and away from one fine-tuned on undesired attributes. For example, when controlling toxicity, it pushes the representations closer to a non-toxic model and farther from a toxic one.
   
   \item \textbf{Preservation Loss ($\mathcal{L}_p$):} Maintains the model's original language capabilities by minimizing the distance between the transformed representations and the base model's representations.
\end{itemize}

The total loss is a weighted combination of these two omponents:

\begin{equation}
    \mathcal{L}_{total} = \lambda\mathcal{L}_p + (1-\lambda)\mathcal{L}_c
\end{equation}

where $\lambda$ balances between preserving the original model's capabilities and achieving strong attribute control. One advantage of CHRT is that it can control multiple attributes by combining different transformation blocks during inference, without needing to retrain the model.

\subsubsection{Reinforcement Learning}

Reinforcement learning methods optimize text generation by providing feedback signals that indicate how well the generated text satisfies the control objectives. 
These methods can handle complex, non-differentiable control criteria and adapt to human preferences. 
The RL optimization adjusts model parameters $\Theta$ according to:

\begin{equation}
   \Theta^* = \Theta + \alpha\nabla_\Theta \mathbb{E}_{\pi_\Theta}[R(X)]
\end{equation}

where $\alpha$ is the learning rate, $\pi_\Theta$ is the policy defined by the model parameters, and $R(X)$ is the reward for generated text $X$. 
Since the exact reward function is often unknown, in practice, RL methods like REINFORCE \cite{10.1007/BF00992696} use the policy gradient to estimate the gradient of the expected reward:

\begin{equation}
   \nabla_\Theta \mathbb{E}_{\pi_\Theta}[R(X)] \approx \sum_{t=1}^T \nabla_\Theta \log P(x_t|x_{<t}, C) R(X)
\end{equation}

RL methods fall into two categories based on how they get their feedback: from humans or from automated systems.

\subsubsection{Human Feedback Methods}

Human feedback methods use human evaluators to provide reward signals for the generated text.
These methods are particularly useful for subjective control attributes that are hard to define algorithmically.

\gls{rlhf} \cite{stiennon2022learningsummarizehumanfeedback} combines human feedback with reinforcement learning to control text summarization.
The method consists of three stages: collecting human feedback, training a reward model, and optimizing the language model using RL.

The first stage creates a dataset of human preferences by presenting annotators with pairs of model outputs and asking them to select the better summary.
This process generates a dataset $D$ of preferences $(x, y_0, y_1, i)$, where $x$ is the input text, $y_0$ and $y_1$ are the two generated summaries, and $i$ indicates the preferred summary.

The reward model $r_\theta(x, y)$ is then trained to predict human preferences:
\begin{equation}
    \mathcal{L}_{reward} = -\mathbb{E}_{(x,y_0,y_1,i)\sim D}[\log(\sigma(r_\theta(x, y_i) - r_\theta(x, y_{1-i}))]
\end{equation}
This loss function encourages the reward model to assign higher scores to summaries preferred by humans.

Finally, the language model is fine-tuned using Proximal Policy Optimization (PPO) to maximize the predicted reward while staying close to the original model's behavior through a KL penalty:
\begin{equation}
    R(x, y) = r_\theta(x, y) - \beta \text{ KL}(\pi_{RL}, \pi_{base})
\end{equation}
where $\beta$ balances between maximizing the predicted reward and maintaining the original model's behavior.

Safe RLHF \cite{dai2023saferlhfsafereinforcement} extends RLHF to address a fundamental tension in language model alignment: balancing helpfulness with harmlessness. 
Traditional \gls{rlhf} approaches often struggle when these objectives conflict, as a model being maximally helpful (e.g., providing detailed information about dangerous topics) might compromise safety.
Safe RLHF decouples these two objectives by introducing a cost model $C_\psi(y, x)$ that penalizes harmful content. The optimization objective becomes:

\begin{equation}
\begin{aligned}
    \text{maximize}_\theta & \quad \mathbb{E}_{x,y}[R_\phi(y,x)] \\
    \text{subject to} & \quad C_\psi(y,x) \leq 0
\end{aligned}
\end{equation}

where $R_\phi$ is the reward model and $C_\psi$ is the cost model. This constraint ensures generated text remains helpful while avoiding harmful content.

The problem is solved using Lagrangian relaxation:

\begin{equation}
    \min_{\lambda \geq 0} \max_\theta [-J_R(\theta) + \lambda J_C(\theta)]
\end{equation}

where $J_R$ and $J_C$ are the expected reward and cost, and $\lambda$ is the Lagrange multiplier that adjusts the trade-off between the two objectives.
When the model becomes less safe, $\lambda$ increases to prioritize harmlessness, and when safety improves, $\lambda$ decreases to focus more on helpfulness.

\subsubsection{Automated Feedback Methods}

Automated feedback methods guide language model training using predefined reward functions rather than human feedback. 
These methods scale better since they don't require collecting human preferences. 

Distributional approaches like GDC \cite{khalifa2021distributionalapproachcontrolledtext} treat controlled generation as a constrained optimization problem. 
The goal is to find a text distribution that satisfies the control requirements while staying close to the original model distribution. This prevents the model from generating unnatural text when trying to meet the control conditions. 
GDC solves this by finding:

\begin{equation}
P(x) = \argmin_{c \in C} D_{KL}(c | a)
\end{equation}

where $P(x)$ is the target distribution, $C$ contains distributions meeting the constraints, and $a$ is the original model distribution. The solution takes an energy-based form:

\begin{equation}
P(x) \propto a(x) e^{\sum_i \lambda_i \phi_i(x)}
\end{equation}

Here $\phi_i(x)$ are feature functions encoding control conditions and $\lambda_i$ are learned parameters. 
GDC learns these parameters through importance sampling, making it more efficient than direct optimization.

A major challenge in reinforcement learning for text generation is the sparse reward problem, where models only receive feedback after generating complete sequences. 
DRL \cite{upadhyay2022efficientreinforcementlearningunsupervised} addresses this by providing dense rewards at each generation step. The method combines three types of token-level rewards:

\begin{equation}
r_t = \lambda_s r_t^s + \lambda_c r_t^c + \lambda_f r_t^f
\end{equation}

The style reward $r_t^s$ uses attention scores from a classifier to identify style-relevant tokens. 
Content preservation reward $r_t^c$ measures n-gram overlap with the input. 
Fluency reward $r_t^f$ comes from a language model to maintain natural text. 
This dense feedback helps the model learn more effectively than with sparse sentence-level rewards.

For medical text simplification, TESLEA \cite{info:doi/10.2196/38095} combines fine-tuning and reinforcement learning. After initial training on paired medical reviews, the method optimizes three rewards:

\begin{itemize}
    \item \textbf{Relevance Reward ($R_{cosine}$):} Uses BioSentVec embeddings \cite{Chen_2019} to measure semantic similarity between original and simplified text, preserving medical meaning.
    
    \item \textbf{Readability Reward ($R_{Flesch}$):} Encourages text to reach a target grade level using Flesch-Kincaid readability scores.
    
    \item \textbf{Lexical Simplicity Reward ($R_{lexical}$):} Promotes common and accessible vocabulary based on word frequency statistics following Zipf's law.
\end{itemize}

The total reward combines these components:

\begin{equation}
    R_{total} = \alpha R_{cosine} + \beta R_{Flesch} + \gamma R_{lexical}
\end{equation}

TESLEA uses Self-Critical Sequence Training \cite{8099614}, generating two versions of each simplification through greedy and sampling-based decoding. The final loss balances reinforcement learning against MLE to prevent drift:

\begin{equation}
    \mathcal{L}_{total} = \lambda \mathcal{L}_{RL} + (1-\lambda)\mathcal{L}_{MLE}
\end{equation}

TESLEA made medical texts significantly easier to read, lowering the FKGL by 2.5 points while maintaining high ROUGE and SARI scores. 
When evaluated by domain experts, over 70\% agreed on the quality of the simplifications across multiple dimensions including fluency, coherence, and factual accuracy.
The outputs from TESLEA were consistently shorter than those produced by baseline models and the input paragraphs themselves, suggesting it learned to remove unnecessary technical details while retaining the most relevant content.

\subsection{Inference-time Methods}

Inference-time methods change how language models generate text without modifying their parameters. 
This makes them flexible and practical, especially when working with large models that are expensive to retrain. 

\subsubsection{Prompt Engineering}

Prompt engineering controls text generation by providing specific instructions or cues to the language model. These can be either hard prompts (natural language instructions) or soft prompts (trainable vectors).

Hard prompts are direct text instructions that tell the model what to do. For example:
\begin{verbatim}
Write a medical text about diabetes using simple language:
Explain the risks of smoking in a positive tone:
\end{verbatim}

While simple to use, hard prompts can be unpredictable, small changes in wording can lead to very different outputs. To address this, researchers developed methods to find effective prompts automatically. AutoPrompt \cite{shin-etal-2020-autoprompt} uses gradient-based search to find the best trigger words for a given task. It starts with a template containing [MASK] tokens and replaces them with words that maximize the probability of desired outputs.

Soft prompts offer more reliable control by using trainable vectors instead of words. Prefix-Tuning \cite{li-liang-2021-prefix} adds these vectors before each layer of the model, while keeping the model's parameters fixed. The vectors learn to guide the model toward specific outputs during training. When tested on table-to-text generation and summarization, Prefix-Tuning matched the performance of full model fine-tuning while changing only 0.1\%-2\% of the parameters.

P-tuning \cite{liu-etal-2022-p} improved soft prompting by processing the prompt vectors through a bidirectional LSTM network. This made training more stable and worked better for both classification and generation tasks. The method was particularly good at handling new situations with few examples.

\subsubsection{Guided Decoding}

Guided decoding is a technique used during the decoding process of a language model to bias the generated text towards the desired control attributes. 
It works by manipulating the logits or probability distribution of the model to favor tokens that align with the control conditions.
This is often done using classifiers or reward models that provide additional supervision signals to guide the generation process.

PPLM \cite{dathathri2020plugplaylanguagemodels} was one of the first methods to show this was possible. It uses simple attribute models (e.g., sentiment classifiers) to adjust the internal states of GPT-2. This lets it control attributes like topic and tone without changing the base model. However, because it needs multiple passes through both models for each word, generation is quite slow.

GeDi \cite{krause-etal-2021-gedi-generative} made this process faster by using small language models trained on specific attributes instead of classifiers. For example, one model trained on positive text and another on negative text. During generation, GeDi compares how likely each potential next word is under both models to decide which words to favor.

DExperts \cite{liu-etal-2021-dexperts} simplified this further by directly combining predictions from three models: a base model, an ``expert'' trained on desired text (like non-toxic), and an ``anti-expert'' trained on undesired text (like toxic). This worked well for making GPT-2 and GPT-3's text less toxic while staying natural. Small expert models (125M parameters) could successfully guide much larger base models (1.5B parameters).

Mix and Match \cite{mireshghallah-etal-2022-mix} took a different approach by treating text generation as a sampling problem. It combines scores from different models (for fluency, topic, safety, etc.) into a single energy function. The method then uses Metropolis-Hastings sampling to find text that balances all these aspects well. While this produced good results, the sampling process made generation slower than other methods.

\subsubsection{Latent Space Manipulation}

Latent space manipulation controls text by adjusting the hidden states inside the language model. These hidden states form a high-dimensional space where similar words and concepts are close together. By moving through this space in specific directions, we can change the attributes of generated text.

ICV \cite{liu2024incontextvectorsmakingcontext} finds these directions by comparing how the model processes examples with different attributes. For instance, comparing the hidden states when processing positive versus negative text reveals a ``sentiment direction''. Adding or subtracting along this direction during generation then makes text more positive or negative.

ActAdd \cite{turner2024steeringlanguagemodelsactivation} showed that even simple changes to these hidden states can effectively control generation. The method first identifies which parts of the hidden states correspond to specific attributes. It then adjusts these values during generation, similar to turning a dial to control different aspects of the text.

MIRACLE \cite{lu-etal-2023-miracle} handles multiple attributes at once by mapping text into a special latent space designed to keep different attributes separate. This helps prevent attributes from interfering with each other, allowing more precise control over multiple aspects of the generated text.

\section{Evaluation Metrics}

\subsection{Traditional Readability Metrics}
\label{subsec:traditional-metrics}

Readability formulas estimate text difficulty using surface-level features such as average sentence length, word length, and proportion of complex words. These metrics help evaluate many types of medical content, from patient education materials for conditions like nocturnal enuresis \cite{Fung2024-uh} and bariatric surgery \cite{Lucy2023-zi}, to discharge instructions \cite{Tuan2023-wc} and critical care information \cite{Hanci2024-wv}.

\textbf{\gls{fkgl}} estimates the U.S. grade level needed to understand a text:

\begin{equation}
    \text{FKGL} = 0.39 \times \left(\frac{\text{words}}{\text{sentences}}\right) + 11.8 \times \left(\frac{\text{syllables}}{\text{words}}\right) - 15.59
\end{equation}

FKGL scores typically range from 0 to 18, with higher scores indicating more difficult text. A score of 9.2 suggests text suitable for ninth-grade students, while scores above 12 indicate college-level or specialized content.

\textbf{\gls{smog}} calculates text difficulty based on polysyllabic words in a 30-sentence sample:

\begin{equation}
    \text{SMOG} = 1.0430 \times \sqrt{\text{polysyllables} \times \left(\frac{30}{\text{sentences}}\right)} + 3.1291
\end{equation}

Like \gls{fkgl}, \gls{smog} scores are interpreted as U.S. grade levels. 
\gls{smog} has gained wide adoption in healthcare settings due to its relative ease of use and focus on vocabulary complexity.

\textbf{\gls{dcrs}} measures text difficulty by calculating the percentage of words not found in a list of 3,000 familiar words:

\begin{equation}
    \text{DCRS} = 0.1579 \times \left(\frac{\text{difficult words}}{\text{total words}} \times 100\right) + 0.0496 \times \left(\frac{\text{total words}}{\text{total sentences}}\right)
\end{equation}

If the percentage of difficult words exceeds 5\%, an additional constant of 3.6365 is added to the raw score to get the final \gls{dcrs}. Scores range from 4.9 or below for easily understood text to 10 or above for very challenging text.

Other commonly used metrics include the \textbf{\gls{ari}}, \textbf{\gls{cli}}, \textbf{\gls{gfi}}, and \textbf{Linsear Write Formula}. 
hile the specific calculations differ, they all aim to estimate text difficulty based on factors like word length, sentence length, and syllable counts.

Traditional readability formulas face several limitations when applied to medical texts:

\begin{itemize}
    \item They only examine surface-level features, missing deeper aspects of conceptual complexity in medical information \cite{Crossley2022, WANG2013503, Singh2024}.
    
    \item They treat the text as a ``bag of words'', ignoring features like information density, organization, coherence, and syntax.
    
    \item They do not consider the cognitive load imposed by specific terms \cite{Swanson2024}. Short terms like ``polyp'' and ``apnea'' may be challenging for lay readers, while longer words like ``gastrointestinal'' and ``cardiovascular'' may be more familiar in a medical context.
    
    \item They were developed using general reading materials, often geared towards children's education levels. The assumptions and thresholds used in these formulas may not generalize well to the specialized language and adult literacy levels of medical texts \cite{Crossley2022}.

\end{itemize}

Despite these limitations, readability formulas provide a quantitative starting point for improving medical text clarity. Experts generally recommend using them alongside other evaluation approaches, combining multiple metrics with expert review and user testing \cite{Ko2024-dd, tanprasert-kauchak-2021-flesch}.

\subsection{Reference-based Metrics}

Reference-based metrics compare generated text against human-written reference simplifications. 
These metrics help measure how well the system's output matches examples created by domain experts or crowdsourced annotators.

\textbf{\gls{sari}} \cite{xu-etal-2016-optimizing} specifically targets text simplification quality by comparing n-grams in the candidate and reference texts.
For a given word sequence, \gls{sari} calculates:

\begin{equation}
    \text{SARI} = \frac{F_{add} + F_{keep} + F_{del}}{3}
\end{equation}

where $F_{add}$, $F_{keep}$, and $F_{del}$ are F1 scores for added, kept, and deleted n-grams compared to the reference text. This metric is particularly relevant for medical text simplification as it captures both readability improvements and content preservation \cite{li2024largelanguagemodelsbiomedical}.

\textbf{BERTScore} \cite{zhang2020bertscoreevaluatingtextgeneration} compares generated text against references using contextual embeddings from BERT models. For a candidate token $x_i$ and reference token $\hat{x}_j$, it computes their similarity using cosine similarity of their BERT embeddings. The metric combines these token-level similarities into precision and recall scores:

\begin{equation}
    R_{BERT} = \frac{1}{|x|} \sum_{x_i \in x} \max_{\hat{x}_j \in \hat{x}} x_i^T \hat{x}_j
\end{equation}

\begin{equation}
    P_{BERT} = \frac{1}{|\hat{x}|} \sum_{\hat{x}_j \in \hat{x}} \max_{x_i \in x} x_i^T \hat{x}_j
\end{equation}

The final BERTScore is an F1 measure that balances precision and recall:

\begin{equation}
    F_{BERT} = \frac{2(P_{BERT} \cdot R_{BERT})}{P_{BERT} + R_{BERT}}
\end{equation}

This approach catches semantic similarities that n-gram metrics miss, since BERT embeddings can match related words like ``car'' and ``vehicle''. 
It also handles word reordering better since matching is done at the token level. 
BERTScore has shown the highest correlations with human judgments of simplicity and meaning preservation compared to other metrics, particularly when evaluating the outputs of neural sequence-to-sequence models \cite{alva-manchego-etal-2021-un}, though it is unclear how well it generalizes to the medical domain. 

\subsection{Reference-free Metrics}

Reference-free metrics evaluate text without relying on human-written reference simplifications.
This makes them appealing because collecting high-quality human reference simplifications is time-consuming, expensive, and may not always be feasible \cite{alva-manchego-etal-2021-un, 94205144dd7945cc99b5a6544451b668, deutsch-etal-2022-limitations}.

\textbf{\gls{samsa}} \cite{sulem-etal-2018-semantic} measures structural simplification by analyzing how well complex sentences are broken down into simpler ones. It uses semantic parsing to identify the main ideas in a sentence and checks if each appears in its own simple sentence. For a simplified text $S$ containing $n$ semantic units, \gls{samsa} calculates:

\begin{equation}
   \text{SAMSA} = \frac{\text{number of properly allocated semantic units}}{n}
\end{equation}

This metric is especially relevant for medical text, where breaking down complex medical concepts into digestible units improves reader understanding.

\subsection{LLM-based Metrics}

LLM-based metrics use large language models to evaluate text quality. 
Unlike traditional metrics that count words or measure sentence length, these metrics can understand meaning and assess writing style.

\textbf{G-Eval} \cite{liu-etal-2023-g} uses GPT-4 to score text on four aspects:

\begin{itemize}
   \item \textbf{Coherence} (1-5): How well ideas connect and flow
   \item \textbf{Consistency} (1-5): Whether all facts match the source text
   \item \textbf{Fluency} (1-3): Grammar and sentence quality 
   \item \textbf{Relevance} (1-5): Whether important information is kept
\end{itemize}

G-Eval samples multiple scores for each aspect using temperature sampling, then combines them with a probability weighting:

\begin{equation}
   \text{Score} = \sum_{i=1}^{n} p_i s_i
\end{equation}

where $p_i$ is the probability of each score, $s_i$ is the score value, and $n$ is the number of samples.

G-Eval prompts GPT-4 to evaluate each aspect separately, asking for step-by-step reasoning before giving a score. 
While originally designed for summarization tasks, its framework could be adapted for evaluating medical text simplification, though it may require additional fine-tuning on medical data.

