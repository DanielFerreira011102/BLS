\chapter{Background}
\label{c2}

This chapter presents some basic tips and a few examples on how to use \LaTeX.


\section{Language Models}
\label{c2:s:language-models}

\section{Measuring Readability in the Biomedical Domain}
\label{c2:s:linguistic-complexity}

The complexity of biomedical terminology represents a unique challenge for language complexity measurement. 
Medical terms often combine elements from multiple languages, primarily Greek and Latin, creating systematic patterns that affect both structural and cognitive complexity. 
These patterns follow predictable rules but can create significant processing challenges for non-expert readers.

The relationship between professional and lay terminology adds another layer of complexity. 
Many medical concepts can be expressed through either technical or lay terms (e.g., ``myocardial infarction'' vs. ``heart attack''), creating parallel vocabularies that must be considered in complexity measurement. 

% The title should be either "Statistical Readability Metrics" or "Traditional Readability Metrics"
\subsection{Traditional Readability Metrics}

Readability formulas are designed to estimate the difficulty of a text based on surface-level features such as average sentence length, average word length, and the proportion of difficult or unfamiliar words. While originally developed for general-domain texts, some of these metrics have been applied to biomedical literature to assess the accessibility and suitability of patient education materials, consent forms, and other health-related documents. For example, researchers have used these metrics to evaluate the readability of online patient education materials for conditions such as nocturnal enuresis \cite{Fung2024-uh}, bariatric surgery \cite{Lucy2023-zi}, and female pelvic floor disorders \cite{Varli2023-ma}. Moreover, these metrics have been utilized to assess the readability of discharge instructions for heart failure patients \cite{Tuan2023-wc} and to analyze the quality of information provided by intensive and critical care societies \cite{Hanci2024-wv}.

One of the most widely reported readability metrics is the \glsxtrfull{fkgl}. This formula estimates the U.S. grade level required to understand a given text, calculated as follows:
$$FKGL = 0.39 \times \left(\frac{\text{words}}{\text{sentences}}\right) + 11.8 \times \left(\frac{\text{syllables}}{\text{words}}\right) - 15.59$$
\gls{fkgl} scores typically vary from 0 to 18, with higher scores indicating more difficult text. For example, a score of 9.2 would suggest that the text is suitable for an average 9th-grade student. Text that scores above 12 suggest college-level or domain-specific expertise.

Another commonly used readability formula is the \glsxtrfull{smog}. This metric estimates the years of education needed to understand a piece of writing based on the number of polysyllabic words (i.e., those with three or more syllables) in a sample of 30 sentences, using the following formula:
$$SMOG = 1.0430 \times \sqrt{\text{polysyllables} \times \left(\frac{30}{\text{sentences}}\right)} + 3.1291$$
Like \gls{fkgl}, \gls{smog} scores correspond to U.S. grade levels. A score of 13 indicates a college freshman reading level, while scores below 6 are considered easily understood by most readers. The \gls{smog} formula has gained popularity in healthcare settings due to its relative ease of use and focus on vocabulary complexity. Its widespread use in healthcare is supported by a 2010 study published in the Journal of the Royal College of Physicians of Edinburgh, which recommended \gls{smog} as the preferred measure for evaluating consumer-oriented healthcare material \cite{Fitzsimmons2010-mq}.

The \glsxtrfull{dcrs} is another well-known readability metric that assesses text difficulty based on the average sentence length and the percentage of ``difficult'' words not found on a list of 3,000 familiar words. The original \gls{dcrs} formula is:
$$DCRS = 0.1579 \times \left(\frac{\text{difficult words}}{\text{total words}} \times 100\right) + 0.0496 \times \left(\frac{\text{total words}}{\text{total sentences}}\right)$$
If the percentage of difficult words exceeds 5\%, an additional constant of 3.6365 is added to the raw score to get the final \gls{dcrs}. Scores range from 4.9 or below for easily understood text to 10 or above for very challenging text.

In addition to \gls{fkgl}, \gls{smog}, and \gls{dcrs}, there are many other readability formulas, such as the \glsxtrfull{ari}, \glsxtrfull{cli}, \glsxtrfull{gfi}, and Linsear Write Formula. While the specific calculations differ, they all aim to estimate text difficulty based on factors like word length, sentence length, and syllable counts.

Although traditional readability formulas are still widely used, they have several notable limitations when applied to biomedical texts. First, these formulas rely on surface-level features like word and sentence length, which may not adequately capture the conceptual complexity of medical information \cite{Crossley2022, WANG2013503, Singh2024}.
They treat the text as a ``bag of words'', ignoring higher-level discourse structures that influence comprehension, such as information density, organization, coherence, and syntax.  
Second, readability formulas often treat all words equally based on length or syllable count, without considering the cognitive load imposed by specific terms \cite{Swanson2024}. Polysyllabic medical terms may be familiar to experts but challenging for lay readers, while short words like ``apnea'' or ``polyp'' may also be difficult for average adults to understand.
Third, these formulas were developed using general reading materials, often geared towards children's education levels. The assumptions and thresholds used in these formulas may not generalize well to the specialized language and adult literacy levels of biomedical texts \cite{Crossley2022}.
Finally, readability scores are typically reported as a single average or grade level for an entire text, obscuring any variability in difficulty within the document.
Despite these drawbacks, readability formulas provide an objective, quantitative measure of text complexity that can serve as a starting point for improving the clarity and accessibility of health-related materials. However, experts generally recommend using these formulas as part of a more holistic approach, combining multiple metrics with expert evaluation and user testing to ensure that health information is truly accessible and understandable for the intended audience.

\subsection{Reference-Based Evaluation Metrics}

A series of reference-based evaluation metrics have recently been proposed in an attempt to address the limitations of traditional readability formulas when applied to the task of text simplification. 
These metrics compare the generated text to a reference simplification, using more sophisticated natural language processing techniques to capture semantic similarity, information preservation, and fluency.

The most widely adopted metric in the text simplification community might be the \glsxtrfull{sari} \cite{xu-etal-2016-optimizing}. 
SARI evaluates the quality of a simplified text by comparing it to multiple reference simplifications and the original complex sentence. 
It computes the arithmetic mean of three sub-scores: 1) the F1 score for n-grams present in the output and the references but not in the input (addition), 2) the F1 score for n-grams in the output and input but not the references (copying), and 3) the F1 score for n-grams in the input and references but not the output (deletion). 
Considering these three operations, SARI rewards simplifications that add simplified n-grams, preserve important n-grams from the input, and delete unimportant n-grams. 
In the biomedical domain, SARI has been shown to be a more reliable metric for assessing the quality of generated simplifications than other n-gram-based metrics like BLEU and ROUGE \cite{li2024largelanguagemodelsbiomedical}.

BERTScore is another popular metric that uses contextualized word embeddings from the BERT language model to compute semantic similarity between the candidate and reference simplifications \cite{zhang2020bertscoreevaluatingtextgeneration}.
It aligns each token in the candidate text with the most similar token in the reference, and then sums these cosine similarities to calculate precision and recall, with the F1 score computed as their harmonic mean.
BERTScore has demonstrated good performance in various text generation tasks, including translation evaluation \cite{Vetrov2022ANA, Tang2024ImprovingBF}. However, its specific correlation with human judgments of simplicity and meaning preservation in biomedical text simplification remains understudied.
As a reference-based metric, BERTScore relies on the availability and quality of human-written reference simplifications. It does not explicitly measure readability aspects such as lexical or syntactic complexity \cite{li2024largelanguagemodelsbiomedical}. 
Additionally, BERTScore has some limitations, such as its greedy alignment approach potentially allowing system outputs to receive excess credit relative to a reference \cite{jin-gildea-2022-rewarding}.

\glsxtrfull{rouge}, a family of metrics originally developed for text summarization, has also been adapted for text simplification \cite{xu-etal-2016-optimizing}.
ROUGE measures the n-gram overlap between the simplified text and the references, with higher scores indicating greater lexical similarity.
Different ROUGE variants focus on various aspects of n-gram matching.
ROUGE-N calculates the recall based on the ratio of matching n-grams (e.g., unigrams for ROUGE-1, bigrams for ROUGE-2) to total reference n-grams, but it does not account for the order or contiguity of the matches. 
ROUGE-L addresses this by considering the precision, recall, and F-measure of the largest common subsequence, which preserves order but allows gaps. 
ROUGE-S offers a more flexible matching by measuring the overlap of skip-bigrams, capturing both local and long-range sentence structure similarities. 
ROUGE-W is a weighted version of ROUGE-L that assigns higher importance to consecutive matches, favoring longer common subsequences.
These ROUGE variants capture lexical and structural similarities at different granularities, but they have limitations when used to evaluate text simplification systems. 
ROUGE does not explicitly measure semantic similarity, potentially missing the nuances of meaning preservation between the original and simplified texts \cite{xu2024reasoningcomparisonllmenhancedsemantic}.
It also does not account for readability factors such as vocabulary or syntactic simplicity, which are crucial for text simplification \cite{?}.
Therefore, while ROUGE provides useful insights into lexical and structural similarities, additional metrics targeting semantic and readability aspects may be necessary for a more well-rounded evaluation of simplification quality \cite{li2024largelanguagemodelsbiomedical}.

Other reference-based metrics that have been used for simplification evaluation include \glsxtrfull{bleu}, which calculates the precision of n-gram matches between the candidate and reference, with a brevity penalty \cite{papineni-etal-2002-bleu}; \glsxtrfull{meteor}, which computes a weighted F-score based on exact, stem, synonym, and paraphrase matches between the candidate and reference \cite{banerjee-lavie-2005-meteor}; and \glsxtrfull{ter}, which measures the minimum number of edits (insertions, deletions, substitutions, and shifts) required to transform the candidate text into the reference \cite{snover-etal-2006-study}.
However, all reference-based metrics share some common limitations. 
They require high-quality reference simplifications, which may be difficult and expensive to obtain, especially for large biomedical corpora. 
They also assume that the references represent the only valid simplifications, penalizing alternative paraphrases that may be equally fluent and meaningful. 
Furthermore, these metrics do not directly measure the readability or accessibility of the simplified text for the target audience, only its similarity to the references. 
Shardlow \cite{Shardlow2014} argues that reference-based metrics may not adequately capture the full range of valid simplifications, especially for complex technical texts like those in the biomedical domain.

\subsection{Reference-Free Evaluation Metrics}

To address the limitations of reference-based methods, some researchers have proposed reference-free evaluation metrics for text simplification. These metrics aim to assess the quality of the simplified text directly, without relying on comparisons to reference simplifications.

LENS (Learnable Evaluation Metric for Simplification) is a recently proposed reference-free metric that uses transfer learning to capture different aspects of simplification quality \cite{maddela-etal-2023-lens}. LENS fine-tunes a pre-trained language model (e.g., BERT) on synthetic training data with relevant features such as readability variables and text coherence measures. The fine-tuned model can then predict simplification quality scores for new texts. LENS has shown strong correlations with human judgments of fluency, meaning preservation, and simplicity, outperforming previous reference-free metrics.

Other reference-free approaches include using pre-trained language models to estimate the probability or perplexity of the simplified text \cite{ke-etal-2022-ctrleval}, calculating readability scores based on lexical and syntactic features \cite{Truica2023-au}, and measuring the text's similarity to a corpus of simple or complex language \cite{Truica2023-au}.
However, reference-free metrics also have some limitations. They may not capture all aspects of simplification quality, such as information preservation or target audience appropriateness. They also require careful selection and validation of features or training data to ensure their relevance and generalizability to the biomedical domain.