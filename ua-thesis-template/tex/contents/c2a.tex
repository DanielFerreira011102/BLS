\chapter{Background}
\label{c2}

This chapter presents some basic tips and a few examples on how to use \LaTeX.


\section{Language Models}
\label{c2:s:language-models}



\section{Controlled Text Generation}
\label{c2:s:controlled-text-generation}

\gls{ctg} is a growing area of \gls{nlp} research that focuses on developing techniques to guide language models in producing outputs with specific desired attributes or properties \cite{liang2024controllabletextgenerationlarge, keskar2019ctrlconditionaltransformerlanguage, dathathri2020plugplaylanguagemodels}.
The goal is to enable fine-grained control over various aspects of the generated text, such as style, sentiment, topic, or complexity level, without compromising the fluency, coherence, and relevance of the output. 
This is in contrast to traditional language generation where the model has little control over the nature of the generated text beyond the input prompt.

Formally, \gls{ctg} can be defined as the task of generating a sequence of tokens $Y = \{y_1, y_2, ..., y_m\}$ that maximizes the conditional probability distribution $P(Y|X, A)$, given an input context $X$ and a set of control attributes $A = \{a_1, a_2, ..., a_k\}$. 
Mathematically, this can be expressed as:

\begin{equation}
    \label{eq:controlled-generation}
    \argmax_Y P(Y | X, A) = \argmax_Y \prod_{i = 1}^{m} P(y_i | y_{<i}, X, A)
\end{equation}

where $y_{<i}$ represents the tokens generated before position $i$. 
The control attributes $A$ can represent any definable text property, and each attribute can be assigned different values based on the specific generation task and desired level of granularity.

The primary challenge in \gls{ctg} lies in effectively integrating the control attributes into the generation process while preserving the quality of the output. 
Researchers have proposed various approaches that can be broadly categorized into training-phase methods and inference-phase methods \cite{liang2024controllabletextgenerationlarge, he-etal-2022-ctrlsum}, which will be explored in detail in the following sections.

\subsection{Training-Phase Methods}
Training-phase methods focus on adapting the language model's architecture, training objective, or training data to incorporate the desired control attributes. 
These methods aim to directly encode the control attributes into the model parameters during the training process, which often results in better control and output quality compared to inference-phase methods while requiring no additional computation at runtime.
However, they may require substantial amounts of labeled data and can be less flexible in adapting to new control attributes.

\subsubsection{Retrain/Refactoring}
Retraining or refactoring methods involve either training a new model from scratch or fundamentally modifying its architecture to better accommodate specific control conditions. 
This approach is typically used when existing pre-trained models fail to meet new, more stringent control requirements or when the desired attributes are too complex to be effectively integrated using other methods.
Retraining ensures that the model intrinsically adapts at both the architectural and parameter levels to generate text that conforms to the desired control attributes, but it can be computationally expensive and time-consuming, especially for large-scale models like GPT-3 or T5.

Keskar et al. \cite{keskar2019ctrlconditionaltransformerlanguage} introduced \gls{ctrl}, a 1.63 billion parameter conditional transformer language model trained on 140GB of text data with over 50 control codes that can be used to condition the generation process on attributes such as the source domain (e.g., Wikipedia, books), website URLs, and even specific Reddit communities (e.g., r/science, r/jokes).
Each text sequence in the training data is prepended with a control code that serves as an explicit metadata tag, allowing the model to learn the association between the text and specific attributes through natural co-occurrence during training.
For instance, a control code like ``Wikipedia'' indicates that the subsequent text is in the style of a Wikipedia article, while ``Reviews Rating: 5.0'' prompts the model to generate a positive product review.
While conceptually simple and effective, training a model like \gls{ctrl} requires massive amounts of data and computational resources. 
Collecting datasets with explicit control codes for every desired attribute is often unfeasible.
CTRL may also struggle with more nuanced control, such as fine-grained sentiment or complexity levels.

FAST

Building upon the idea of CTRL, Chan et al. \cite{chan2021cocontentcontrollertextgeneration} developed Content-Conditioner (CoCon), a self-supervised framework for fine-grained controllable text generation. 
CoCon adds a content-conditioning layer to GPT-2, allowing it to generate text that incorporates specific content inputs at the word and phrase level. 
The CoCon layer is trained using a self-reconstruction objective, where the model learns to reconstruct text sequences by conditioning on content extracted from the target sequence itself, rather than relying on external control codes.
Thus, removing the need for large-scale labeled datasets.
CoCon demonstrates strong performance in controlling semantic attributes (e.g., sentiment, topic) and integrating content-level constraints.
At the same time, by keeping the original language model intact and only adding a lightweight content-conditioning module, CoCon largely preserves the diversity and fluency of the pre-trained model.

Zhang et al. \cite{zhang2020pointerconstrainedprogressivetext} proposed PrOgressive INsertion-based TransformER (POINTER), an insertion-based transformer architecture for hard-constrained text generation.
Unlike traditional left-to-right language models, POINTER generates text by progressively inserting new tokens between existing ones. 
This allows for precise control over the inclusion of specified keywords or phrases in the output. 
The model is trained from scratch on a large corpus using an insertion-based objective function. 
During inference, POINTER first inserts the target keywords into the input sequence and then fills in the remaining tokens to produce a coherent output.
POINTER offers a promising solution for generating medical text with strict lexical constraints, such as including specific medical terms or phrases. 
However, the model's reliance on keyword insertion limits its ability to control more abstract attributes like text complexity, which often depends on factors beyond vocabulary choice.

\subsubsection{Fine-tuning}
Fine-tuning is another common training-phase method, where a pre-trained language model is further trained on a smaller, task-specific dataset to learn the desired control attributes. 
Fine-tuning makes slight adjustments to the model's parameters to better align with the target task or control conditions, while aiming to preserve the general knowledge learned during pre-training. 
Compared to retraining from scratch, fine-tuning is more computationally efficient as it requires less data and computation.
However, if the dataset is too small or narrow in scope, the model may overfit and lose some of the general knowledge acquired during pre-training.

Zeldes et al. \cite{zeldes2020auxiliarytuning} proposed Auxiliary Tuning, where an auxiliary model is trained to augment a frozen pre-trained language model.
The auxiliary model takes the same input as the pre-trained model, along with additional task-specific information or control codes. It produces a set of logits that are added to the logits of the pre-trained model before applying the softmax function to obtain the final output probabilities.
Intuitively, the auxiliary model learns the residual logits needed to shift the probability distribution of the pre-trained model towards the target distribution.
There are no constraints on the auxiliary model architecture, except that it must output logits over the same vocabulary as the original model.
It is typically much smaller than the pre-trained model (e.g., a few Transformer layers) and can be trained efficiently on a small amount of data.
In experiments on keyword-conditioned generation, auxiliary tuning achieved similar performance to training from scratch, while being more parameter-efficient and avoiding the risk of catastrophic forgetting.


LiFi
FLAN
InstructCTG

\subsubsection{Reinforcement Learning}
\gls{rl} offers a more flexible and dynamic approach to \gls{ctg}, where the language model learns to optimize its output based on feedback or reward signals that indicate the quality and alignment of the generated text with the desired attributes. 
It is particularly well-suited for scenarios where the control attributes are complex, subjective, or difficult to explicitly define, such as generating text with specific styles, tones, or linguistic properties. 
RL-based methods formulate the generation process as a sequential decision-making problem, where the model learns to take actions (i.e., generate tokens) that maximize the expected reward, using algorithms such as REINFORCE \cite{williams1992reinforce}, Proximal Policy Optimization (PPO) \cite{schulman2017ppo}, Q-Learning \cite{watkins1992qlearning}, and Advantage Actor-Critic (A2C) \cite{mnih2016a3c}.

\subsection{Inference-Phase Methods}
Inference-phase methods aim to steer the generation process at inference time without changing the language model's parameters.
They dynamically manipulate the model's output probabilities or embedding space in real-time to align the generated text with the desired attributes or control conditions.
Compared to training-phase approaches, inference-phase methods are more flexible, avoiding the need for retraining or fine-tuning the model.
This makes them an alternative or complementary solution, especially for applications requiring on-the-fly output adjustments.

\subsubsection{Prompt Engineering}
Prompt engineering refers to the practice of designing input prompts that guide the language model to generate text with specific attributes or properties. 
The idea is to provide the model with explicit instructions or cues that signal the desired control conditions, such as keywords, phrases, or templates, which the model can then use to shape the output.
Prompt engineering is an effective and easy way to control the output of language models without having to change the model architecture or training data.
The main challenge is in trying to design informative yet unambiguous prompts that the model can accurately interpret and follow.
To find the right prompts, researchers often rely on human intuition, trial-and-error, and, more recently, automated methods like AutoPrompt \cite{shin2022autoprompt} and P-tuning \cite{peng2022ptuning}.

DAs (Dialogue Acts)

\subsubsection{Guided Decoding}
Guided decoding is a technique used during the decoding process of a language model to bias the generated text towards the desired control attributes. 
It works by manipulating the logits or probability distribution of the model to favor tokens that align with the control conditions.
This is often done using classifiers or reward models that provide additional supervision signals to guide the generation process.
Guided decoding approaches are often plug-and-play, meaning they can be easily integrated into existing language models without requiring extensive retraining or fine-tuning.
However, the use of external guiding models can slow down the generation process and make the text sound less natural and coherent.

PPLM
GeDi
DExperts
MUCOCO
Mix&Match
LM-Steer

\subsubsection{Latent Space Manipulation}
Latent space manipulation is a technique for controlling the attributes of machine-generated text by strategically adjusting the hidden representations within a pre-trained language model. 
These hidden representations exist in a high-dimensional space called the latent space, which can be thought of as a compressed representation of the input text, capturing its essential features and characteristics.
Similar words, phrases, and concepts are clustered together in this space, allowing for semantic relationships to be encoded in the form of geometric distances and directions between points.
% continue this text, including the term "guiding vector"
By identifying specific directions or vectors in this latent space, 

By identifying and extracting these semantic patterns, we can create guiding vectors that represent specific attributes or styles in the latent space.
Common approaches to find these guiding vectors include ...
Once identified, these guiding vectors can be used to steer the model's hidden representations during text generation, effectively pushing the output toward or away from certain attributes.
The magnitude of the vector determines how strongly the attribute is expressed in the text, allowing for fine-grained and continuous control over the output.

GENhance
ICV
MIRACLE

There are also other methods for \gls{ctg} that do not fit into any of the aforementioned categories.

\section{Language Complexity}
\label{c2:s:linguistic-complexity}

The complexity of biomedical terminology represents a unique challenge for language complexity measurement. 
Medical terms often combine elements from multiple languages, primarily Greek and Latin, creating systematic patterns that affect both structural and cognitive complexity. 
These patterns follow predictable rules but can create significant processing challenges for non-expert readers.

The relationship between professional and lay terminology adds another layer of complexity. 
Many medical concepts can be expressed through either technical or lay terms (e.g., ``myocardial infarction'' vs. ``heart attack''), creating parallel vocabularies that must be considered in complexity measurement. 

% The title should be either "Statistical Readability Metrics" or "Traditional Readability Metrics"
\subsection{Traditional Readability Metrics}

Readability formulas are designed to estimate the difficulty of a text based on surface-level features such as average sentence length, average word length, and the proportion of difficult or unfamiliar words. While originally developed for general-domain texts, some of these metrics have been applied to biomedical literature to assess the accessibility and suitability of patient education materials, consent forms, and other health-related documents. For example, researchers have used these metrics to evaluate the readability of online patient education materials for conditions such as nocturnal enuresis \cite{Fung2024-uh}, bariatric surgery \cite{Lucy2023-zi}, and female pelvic floor disorders \cite{Varli2023-ma}. Moreover, these metrics have been used to assess the readability of discharge instructions for heart failure patients \cite{Tuan2023-wc} and to analyze the quality of information provided by intensive and critical care societies \cite{Hanci2024-wv}.

The most popular readability metric is the \gls{fkgl}. This formula estimates the U.S. grade level required to understand a given text, calculated as follows:
\begin{equation}
    \label{eq:fkgl}
    FKGL = 0.39 \times \left(\frac{\text{words}}{\text{sentences}}\right) + 11.8 \times \left(\frac{\text{syllables}}{\text{words}}\right) - 15.59
\end{equation}
\gls{fkgl} scores typically vary from 0 to 18, with higher scores indicating more difficult text. For example, a score of 9.2 would suggest that the text is suitable for an average 9th-grade student. Text that scores above 12 suggest college-level or domain-specific expertise.

Another commonly used readability formula is the \gls{smog}. 
This metric estimates the years of education needed to understand a piece of writing based on the number of polysyllabic words (i.e., those with three or more syllables) in a sample of 30 sentences, using the following formula:
\begin{equation}
    \label{eq:smog}
    SMOG = 1.0430 \times \sqrt{\text{polysyllables} \times \left(\frac{30}{\text{sentences}}\right)} + 3.1291
\end{equation}
Like \gls{fkgl}, \gls{smog} scores correspond to U.S. grade levels. 
The \gls{smog} formula has gained popularity in healthcare settings due to its relative ease of use and focus on vocabulary complexity. Its widespread use in healthcare is supported by a 2010 study published in the Journal of the Royal College of Physicians of Edinburgh, which recommended \gls{smog} as the preferred measure for evaluating consumer-oriented healthcare material \cite{Fitzsimmons2010-mq}.

The \gls{dcrs} is another well-known readability metric that assesses text difficulty based on the average sentence length and the percentage of ``difficult'' words not found on a list of 3,000 familiar words. The original \gls{dcrs} formula is:
\begin{equation}
    \label{eq:dcrs}
    DCRS = 0.1579 \times \left(\frac{\text{difficult words}}{\text{total words}} \times 100\right) + 0.0496 \times \left(\frac{\text{total words}}{\text{total sentences}}\right)
\end{equation}
If the percentage of difficult words exceeds 5\%, an additional constant of 3.6365 is added to the raw score to get the final \gls{dcrs}. Scores range from 4.9 or below for easily understood text to 10 or above for very challenging text.

In addition to \gls{fkgl}, \gls{smog}, and \gls{dcrs}, there are many other readability formulas, such as the \gls{ari}, \gls{cli}, \gls{gfi}, and Linsear Write Formula. While the specific calculations differ, they all aim to estimate text difficulty based on factors like word length, sentence length, and syllable counts.
Although traditional readability formulas are still widely used, they have several notable limitations when applied to biomedical texts. 
First, these formulas rely on surface-level features like word and sentence length, which may not adequately capture the conceptual complexity of medical information \cite{Crossley2022, WANG2013503, Singh2024}.
They treat the text as a ``bag of words'', ignoring higher-level discourse structures that influence comprehension, such as information density, organization, coherence, and syntax.  
Second, readability formulas often treat all words equally based on length or syllable count, without considering the cognitive load imposed by specific terms \cite{Swanson2024}. 
Polysyllabic medical terms may be familiar to experts but challenging for lay readers. Conversely, short words like ``apnea'' or ``polyp'' may also be difficult for average adults to understand.
Third, these formulas were developed using general reading materials, often geared towards children's education levels. 
The assumptions and thresholds used in these formulas may not generalize well to the specialized language and adult literacy levels of biomedical texts \cite{Crossley2022}.
Finally, readability scores are typically reported as a single average or grade level for an entire text, obscuring any variability in difficulty within the document.
Despite these drawbacks, readability formulas provide an objective, quantitative measure of text complexity that can serve as a starting point for improving the clarity and accessibility of health-related materials. 
However, experts generally recommend using these formulas as part of a more holistic approach, combining multiple metrics with expert evaluation and user testing to ensure that health information is truly accessible and understandable for the intended audience \cite{Ko2024-dd, tanprasert-kauchak-2021-flesch}.

\subsection{Reference-Based Evaluation Metrics}

Reference-based evaluation metrics compare machine-generated text simplifications to one or more predefined gold-standard references.
The core idea is to measure how closely the generated text aligns with the references in terms of content, structure, and style.

The most widely adopted metric in the text simplification community might be the \gls{sari} \cite{xu-etal-2016-optimizing}. 
\gls{sari} evaluates the quality of a simplified text by comparing it to multiple reference simplifications and the original complex sentence. 
It computes the arithmetic mean of three sub-scores: 1) the F1 score for n-grams present in the output and the references but not in the input (addition), 2) the F1 score for n-grams in the output and input but not the references (copying), and 3) the F1 score for n-grams in the input and references but not the output (deletion). 
Considering these three operations, \gls{sari} rewards simplifications that add simplified n-grams, preserve important n-grams from the input, and delete unimportant n-grams. 
In the biomedical domain, \gls{sari} has been shown to be a more reliable metric for assessing text quality than other n-gram-based metrics like BLEU and ROUGE \cite{li2024largelanguagemodelsbiomedical}.
However, since it was originally designed to evaluate lexical (word-level) changes, it may not adequately measure how well a system preserves meaning or reduces complexity when evaluating multi-operation simplifications, such as rephrasing sentences or altering sentence structure \cite{alva-manchego-etal-2021-un}.
It also struggles with correctly identifying and rewarding appropriate lexical replacements in such scenarios.

BERTScore \cite{zhang2020bertscoreevaluatingtextgeneration} is another popular metric that uses contextualized word embeddings from the BERT language model to compute semantic similarity between the candidate and reference simplifications. 
It aligns each token in the candidate text with the most similar token in the reference, and then sums these cosine similarities to calculate precision and recall, with the F1 score computed as their harmonic mean. 
BERTScore has demonstrated good performance in various text generation tasks, including machine translation \cite{Vetrov2022ANA, Tang2024ImprovingBF}. 
In the context of text simplification, BERTScore has shown the highest correlations with human judgments of simplicity and meaning preservation compared to other metrics, particularly when evaluating the outputs of neural sequence-to-sequence models \cite{alva-manchego-etal-2021-un}, though it is unclear how well it generalizes to the biomedical domain. 
It does not explicitly measure readability aspects such as lexical or syntactic complexity \cite{li2024largelanguagemodelsbiomedical}. 
Furthermore, BERTScore's greedy alignment approach may allow system outputs to receive excess credit relative to the reference \cite{jin-gildea-2022-rewarding}.

\gls{rouge} \cite{xu-etal-2016-optimizing}, a family of metrics originally developed for text summarization, has also been adapted for text simplification.
\gls{rouge} measures the n-gram overlap between the simplified text and the references, with higher scores indicating greater lexical similarity.
Different \gls{rouge} variants focus on various aspects of n-gram matching:
\begin{itemize}
    \item \gls{rouge}-N calculates the recall based on the ratio of matching n-grams (e.g., unigrams for \gls{rouge}-1, bigrams for \gls{rouge}-2) to total reference n-grams, but it does not account for the order or contiguity of the matches. 
    \item \gls{rouge}-L addresses this by considering the precision, recall, and F-measure of the largest common subsequence, which preserves order but allows gaps. 
    \item \gls{rouge}-S offers a more flexible matching by measuring the overlap of skip-bigrams, capturing both local and long-range sentence structure similarities. 
    \item \gls{rouge}-W is a weighted version of \gls{rouge}-L that assigns higher importance to consecutive matches, favoring longer common subsequences.
\end{itemize}
These \gls{rouge} variants can capture lexical and structural similarities at different granularities, but they do not explicitly measure semantic similarity, potentially missing differences in meaning between the original and simplified texts, and do not account for readability factors such as vocabulary or syntax, which are crucial for text simplification \cite{xu2024reasoningcomparisonllmenhancedsemantic}.
Ganesan \cite{ganesan2018rouge20updatedimproved} addressed some of these shortcomings in \gls{rouge} 2.0, which improves upon the original metric by incorporating WordNet-based synonym matching and allowing evaluation of specific topics through \gls{pos} based filtering. Similarly, Zhang et al. \cite{ZHANG2024121364} proposed \gls{rouge}-SEM, which combines \gls{rouge} with a Siamese-BERT network to measure semantic similarity and uses back-translation to rewrite texts that are semantically similar but lexically different.
Despite that, other metrics specifically targeting readability aspects may still be necessary for a more well-rounded evaluation of simplification quality \cite{li2024largelanguagemodelsbiomedical}.

Other reference-based metrics that have been used for text simplification include the \gls{bleu}, which calculates the precision of n-gram matches between the candidate and reference, with a brevity penalty \cite{papineni-etal-2002-bleu}; \gls{meteor}, which computes a weighted F-score based on exact, stem, synonym, and paraphrase matches between the candidate and reference \cite{banerjee-lavie-2005-meteor}; and \gls{ter}, which measures the minimum number of edits (insertions, deletions, substitutions, and shifts) required to transform the candidate text into the reference \cite{snover-etal-2006-study}.
All reference-based metrics require high-quality reference simplifications, which may be difficult and expensive to obtain, especially for large biomedical corpora \cite{alva-manchego-etal-2021-un}. 
They also assume that the references represent the only valid simplifications, penalizing alternative paraphrases that may be equally fluent and meaningful \cite{sottana2023evaluationmetricseragpt4, huang-kochmar-2024-referee, lyu2024scigispynovelmetricbiomedical}. 
Furthermore, these metrics do not directly measure the readability or accessibility of the simplified text for the target audience, only its similarity to the references.

\subsection{Reference-Free Evaluation Metrics}

Recently, there has been growing interest in developing reference-free (context-based) evaluation metrics for text simplification that can assess output quality without relying on human-written reference simplifications. 
Reference-free metrics are appealing because collecting high-quality human reference simplifications is time-consuming, expensive, and may not always be feasible \cite{alva-manchego-etal-2021-un, 94205144dd7945cc99b5a6544451b668, deutsch-etal-2022-limitations}.

One such metric is \gls{samsa} \cite{sulem-etal-2018-semantic} that uses the semantic annotations in the \gls{ucca} framework to measure the structural simplicity of a text. 
It stipulates that an optimal simplification should assign each event in the input sentence to its own output sentence. 
The metric then computes the extent to which this is achieved by comparing the semantic parse of the original and simplified texts. 
By focusing on meaning-preserving structural changes, \gls{samsa} addresses some of the limitations of surface-level readability formulas and n-gram based metrics. 
However, it is designed primarily for sentence splitting and does not assess other simplification operations like lexical substitution or paraphrasing.

More recently, several metrics have been proposed that leverage large pre-trained language models to evaluate simplification quality. 
Maddela et al. \cite{maddela-etal-2023-lens} introduced \gls{lens}, a metric trained on human ratings of simplification to predict overall quality. 
It uses the RoBERTa model to encode the original and simplified sentences, and a new component to weigh the similarity of the simplified sentence to different possible references. 
\gls{lens} has shown high correlation with human judgments on both overall and aspect-level simplification quality.
Building on this work, Huang and Kochmar \cite{huang-kochmar-2024-referee} proposed REFeREE, which incorporates a curriculum learning approach with synthetic data to further improve performance.

\gls{siera} \cite{yamanaka-tokunaga-2024-siera} is another reference-free metric specifically designed for sentence simplification. 
It trains a ranking model to determine the relative simplicity between sentence pairs and uses a data augmentation technique to expand the training data with simplification edit operations. 
In experiments, \gls{siera} outperformed reference-based and traditional readability metrics on evaluating sentence simplification.

\gls{salsa} \cite{heineman-etal-2023-dancing} takes a different approach by combining error and quality evaluation into an edit-based reference-free framework. 
It defines 21 edit types covering lexical, syntactic and conceptual simplification operations, and uses human annotation to train a model to predict sentence-level quality from these fine-grained edits. 
\gls{salsa} provides an interpretable way to identify specific strengths and weaknesses of simplification systems. 
The authors also used the \gls{salsa} annotations to train an improved version of \gls{lens} called \gls{lens}-\gls{salsa} that can predict both overall and fine-grained edit-level quality.

Other reference-free metrics include QuestEval \cite{scialom-etal-2021-rethinking}, which uses a question-answering approach to measure information preservation, and \gls{sle} \cite{kriz-etal-2020-simple} which evaluates relative readability improvements using contrastive texts.

Other reference-free approaches include using pre-trained language models to estimate the probability or perplexity of the simplified text \cite{ke-etal-2022-ctrleval}, calculating readability scores based on lexical and syntactic features \cite{Truica2023-au}, and measuring the text's similarity to a corpus of simple or complex language \cite{Truica2023-au}.
However, reference-free metrics also have some limitations. They may not capture all aspects of simplification quality, such as information preservation or target audience appropriateness. They also require careful selection and validation of features or training data to ensure their relevance and generalizability to the biomedical domain.