@article{atherton2023readability,
  title={Readability of online health information pertaining to migraine and headache in the UK},
  author={Atherton, Kate and Forshaw, Mark J and Kidd, Tara M},
  journal={British Journal of Pain},
  volume={17},
  number={2},
  pages={117--125},
  year={2023},
  publisher={},
  url={https://doi.org/10.1177/20494637221134461},
  pmid={37057254},
  pmcid={PMC10088424},
  issn={2049-4637},
  month=apr,
}

@UNPUBLISHED{Tuan2023-wc,
  title    = "Using machine learning to improve the readability of hospital
              discharge instructions for heart failure",
  author   = "Tuan, Alyssa W and Cannon, Nathan and Foley, David and Gupta,
              Neha and Park, Christian and Chester-Paul, Kyra and Bhasker,
              Joanna and Pearson, Cara and Amarnani, Avisha and High, Zachary
              and Kraschnewski, Jennifer and Shah, Ravi",
  abstract = "AbstractBackgroundLow health literacy is associated with poor
              health outcomes. Hospital discharge instructions are often
              written at advanced reading levels, limiting patients' with low
              health literacy ability to follow medication instructions or
              complete other necessary care. Previous research demonstrates
              that improving the readability of discharge instructions reduces
              hospital readmissions and decreases healthcare costs. We aimed to
              use artificial intelligence (AI) to improve the readability of
              discharge instructions.Methodology/Principal FindingsWe collected
              a series of discharge instructions for adults hospitalized for
              heart failure (n=423), which were then manually simplified to a
              lower reading level to create two parallel sets of discharge
              instructions. Only 343 sets were then processed via AI-based
              machine learning to create a trained algorithm. We then tested
              the algorithm on the remaining 80 discharge instructions. Output
              was evaluated quantitatively using Simple Measure of Gobbledygook
              (SMOG) and Flesch-Kincaid readability scores and cross-entropy
              analysis and qualitatively. Using this test dataset (n=80), the
              average reading levels were: original discharge instructions
              (SMOG: 10.5669$\pm$1.2634, Flesch-Kincaid: 8.6038$\pm$1.5509),
              human-simplified instructions (SMOG: 9.4406$\pm$1.0791,
              Flesch-Kincaid: 7.2221$\pm$1.3794), and AI-simplified
              instructions (SMOG: 9.3045$\pm$0.9531, Flesch-Kincaid:
              7.0464$\pm$1.1308). AI-simplified instructions were significantly
              different from original instructions
              (pConclusions/SignificanceThe AI-based algorithm learned
              meaningful phrase-level simplifications from the human-simplified
              discharge instructions. The AI simplifications, while not in
              complete agreement with the human simplifications, do appear as
              statistically significant improvements to SMOG and Flesch-Kincaid
              reading levels. The algorithm will likely produce more meaningful
              and concise simplifications among discharge instructions as it is
              trained on more data. This study demonstrates an important
              opportunity for AI integration into healthcare delivery to
              address health disparities related to limited health literacy and
              potentially improve patient health.Author summaryPatient-facing
              materials are often written at too high of a reading level for
              patients, such as hospital discharge instructions. These
              instructions provide critical information on how to control
              health conditions, take medications, and attend follow-up visits.
              Difficulty understanding these instructions could lead to the
              patient returning to the hospital if they do not understand how
              to control their health condition.Improving the readability of
              discharge instructions can reduce hospital readmissions. It may
              improve health outcomes for patients and reduce healthcare costs.
              Artificial intelligence (AI) may be used to improve the reading
              level of patient-facing materials. Our work aims to create a tool
              that can accomplish this goal.We obtained hospital discharge
              instructions for heart failure. Discharge instructions were
              edited by medical experts to improve their readability. This
              created two sets of discharge instructions that were processed
              using AI. We created and tested an AI tool to automatically
              simplify discharge instructions. Although not perfect, we found
              that the tool was successful. This research shows that AI can be
              used to address health literacy needs within health care by
              making patient-facing health materials easier to understand. This
              is important to empower all patients to take action to improve
              their health.",
  journal  = "medRxiv",
  month    =  jun,
  year     =  2023
}

@ARTICLE{Varli2023-ma,
  title     = "Evaluation of readability levels of online patient education
               materials for female pelvic floor disorders",
  author    = "Varli, Bulut and Cetindag, Elif Nazli and Koyuncu Demir, Kazibe
               and Coban, Ulas and Islamova, Gunel and Dokmeci, Fulya",
  abstract  = "Most women hesitate to seek help from healthcare providers as
               they find it difficult to share complaints of involuntary
               leakage or vaginal prolapse. Hence, they often refer to the
               websites of national and/or international bodies' patient
               education materials (PEMs), which are considered the most
               reliable sources. The crucial factor that determines their
               usefulness is their readability level, which makes them ``easy''
               or ``difficult'' to read, and is recommended, not to exceed the
               sixth grade level. In this study, we aimed to assess the
               readability levels of Turkish translated PEMs from the websites
               of the International Urogynecological Association and the
               European Association of Urology and the PEMs originally written
               in Turkish from the website of the Society of Urological Surgery
               in Turkey. All the PEMs (n = 52) were analyzed by online
               calculators using the Atesman formula, Flesch-Kincaid grade
               level, and Gunning Fog index. The readability parameters, number
               of sentences, words, letters, syllables, and readability
               intervals of these methods were compared among the groups using
               the Kruskal-Wallis test, or ANOVA test, with post hoc
               comparisons where appropriate. The readability level of all PEMs
               is at least at an ``averagely difficult'' interval, according to
               both assessment methods. No significant differences were found
               among the PEM groups in terms of readability parameters and
               assessment methods (P > .05). Whether original or translated,
               international or national societies' PEMs' readability scores
               were above the recommended level of sixth grade. Thus, the
               development of PEMs needs to be revised accordingly by relevant
               authorities.",
  journal   = "Medicine (Baltimore)",
  publisher = "Ovid Technologies (Wolters Kluwer Health)",
  volume    =  102,
  number    =  52,
  pages     = "e36636",
  month     =  dec,
  year      =  2023,
  language  = "en"
}

@article{Crossley2022,
  title = {A large-scaled corpus for assessing text readability},
  volume = {55},
  ISSN = {1554-3528},
  url = {http://dx.doi.org/10.3758/s13428-022-01802-x},
  DOI = {10.3758/s13428-022-01802-x},
  number = {2},
  journal = {Behavior Research Methods},
  publisher = {Springer Science and Business Media LLC},
  author = {Crossley,  Scott and Heintz,  Aron and Choi,  Joon Suh and Batchelor,  Jordan and Karimi,  Mehrnoush and Malatinszky,  Agnes},
  year = {2022},
  month = mar,
  pages = {491–507}
}

@article{Singh2024,
  title = {Readability Metrics in Patient Education: Where Do We Innovate?},
  volume = {14},
  ISSN = {2039-7283},
  url = {http://dx.doi.org/10.3390/clinpract14060183},
  DOI = {10.3390/clinpract14060183},
  number = {6},
  journal = {Clinics and Practice},
  publisher = {MDPI AG},
  author = {Singh,  Som and Jamal,  Aleena and Qureshi,  Fawad},
  year = {2024},
  month = nov,
  pages = {2341–2349}
}

@article{Swanson2024,
  title = {Biomedical text readability after hypernym substitution with fine-tuned large language models},
  volume = {3},
  ISSN = {2767-3170},
  url = {http://dx.doi.org/10.1371/journal.pdig.0000489},
  DOI = {10.1371/journal.pdig.0000489},
  number = {4},
  journal = {PLOS Digital Health},
  publisher = {Public Library of Science (PLoS)},
  author = {Swanson,  Karl and He,  Shuhan and Calvano,  Josh and Chen,  David and Telvizian,  Talar and Jiang,  Lawrence and Chong,  Paul and Schwell,  Jacob and Mak,  Gin and Lee,  Jarone},
  editor = {Tariq,  Amara},
  year = {2024},
  month = apr,
  pages = {e0000489}
}

@article{WANG2013503,
title = {Assessing readability formula differences with written health information materials: Application, results, and recommendations},
journal = {Research in Social and Administrative Pharmacy},
volume = {9},
number = {5},
pages = {503-516},
year = {2013},
issn = {1551-7411},
doi = {https://doi.org/10.1016/j.sapharm.2012.05.009},
url = {https://www.sciencedirect.com/science/article/pii/S1551741112000770},
author = {Lih-Wern Wang and Michael J. Miller and Michael R. Schmitt and Frances K. Wen},
}

@ARTICLE{Fitzsimmons2010-mq,
  title     = "A readability assessment of online Parkinson's disease
               information",
  author    = "Fitzsimmons, P R and Michael, B D and Hulley, J L and Scott, G O",
  abstract  = "BACKGROUND: Patients increasingly use the internet to access
               health information. Inadequate health literacy is common and
               frequently limits patient comprehension of healthcare
               literature. We aimed to assess the readability of online
               consumer-orientated Parkinson's disease (PD) information using
               two validated measures. METHOD: We identified the 100 highest
               ranked consumer-orientated PD webpages and determined webpage
               readability using the Flesch-Kincaid and Simple Measure Of
               Gobbledygook (SMOG) formulae. RESULTS: None of the webpages
               analysed complied with current readability guidelines.
               Commercial websites were significantly easier to read (p =
               0.035). The Flesch-Kincaid formula significantly underestimated
               reading difficulty (p < 0.0001). Ease of reading correlated
               weakly with search engine ranking (r = 0.35, p = 0.0004).
               CONCLUSIONS: Only 1\% of the top 100 PD information webpages are
               fully comprehensible to the average adult. Simple Measure Of
               Gobbledygook should be the preferred measure of webpage
               readability. Parkinson's disease information websites require
               major text revision to comply with readability guidelines and to
               be comprehensible to the average patient.",
  journal   = "J. R. Coll. Physicians Edinb.",
  publisher = "SAGE Publications",
  volume    =  40,
  number    =  4,
  pages     = "292--296",
  month     =  dec,
  year      =  2010,
  language  = "en"
}


@ARTICLE{Wang2013-hu,
  title     = "Assessing readability formula differences with written health
               information materials: application, results, and recommendations",
  author    = "Wang, Lih-Wern and Miller, Michael J and Schmitt, Michael R and
               Wen, Frances K",
  abstract  = "BACKGROUND: Readability formulas are often used to guide the
               development and evaluation of literacy-sensitive written health
               information. However, readability formula results may vary
               considerably as a result of differences in software processing
               algorithms and how each formula is applied. These variations
               complicate interpretations of reading grade level estimates,
               particularly without a uniform guideline for applying and
               interpreting readability formulas. OBJECTIVES: This research
               sought to (1) identify commonly used readability formulas
               reported in the health care literature, (2) demonstrate the use
               of the most commonly used readability formulas on written health
               information, (3) compare and contrast the differences when
               applying common readability formulas to identical selections of
               written health information, and (4) provide recommendations for
               choosing an appropriate readability formula for written
               health-related materials to optimize their use. METHODS: A
               literature search was conducted to identify the most commonly
               used readability formulas in health care literature. Each of the
               identified formulas was subsequently applied to word samples
               from 15 unique examples of written health information about the
               topic of depression and its treatment. Readability estimates
               from common readability formulas were compared based on text
               sample size, selection, formatting, software type, and/or hand
               calculations. Recommendations for their use were provided.
               RESULTS: The Flesch-Kincaid formula was most commonly used
               (57.42\%). Readability formulas demonstrated variability up to 5
               reading grade levels on the same text. The Simple Measure of
               Gobbledygook (SMOG) readability formula performed most
               consistently. Depending on the text sample size, selection,
               formatting, software, and/or hand calculations, the individual
               readability formula estimated up to 6 reading grade levels of
               variability. CONCLUSIONS: The SMOG formula appears best suited
               for health care applications because of its consistency of
               results, higher level of expected comprehension, use of more
               recent validation criteria for determining reading grade level
               estimates, and simplicity of use. To improve interpretation of
               readability results, reporting reading grade level estimates
               from any formula should be accompanied with information about
               word sample size, location of word sampling in the text,
               formatting, and method of calculation.",
  journal   = "Res. Social Adm. Pharm.",
  publisher = "Elsevier BV",
  volume    =  9,
  number    =  5,
  pages     = "503--516",
  month     =  sep,
  year      =  2013,
  keywords  = "Health literacy; Readability; Readability formula",
  language  = "en"
}

@ARTICLE{Hanci2024-wv,
  title     = "Assessment of the readability of the online patient education
               materials of intensive and Critical Care societies",
  author    = "Hanci, Volkan and Otlu, B{\"u}{\c s}ra and Biyiko{\u g}lu, Ali
               Salih",
  abstract  = "OBJECTIVES: This study aimed to evaluate the readability of
               patient education materials (PEMs) on websites of intensive and
               critical care societies. DATA SOURCES: Websites of intensive and
               critical care societies, which are members of The World
               Federation of Intensive and Critical Care and The European
               Society of Intensive Care Medicine. SETTING: Cross-sectional
               observational, internet-based, website, PEMs, readability study.
               STUDY SELECTION: The readability of the PEMs available on
               societies' sites was evaluated. DATA EXTRACTION: The readability
               formulas used were the Flesch Reading Ease Score (FRES),
               Flesch-Kincaid Grade Level (FKGL), Simple Measure of
               Gobbledygook (SMOG), and Gunning Fog (GFOG). DATA SYNTHESIS: One
               hundred twenty-seven PEM from 11 different societies were
               included in our study. In the readability analysis of PEM, the
               FRES was 58.10 (48.85-63.77) (difficult), the mean FKGL and SMOG
               were 10.19 (8.93-11.72) and 11.10 (10.11-11.87) years,
               respectively, and the mean GFOG score was 12.73 (11.37-14.15)
               (very difficult). All readability formula results were
               significantly higher than the recommended sixth-grade level ( p
               < 0.001). All PEMs were above the sixth-grade level when the
               societies were evaluated individually according to all
               readability results ( p < 0.05). CONCLUSIONS: Compared with the
               sixth-grade level recommended by the American Medical
               Association and the National Institutes of Health, the
               readability of PEMs in intensive and critical care societies is
               relatively high. PEMs in intensive and critical care societies
               should be prepared with attention to recommendations on
               readability.",
  journal   = "Crit. Care Med.",
  publisher = "Ovid Technologies (Wolters Kluwer Health)",
  volume    =  52,
  number    =  2,
  pages     = "e47--e57",
  month     =  feb,
  year      =  2024,
  language  = "en"
}

@ARTICLE{Wu2023-fl,
  title     = "{ChatGPT}: is it good for our glaucoma patients?",
  author    = "Wu, Gloria and Lee, David A and Zhao, Weichen and Wong, Adrial
               and Sidhu, Sahej",
  abstract  = "Purpose: Our study investigates ChatGPT and its ability to
               communicate with glaucoma patients. Methods: We inputted eight
               glaucoma-related questions/topics found on the American Academy
               of Ophthalmology (AAO)'s website into ChatGPT. We used the
               Flesch-Kincaid test, Gunning Fog Index, SMOG Index, and
               Dale-Chall readability formula to evaluate the comprehensibility
               of its responses for patients. ChatGPT's answers were compared
               with those found on the AAO's website. Results: ChatGPT's
               responses required reading comprehension of a higher grade level
               (average = grade 12.5 $\pm$ 1.6) than that of the text on the
               AAO's website (average = 9.4 grade $\pm$ 3.5), (0.0384). For the
               eight responses, the key ophthalmic terms appeared 34 out of 86
               times in the ChatGPT responses vs. 86 out of 86 times in the
               text on the AAO's website. The term ``eye doctor'' appeared once
               in the ChatGPT text, but the formal term ``ophthalmologist'' did
               not appear. The term ``ophthalmologist'' appears 26 times on the
               AAO's website. The word counts of the answers produced by
               ChatGPT and those on the AAO's website were similar (p = 0.571),
               with phrases of a homogenous length. Conclusion: ChatGPT trains
               on the texts, phrases, and algorithms inputted by software
               engineers. As ophthalmologists, through our websites and
               journals, we should consider encoding the phrase ``see an
               ophthalmologist''. Our medical assistants should sit with
               patients during their appointments to ensure that the text is
               accurate and that they fully comprehend its meaning. ChatGPT is
               effective for providing general information such as definitions
               or potential treatment options for glaucoma. However, ChatGPT
               has a tendency toward repetitive answers and, due to their
               elevated readability scores, these could be too difficult for a
               patient to read.",
  journal   = "Front. Ophthalmol. (Lausanne)",
  publisher = "Frontiers Media SA",
  volume    =  3,
  pages     = "1260415",
  month     =  nov,
  year      =  2023,
  keywords  = "ChatGPT; artificial intelligence; glaucoma; ophthalmology;
               patient education",
  copyright = "https://creativecommons.org/licenses/by/4.0/",
  language  = "en"
}

@ARTICLE{Lucy2023-zi,
  title    = "Readability of patient education materials for bariatric surgery",
  author   = "Lucy, Adam Timothy and Rakestraw, Stephanie L and Stringer,
              Courtney and Chu, Daniel and Grams, Jayleen and Stahl, Richard
              and Mustian, Margaux N",
  abstract = "INTRODUCTION: Bariatric surgery is a successful treatment for
              obesity, but barriers to surgery exist, including low health
              literacy. National organizations recommend patient education
              materials (PEM) not exceed a sixth-grade reading level. Difficult
              to comprehend PEM can exacerbate barriers to bariatric surgery,
              especially in the Deep South where high obesity and low literacy
              rates exist. This study aimed to assess and compare the
              readability of webpages and electronic medical record (EMR)
              bariatric surgery PEM from one institution. METHODS: Readability
              of online bariatric surgery and standardized perioperative EMR
              PEM were analyzed and compared. Text readability was assessed by
              validated instruments: Flesch Reading Ease Formula (FRE), Flesch
              Kincaid Grade Level (FKGL), Gunning Fog (GF), Coleman-Liau Index
              (CL), Simple Measure of Gobbledygook (SMOG), Automated
              Readability Index (ARI), and Linsear Write Formula (LWF). Mean
              readability scores were calculated with standard deviations and
              compared using unpaired t-tests. RESULTS: 32 webpages and seven
              EMR education documents were analyzed. Webpages were overall
              ``difficult to read'' compared to ``standard/average''
              readability EMR materials (mean FRE 50.5 $\pm$ 18.3 vs. 67.4
              $\pm$ 4.2, p = 0.023). All webpages were at or above high school
              reading level: mean FKGL 11.8 $\pm$ 4.4, GF 14.0 $\pm$ 3.9, CL
              9.5 $\pm$ 3.2, SMOG 11.0 $\pm$ 3.2, ARI 11.7 $\pm$ 5.1, and LWF
              14.9 $\pm$ 6.6. Webpages with highest reading levels were
              nutrition information and lowest were patient testimonials. EMR
              materials were sixth to ninth grade reading level: FKGL 6.2 $\pm$
              0.8, GF 9.3 $\pm$ 1.4, CL 9.7 $\pm$ 0.9, SMOG 7.1 $\pm$ 0.8, ARI
              6.1 $\pm$ 1.0, and LWF 5.9 $\pm$ 0.8. CONCLUSION: Surgeon curated
              bariatric surgery webpages have advanced reading levels above
              recommended thresholds compared to standardized PEM from an EMR.
              This readability gap may unintentionally contribute to barriers
              to surgery and affect postoperative outcomes. Streamlined efforts
              are needed to create materials that are easier to read and comply
              with recommendations.",
  journal  = "Surg. Endosc.",
  volume   =  37,
  number   =  8,
  pages    = "6519--6525",
  month    =  aug,
  year     =  2023,
  keywords = "Bariatric surgery; Health literacy; Obesity; Readability",
  language = "en"
}


@ARTICLE{Fung2024-uh,
  title    = "Internet health resources on nocturnal enuresis: A readability,
              quality, and accuracy analysis",
  author   = "Fung, Adrian C H and Lee, Matthew H L and Leung, Jessie L and
              Chan, Ivy H Y and Wong, Kenneth K Y",
  abstract = "INTRODUCTION: Nocturnal enuresis is a common yet
              quality-of-life-limiting pediatric condition. There is an
              increasing trend for parents to obtain information on the
              disease's nature and treatment options via the internet. However,
              the quality of health-related information on the internet varies
              greatly and is largely uncontrolled and unregulated. With this
              study, a readability, quality, and accuracy evaluation of the
              health information regarding nocturnal enuresis is carried out.
              MATERIALS AND METHODS: A questionnaire was administered to
              parents and patients with nocturnal enuresis to determine their
              use of the internet to research their condition. The most common
              search terms were determined, and the first 30 websites returned
              by the most popular search engines were used to assess the
              quality of information about nocturnal enuresis. Each site was
              categorized by type and assessed for readability using the
              Gunning fog score, Simple Measure of Gobbledygook (SMOG) index,
              and Dale-Chall score; for quality using the DISCERN score; and
              for accuracy by comparison to the International Children's
              Continence Society guidelines by three experienced pediatric
              urologists and nephrologists. RESULTS: A total of 30 websites
              were assessed and classified into five categories: professional
              (n = 13), nonprofit (n = 8), commercial (n = 4), government (n =
              3), and other (n = 2). The information was considered difficult
              for the public to comprehend, with mean Gunning fog, SMOG index,
              and Dale-Chall scores of 12.1 $\pm$ 4.3, 14.1 $\pm$ 4.3, and 8.1
              $\pm$ 1.3, respectively. The mean summed DISCERN score was 41
              $\pm$ 11.6 out of 75. Only seven (23\%) websites were considered
              of good quality (DISCERN score > 50). The mean accuracy score of
              the websites was 3.2 $\pm$ 0.6 out of 5. Commercial websites were
              of the poorest quality and accuracy. Websites generally scored
              well in providing their aims and identifying treatment benefits
              and options, while they lacked references and information
              regarding treatment risks and mechanisms. CONCLUSION: Online
              information about nocturnal enuresis exists for parents; however,
              most websites are of suboptimal quality, readability, and
              accuracy. Pediatric surgeons should be aware of parents'
              health-information-seeking behavior and be proactive in guiding
              parents to identify high-quality resources.",
  journal  = "Eur. J. Pediatr. Surg.",
  volume   =  34,
  number   =  1,
  pages    = "84--90",
  month    =  feb,
  year     =  2024,
  language = "en"
}
