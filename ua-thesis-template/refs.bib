@article{atherton2023readability,
  title={Readability of online health information pertaining to migraine and headache in the UK},
  author={Atherton, Kate and Forshaw, Mark J and Kidd, Tara M},
  journal={British Journal of Pain},
  volume={17},
  number={2},
  pages={117--125},
  year={2023},
  publisher={},
  url={https://doi.org/10.1177/20494637221134461},
  pmid={37057254},
  pmcid={PMC10088424},
  issn={2049-4637},
  month=apr,
}

@inproceedings{Chen_2019,
   title={BioSentVec: creating sentence embeddings for biomedical texts},
   url={http://dx.doi.org/10.1109/ICHI.2019.8904728},
   DOI={10.1109/ichi.2019.8904728},
   booktitle={2019 IEEE International Conference on Healthcare Informatics (ICHI)},
   publisher={IEEE},
   author={Chen, Qingyu and Peng, Yifan and Lu, Zhiyong},
   year={2019},
   month=jun 
}

@inproceedings{lester-etal-2021-power,
    title = "The Power of Scale for Parameter-Efficient Prompt Tuning",
    author = "Lester, Brian  and
      Al-Rfou, Rami  and
      Constant, Noah",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.243/",
    doi = "10.18653/v1/2021.emnlp-main.243",
    pages = "3045--3059",
    abstract = "In this work, we explore {\textquotedblleft}prompt tuning,{\textquotedblright} a simple yet effective mechanism for learning {\textquotedblleft}soft prompts{\textquotedblright} to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are learned through backpropagation and can be tuned to incorporate signals from any number of labeled examples. Our end-to-end learned approach outperforms GPT-3`s few-shot learning by a large margin. More remarkably, through ablations on model size using T5, we show that prompt tuning becomes more competitive with scale: as models exceed billions of parameters, our method {\textquotedblleft}closes the gap{\textquotedblright} and matches the strong performance of model tuning (where all model weights are tuned). This finding is especially relevant because large models are costly to share and serve and the ability to reuse one frozen model for multiple downstream tasks can ease this burden. Our method can be seen as a simplification of the recently proposed {\textquotedblleft}prefix tuning{\textquotedblright} of Li and Liang (2021) and we provide a comparison to this and other similar approaches. Finally, we show that conditioning a frozen model with soft prompts confers benefits in robustness to domain transfer and enables efficient {\textquotedblleft}prompt ensembling.{\textquotedblright} We release code and model checkpoints to reproduce our experiments."
}

@inproceedings{li-liang-2021-prefix,
    title = "Prefix-Tuning: Optimizing Continuous Prompts for Generation",
    author = "Li, Xiang Lisa  and
      Liang, Percy",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.353/",
    doi = "10.18653/v1/2021.acl-long.353",
    pages = "4582--4597",
    abstract = "Fine-tuning is the de facto way of leveraging large pretrained language models for downstream tasks. However, fine-tuning modifies all the language model parameters and therefore necessitates storing a full copy for each task. In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen and instead optimizes a sequence of continuous task-specific vectors, which we call the prefix. Prefix-tuning draws inspiration from prompting for language models, allowing subsequent tokens to attend to this prefix as if it were {\textquotedblleft}virtual tokens{\textquotedblright}. We apply prefix-tuning to GPT-2 for table-to-text generation and to BART for summarization. We show that by learning only 0.1{\%} of the parameters, prefix-tuning obtains comparable performance in the full data setting, outperforms fine-tuning in low-data settings, and extrapolates better to examples with topics that are unseen during training."
}

@inproceedings{zhang-etal-2023-pcfg,
    title = "{PCFG}-Based Natural Language Interface Improves Generalization for Controlled Text Generation",
    author = "Zhang, Jingyu  and
      Glass, James  and
      He, Tianxing",
    editor = "Palmer, Alexis  and
      Camacho-collados, Jose",
    booktitle = "Proceedings of the 12th Joint Conference on Lexical and Computational Semantics (*SEM 2023)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.starsem-1.27/",
    doi = "10.18653/v1/2023.starsem-1.27",
    pages = "295--313",
    abstract = "Existing work on controlled text generation (CTG) assumes a control interface of categorical attributes. In this work, we propose a natural language (NL) interface, where we craft a PCFG to embed the control attributes into natural language commands, and propose variants of existing CTG models that take commands as input. In our experiments, we design tailored setups to test the model`s generalization abilities. We find our PCFG-based command generation approach is effective for handling unseen commands compared to fix-set templates. Further, our proposed NL models can effectively generalize to unseen attributes (a new ability enabled by the NL interface), as well as unseen attribute combinations. Interestingly, in model comparisons, the simple conditional generation approach, enhanced with our proposed NL interface, is shown to be a strong baseline in those challenging settings."
}

@inproceedings{ramirez-etal-2023-controllable,
    title = "Controllable Generation of Dialogue Acts for Dialogue Systems via Few-Shot Response Generation and Ranking",
    author = "Ramirez, Angela  and
      Agarwal, Kartik  and
      Juraska, Juraj  and
      Garg, Utkarsh  and
      Walker, Marilyn",
    editor = "Stoyanchev, Svetlana  and
      Joty, Shafiq  and
      Schlangen, David  and
      Dusek, Ondrej  and
      Kennington, Casey  and
      Alikhani, Malihe",
    booktitle = "Proceedings of the 24th Annual Meeting of the Special Interest Group on Discourse and Dialogue",
    month = sep,
    year = "2023",
    address = "Prague, Czechia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.sigdial-1.32/",
    doi = "10.18653/v1/2023.sigdial-1.32",
    pages = "355--369",
    abstract = "Dialogue systems need to produce responses that realize multiple types of dialogue acts (DAs) with high semantic fidelity. In the past, natural language generators (NLGs) for dialogue were trained on large parallel corpora that map from a domain-specific DA and its semantic attributes to an output utterance. Recent work shows that pretrained language models (LLMs) offer new possibilities for controllable NLG using prompt-based learning. Here we develop a novel few-shot overgenerate-and-rank approach that achieves the controlled generation of DAs. We compare eight few-shot prompt styles that include a novel method of generating from textual pseudo-references using a textual style transfer approach. We develop six automatic ranking functions that identify outputs with both the correct DA and high semantic accuracy at generation time. We test our approach on three domains and four LLMs. To our knowledge, this is the first work on NLG for dialogue that automatically ranks outputs using both DA and attribute accuracy. For completeness, we compare our results to fine-tuned few-shot models trained with 5 to 100 instances per DA. Our results show that several prompt settings achieve perfect DA accuracy, and near perfect semantic accuracy (99.81{\%}) and perform better than few-shot fine-tuning."
}

@Article{info:doi/10.2196/38095,
author="Phatak, Atharva
and Savage, David W
and Ohle, Robert
and Smith, Jonathan
and Mago, Vijay",
title="Medical Text Simplification Using Reinforcement Learning (TESLEA): Deep Learning--Based Text Simplification Approach",
journal="JMIR Med Inform",
year="2022",
month="Nov",
day="18",
volume="10",
number="11",
pages="e38095",
keywords="medical text simplification; reinforcement learning; natural language processing; manual evaluation",
abstract="Background: In most cases, the abstracts of articles in the medical domain are publicly available. Although these are accessible by everyone, they are hard to comprehend for a wider audience due to the complex medical vocabulary. Thus, simplifying these complex abstracts is essential to make medical research accessible to the general public. Objective: This study aims to develop a deep learning--based text simplification (TS) approach that converts complex medical text into a simpler version while maintaining the quality of the generated text. Methods: A TS approach using reinforcement learning and transformer--based language models was developed. Relevance reward, Flesch-Kincaid reward, and lexical simplicity reward were optimized to help simplify jargon-dense complex medical paragraphs to their simpler versions while retaining the quality of the text. The model was trained using 3568 complex-simple medical paragraphs and evaluated on 480 paragraphs via the help of automated metrics and human annotation. Results: The proposed method outperformed previous baselines on Flesch-Kincaid scores (11.84) and achieved comparable performance with other baselines when measured using ROUGE-1 (0.39), ROUGE-2 (0.11), and SARI scores (0.40). Manual evaluation showed that percentage agreement between human annotators was more than 70{\%} when factors such as fluency, coherence, and adequacy were considered. Conclusions: A unique medical TS approach is successfully developed that leverages reinforcement learning and accurately simplifies complex medical paragraphs, thereby increasing their readability. The proposed TS approach can be applied to automatically generate simplified text for complex medical text data, which would enhance the accessibility of biomedical research to a wider audience. ",
issn="2291-9694",
doi="10.2196/38095",
url="https://medinform.jmir.org/2022/11/e38095",
url="https://doi.org/10.2196/38095",
url="http://www.ncbi.nlm.nih.gov/pubmed/36399375"
}


@INPROCEEDINGS{8099614,
  author={Rennie, Steven J. and Marcheret, Etienne and Mroueh, Youssef and Ross, Jerret and Goel, Vaibhava},
  booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Self-Critical Sequence Training for Image Captioning}, 
  year={2017},
  volume={},
  number={},
  pages={1179-1195},
  keywords={Training;Inference algorithms;Measurement;Logic gates;Predictive models;Learning (artificial intelligence)},
  doi={10.1109/CVPR.2017.131}
}

@misc{brown2020languagemodelsfewshotlearners,
      title={Language Models are Few-Shot Learners}, 
      author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
      year={2020},
      eprint={2005.14165},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2005.14165}, 
}

@misc{dai2023saferlhfsafereinforcement,
      title={Safe RLHF: Safe Reinforcement Learning from Human Feedback}, 
      author={Josef Dai and Xuehai Pan and Ruiyang Sun and Jiaming Ji and Xinbo Xu and Mickel Liu and Yizhou Wang and Yaodong Yang},
      year={2023},
      eprint={2310.12773},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2310.12773}, 
}

@inproceedings{liu-etal-2022-p,
    title = "{P}-Tuning: Prompt Tuning Can Be Comparable to Fine-tuning Across Scales and Tasks",
    author = "Liu, Xiao  and
      Ji, Kaixuan  and
      Fu, Yicheng  and
      Tam, Weng  and
      Du, Zhengxiao  and
      Yang, Zhilin  and
      Tang, Jie",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-short.8/",
    doi = "10.18653/v1/2022.acl-short.8",
    pages = "61--68",
    abstract = "Prompt tuning, which only tunes continuous prompts with a frozen language model, substantially reduces per-task storage and memory usage at training. However, in the context of NLU, prior work reveals that prompt tuning does not perform well for normal-sized pretrained models. We also find that existing methods of prompt tuning cannot handle hard sequence labeling tasks, indicating a lack of universality. We present a novel empirical finding that properly optimized prompt tuning can be universally effective across a wide range of model scales and NLU tasks. It matches the performance of finetuning while having only 0.1{\%}-3{\%} tuned parameters. Our method P-Tuning v2 is an implementation of Deep Prompt Tuning (CITATION) optimized and adapted for NLU. Given the universality and simplicity of P-Tuning v2, we believe it can serve as an alternative to finetuning and a strong baseline for future research."
}

@inproceedings{shin-etal-2020-autoprompt,
    title = "{A}uto{P}rompt: {E}liciting {K}nowledge from {L}anguage {M}odels with {A}utomatically {G}enerated {P}rompts",
    author = "Shin, Taylor  and
      Razeghi, Yasaman  and
      Logan IV, Robert L.  and
      Wallace, Eric  and
      Singh, Sameer",
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.346/",
    doi = "10.18653/v1/2020.emnlp-main.346",
    pages = "4222--4235",
    abstract = "The remarkable success of pretrained language models has motivated the study of what kinds of knowledge these models learn during pretraining. Reformulating tasks as fill-in-the-blanks problems (e.g., cloze tests) is a natural approach for gauging such knowledge, however, its usage is limited by the manual effort and guesswork required to write suitable prompts. To address this, we develop AutoPrompt, an automated method to create prompts for a diverse set of tasks, based on a gradient-guided search. Using AutoPrompt, we show that masked language models (MLMs) have an inherent capability to perform sentiment analysis and natural language inference without additional parameters or finetuning, sometimes achieving performance on par with recent state-of-the-art supervised models. We also show that our prompts elicit more accurate factual knowledge from MLMs than the manually created prompts on the LAMA benchmark, and that MLMs can be used as relation extractors more effectively than supervised relation extraction models. These results demonstrate that automatically generated prompts are a viable parameter-free alternative to existing probing methods, and as pretrained LMs become more sophisticated and capable, potentially a replacement for finetuning."
}

@misc{stiennon2022learningsummarizehumanfeedback,
      title={Learning to summarize from human feedback}, 
      author={Nisan Stiennon and Long Ouyang and Jeff Wu and Daniel M. Ziegler and Ryan Lowe and Chelsea Voss and Alec Radford and Dario Amodei and Paul Christiano},
      year={2022},
      eprint={2009.01325},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2009.01325}, 
}

@misc{zeldes2020technicalreportauxiliarytuning,
      title={Technical Report: Auxiliary Tuning and its Application to Conditional Text Generation}, 
      author={Yoel Zeldes and Dan Padnos and Or Sharir and Barak Peleg},
      year={2020},
      eprint={2006.16823},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2006.16823}, 
}

@misc{mnih2016asynchronousmethodsdeepreinforcement,
      title={Asynchronous Methods for Deep Reinforcement Learning}, 
      author={Volodymyr Mnih and Adrià Puigdomènech Badia and Mehdi Mirza and Alex Graves and Timothy P. Lillicrap and Tim Harley and David Silver and Koray Kavukcuoglu},
      year={2016},
      eprint={1602.01783},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1602.01783}, 
}

@ARTICLE{Watkins1992-pa,
  title     = "Q-learning",
  author    = "Watkins, Christopher J C H and Dayan, Peter",
  abstract  = "Q-learning (Watkins, 1989) is a simple way for agents to learn
               how to act optimally in controlled Markovian domains. It amounts
               to an incremental method for dynamic programming which imposes
               limited computational demands. It works by successively
               improving its evaluations of the quality of particular actions
               at particular states.This paper presents and proves in detail a
               convergence theorem forQ-learning based on that outlined in
               Watkins (1989). We show thatQ-learning converges to the optimum
               action-values with probability 1 so long as all actions are
               repeatedly sampled in all states and the action-values are
               represented discretely. We also sketch extensions to the cases
               of non-discounted, but absorbing, Markov environments, and where
               manyQ values can be changed each iteration, rather than just
               one.",
  journal   = "Mach. Learn.",
  publisher = "Springer Science and Business Media LLC",
  volume    =  8,
  number    = "3-4",
  pages     = "279--292",
  month     =  may,
  year      =  1992,
  language  = "en"
}


@article{10.1007/BF00992696,
author = {Williams, Ronald J.},
title = {Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning},
year = {1992},
issue_date = {May 1992},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {8},
number = {3–4},
issn = {0885-6125},
url = {https://doi.org/10.1007/BF00992696},
doi = {10.1007/BF00992696},
abstract = {This article presents a general class of associative reinforcement learning algorithms for connectionist networks containing stochastic units. These algorithms, called REINFORCE algorithms, are shown to make weight adjustments in a direction that lies along the gradient of expected reinforcement in both immediate-reinforcement tasks and certain limited forms of delayed-reinforcement tasks, and they do this without explicitly computing gradient estimates or even storing information from which such estimates could be computed. Specific examples of such algorithms are presented, some of which bear a close relationship to certain existing algorithms while others are novel but potentially interesting in their own right. Also given are results that show how such algorithms can be naturally integrated with backpropagation. We close with a brief discussion of a number of additional issues surrounding the use of such algorithms, including what is known about their limiting behaviors as well as further considerations that might be used to help develop similar but potentially more powerful reinforcement learning algorithms.},
journal = {Mach. Learn.},
month = may,
pages = {229–256},
numpages = {28},
keywords = {Reinforcement learning, connectionist networks, gradient descent, mathematical analysis}
}

@misc{schulman2017proximalpolicyoptimizationalgorithms,
      title={Proximal Policy Optimization Algorithms}, 
      author={John Schulman and Filip Wolski and Prafulla Dhariwal and Alec Radford and Oleg Klimov},
      year={2017},
      eprint={1707.06347},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1707.06347}, 
}

@misc{shi2024lifilightweightcontrolledtext,
      title={LiFi: Lightweight Controlled Text Generation with Fine-Grained Control Codes}, 
      author={Chufan Shi and Deng Cai and Yujiu Yang},
      year={2024},
      eprint={2402.06930},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.06930}, 
}

@inproceedings{zhang-etal-2020-pointer,
    title = "{POINTER}: Constrained Progressive Text Generation via Insertion-based Generative Pre-training",
    author = "Zhang, Yizhe  and
      Wang, Guoyin  and
      Li, Chunyuan  and
      Gan, Zhe  and
      Brockett, Chris  and
      Dolan, Bill",
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.698/",
    doi = "10.18653/v1/2020.emnlp-main.698",
    pages = "8649--8670",
    abstract = "Large-scale pre-trained language models, such as BERT and GPT-2, have achieved excellent performance in language representation learning and free-form text generation. However, these models cannot be directly employed to generate text under specified lexical constraints. To address this challenge, we present POINTER (PrOgressive INsertion-based TransformER), a simple yet novel insertion-based approach for hard-constrained text generation. The proposed method operates by progressively inserting new tokens between existing tokens in a parallel manner. This procedure is recursively applied until a sequence is completed. The resulting coarse-to-fine hierarchy makes the generation process intuitive and interpretable. We pre-train our model with the proposed progressive insertion-based objective on a 12GB Wikipedia dataset, and fine-tune it on downstream hard-constrained generation tasks. Non-autoregressive decoding yields a logarithmic time complexity during inference time. Experimental results on both News and Yelp datasets demonstrate that Pointer achieves state-of-the-art performance on constrained text generation. We released the pre-trained models and the source code to facilitate future research."
}

@misc{chan2022coconselfsupervisedapproachcontrolled,
      title={CoCon: A Self-Supervised Approach for Controlled Text Generation}, 
      author={Alvin Chan and Yew-Soon Ong and Bill Pung and Aston Zhang and Jie Fu},
      year={2022},
      eprint={2006.03535},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2006.03535}, 
}

@inproceedings{tanprasert-kauchak-2021-flesch,
    title = "Flesch-Kincaid is Not a Text Simplification Evaluation Metric",
    author = "Tanprasert, Teerapaun  and
      Kauchak, David",
    editor = "Bosselut, Antoine  and
      Durmus, Esin  and
      Gangal, Varun Prashant  and
      Gehrmann, Sebastian  and
      Jernite, Yacine  and
      Perez-Beltrachini, Laura  and
      Shaikh, Samira  and
      Xu, Wei",
    booktitle = "Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.gem-1.1",
    doi = "10.18653/v1/2021.gem-1.1",
    pages = "1--14",
    abstract = "Sentence-level text simplification is currently evaluated using both automated metrics and human evaluation. For automatic evaluation, a combination of metrics is usually employed to evaluate different aspects of the simplification. Flesch-Kincaid Grade Level (FKGL) is one metric that has been regularly used to measure the readability of system output. In this paper, we argue that FKGL should not be used to evaluate text simplification systems. We provide experimental analyses on recent system output showing that the FKGL score can easily be manipulated to improve the score dramatically with only minor impact on other automated metrics (BLEU and SARI). Instead of using FKGL, we suggest that the component statistics, along with others, be used for posthoc analysis to understand system behavior.",
}

@ARTICLE{Ko2024-dd,
  title     = "Evaluation of the quality and readability of web-based
               information regarding foreign bodies of the ear, nose, and
               throat: Qualitative content analysis",
  author    = "Ko, Tsz Ki and Tan, Denise Jia Yun and Fan, Ka Siu",
  abstract  = "BACKGROUND: Foreign body (FB) inhalation, ingestion, and
               insertion account for 11\% of emergency admissions for ear,
               nose, and throat conditions. Children are disproportionately
               affected, and urgent intervention may be needed to maintain
               airway patency and prevent blood vessel occlusion. High-quality,
               readable online information could help reduce poor outcomes from
               FBs. OBJECTIVE: We aim to evaluate the quality and readability
               of available online health information relating to FBs. METHODS:
               In total, 6 search phrases were queried using the Google search
               engine. For each search term, the first 30 results were
               captured. Websites in the English language and displaying health
               information were included. The provider and country of origin
               were recorded. The modified 36-item Ensuring Quality Information
               for Patients tool was used to assess information quality.
               Readability was assessed using a combination of tools: Flesch
               Reading Ease score, Flesch-Kincaid Grade Level, Gunning-Fog
               Index, and Simple Measure of Gobbledygook. RESULTS: After the
               removal of duplicates, 73 websites were assessed, with the
               majority originating from the United States (n=46, 63\%).
               Overall, the quality of the content was of moderate quality,
               with a median Ensuring Quality Information for Patients score of
               21 (IQR 18-25, maximum 29) out of a maximum possible score of
               36. Precautionary measures were not mentioned on 41\% (n=30) of
               websites and 30\% (n=22) did not identify disk batteries as a
               risky FB. Red flags necessitating urgent care were identified on
               95\% (n=69) of websites, with 89\% (n=65) advising patients to
               seek medical attention and 38\% (n=28) advising on safe FB
               removal. Readability scores (Flesch Reading Ease score=12.4,
               Flesch-Kincaid Grade Level=6.2, Gunning-Fog Index=6.5, and
               Simple Measure of Gobbledygook=5.9 years) showed most websites
               (56\%) were below the recommended sixth-grade level.
               CONCLUSIONS: The current quality and readability of information
               regarding FBs is inadequate. More than half of the websites were
               above the recommended sixth-grade reading level, and important
               information regarding high-risk FBs such as disk batteries and
               magnets was frequently excluded. Strategies should be developed
               to improve access to high-quality information that informs
               patients and parents about risks and when to seek medical help.
               Strategies to promote high-quality websites in search results
               also have the potential to improve outcomes.",
  journal   = "JMIR Form. Res.",
  publisher = "JMIR Publications Inc.",
  volume    =  8,
  pages     = "e55535",
  month     =  aug,
  year      =  2024,
  keywords  = "EQIP; Ensuring Quality Information for Patients; evaluation;
               evaluations; foreign body; grade level; health information;
               information resource; information resources; medical
               informatics; online information; quality; quality of internet
               information; readability; readability of internet information;
               readable; reading level; website; websites",
  language  = "en"
}

@misc{lyu2024scigispynovelmetricbiomedical,
      title={SciGisPy: a Novel Metric for Biomedical Text Simplification via Gist Inference Score}, 
      author={Chen Lyu and Gabriele Pergola},
      year={2024},
      eprint={2410.09632},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.09632}, 
}

@inproceedings{huang-kochmar-2024-referee,
    title = "{REF}e{REE}: A {RE}ference-{FREE} Model-Based Metric for Text Simplification",
    author = "Huang, Yichen  and
      Kochmar, Ekaterina",
    editor = "Calzolari, Nicoletta  and
      Kan, Min-Yen  and
      Hoste, Veronique  and
      Lenci, Alessandro  and
      Sakti, Sakriani  and
      Xue, Nianwen",
    booktitle = "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)",
    month = may,
    year = "2024",
    address = "Torino, Italia",
    publisher = "ELRA and ICCL",
    url = "https://aclanthology.org/2024.lrec-main.1200",
    pages = "13740--13753",
    abstract = "Text simplification lacks a universal standard of quality, and annotated reference simplifications are scarce and costly. We propose to alleviate such limitations by introducing REFeREE, a reference-free model-based metric with a 3-stage curriculum. REFeREE leverages an arbitrarily scalable pretraining stage and can be applied to any quality standard as long as a small number of human annotations are available. Our experiments show that our metric outperforms existing reference-based metrics in predicting overall ratings and reaches competitive and consistent performance in predicting specific ratings while requiring no reference simplifications at inference time.",
}

@misc{sottana2023evaluationmetricseragpt4,
      title={Evaluation Metrics in the Era of GPT-4: Reliably Evaluating Large Language Models on Sequence to Sequence Tasks}, 
      author={Andrea Sottana and Bin Liang and Kai Zou and Zheng Yuan},
      year={2023},
      eprint={2310.13800},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.13800}, 
}

@inproceedings{heineman-etal-2023-dancing,
    title = "Dancing Between Success and Failure: Edit-level Simplification Evaluation using {SALSA}",
    author = "Heineman, David  and
      Dou, Yao  and
      Maddela, Mounica  and
      Xu, Wei",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.211",
    doi = "10.18653/v1/2023.emnlp-main.211",
    pages = "3466--3495",
    abstract = "Large language models (e.g., GPT-4) are uniquely capable of producing highly rated text simplification, yet current human evaluation methods fail to provide a clear understanding of systems{'} specific strengths and weaknesses. To address this limitation, we introduce SALSA, an edit-based human annotation framework that enables holistic and fine-grained text simplification evaluation. We develop twenty one linguistically grounded edit types, covering the full spectrum of success and failure across dimensions of conceptual, syntactic and lexical simplicity. Using SALSA, we collect 19K edit annotations on 840 simplifications, revealing discrepancies in the distribution of simplification strategies performed by fine-tuned models, prompted LLMs and humans, and find GPT-3.5 performs more quality edits than humans, but still exhibits frequent errors. Using our fine-grained annotations, we develop LENS-SALSA, a reference-free automatic simplification metric, trained to predict sentence- and word-level quality simultaneously. Additionally, we introduce word-level quality estimation for simplification and report promising baseline results. Our data, new metric, and annotation toolkit are available at https://salsa-eval.com.",
}

@inproceedings{sulem-etal-2018-semantic,
    title = "Semantic Structural Evaluation for Text Simplification",
    author = "Sulem, Elior  and
      Abend, Omri  and
      Rappoport, Ari",
    editor = "Walker, Marilyn  and
      Ji, Heng  and
      Stent, Amanda",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-1063",
    doi = "10.18653/v1/N18-1063",
    pages = "685--696",
    abstract = "Current measures for evaluating text simplification systems focus on evaluating lexical text aspects, neglecting its structural aspects. In this paper we propose the first measure to address structural aspects of text simplification, called SAMSA. It leverages recent advances in semantic parsing to assess simplification quality by decomposing the input based on its semantic structure and comparing it to the output. SAMSA provides a reference-less automatic evaluation procedure, avoiding the problems that reference-based methods face due to the vast space of valid simplifications for a given sentence. Our human evaluation experiments show both SAMSA{'}s substantial correlation with human judgments, as well as the deficiency of existing reference-based measures in evaluating structural simplification.",
}

@inproceedings{maddela-etal-2023-lens,
    title = "{LENS}: A Learnable Evaluation Metric for Text Simplification",
    author = "Maddela, Mounica  and
      Dou, Yao  and
      Heineman, David  and
      Xu, Wei",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.905",
    doi = "10.18653/v1/2023.acl-long.905",
    pages = "16383--16408",
    abstract = "Training learnable metrics using modern language models has recently emerged as a promising method for the automatic evaluation of machine translation. However, existing human evaluation datasets for text simplification have limited annotations that are based on unitary or outdated models, making them unsuitable for this approach. To address these issues, we introduce the SimpEval corpus that contains: SimpEval{\_}past, comprising 12K human ratings on 2.4K simplifications of 24 past systems, and SimpEval{\_}2022, a challenging simplification benchmark consisting of over 1K human ratings of 360 simplifications including GPT-3.5 generated text. Training on SimpEval, we present LENS, a Learnable Evaluation Metric for Text Simplification. Extensive empirical results show that LENS correlates much better with human judgment than existing metrics, paving the way for future progress in the evaluation of text simplification. We also introduce Rank {\&} Rate, a human evaluation framework that rates simplifications from several models in a list-wise manner using an interactive interface, which ensures both consistency and accuracy in the evaluation process and is used to create the SimpEval datasets.",
}

@inproceedings{deutsch-etal-2022-limitations,
    title = "On the Limitations of Reference-Free Evaluations of Generated Text",
    author = "Deutsch, Daniel  and
      Dror, Rotem  and
      Roth, Dan",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.753",
    doi = "10.18653/v1/2022.emnlp-main.753",
    pages = "10960--10977",
    abstract = "There is significant interest in developing evaluation metrics which accurately estimate the quality of generated text without the aid of a human-written reference text, which can be time consuming and expensive to collect or entirely unavailable in online applications. However, in this work, we demonstrate that these reference-free metrics are inherently biased and limited in their ability to evaluate generated text, and we argue that they should not be used to measure progress on tasks like machine translation or summarization. We show how reference-free metrics are equivalent to using one generation model to evaluate another, which has several limitations: (1) the metrics can be optimized at test time to find the approximate best-possible output, (2) they are inherently biased toward models which are more similar to their own, and (3) they can be biased against higher-quality outputs, including those written by humans. Therefore, we recommend that reference-free metrics should be used as diagnostic tools for analyzing and understanding model behavior instead of measures of how well models perform a task, in which the goal is to achieve as high of a score as possible.",
}

@inproceedings{94205144dd7945cc99b5a6544451b668,
title = "From humans to machines: can ChatGPT-like LLMs effectively replace human annotators in NLP tasks",
author = "Surendrabikram Thapa and Usman Naseem and Mehwish Nasim",
year = "2023",
month = jun,
doi = "10.36190/2023.15",
language = "English",
booktitle = "Workshop Proceedings of the 17th International AAAI Conference on Web and Social Media",
publisher = "Association for the Advancement of Artificial Intelligence (AAAI)",
note = "17th International AAAI Conference on Web and Social Media, ICWSM 2023 ; Conference date: 05-06-2023 Through 08-06-2023",
}

@article{alva-manchego-etal-2021-un,
    title = "The (Un)Suitability of Automatic Evaluation Metrics for Text Simplification",
    author = "Alva-Manchego, Fernando  and
      Scarton, Carolina  and
      Specia, Lucia",
    journal = "Computational Linguistics",
    volume = "47",
    number = "4",
    month = dec,
    year = "2021",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2021.cl-4.28",
    doi = "10.1162/coli_a_00418",
    pages = "861--889",
    abstract = "In order to simplify sentences, several rewriting operations can be performed, such as replacing complex words per simpler synonyms, deleting unnecessary information, and splitting long sentences. Despite this multi-operation nature, evaluation of automatic simplification systems relies on metrics that moderately correlate with human judgments on the simplicity achieved by executing specific operations (e.g., simplicity gain based on lexical replacements). In this article, we investigate how well existing metrics can assess sentence-level simplifications where multiple operations may have been applied and which, therefore, require more general simplicity judgments. For that, we first collect a new and more reliable data set for evaluating the correlation of metrics and human judgments of overall simplicity. Second, we conduct the first meta-evaluation of automatic metrics in Text Simplification, using our new data set (and other existing data) to analyze the variation of the correlation between metrics{'} scores and human judgments across three dimensions: the perceived simplicity level, the system type, and the set of references used for computation. We show that these three aspects affect the correlations and, in particular, highlight the limitations of commonly used operation-specific metrics. Finally, based on our findings, we propose a set of recommendations for automatic evaluation of multi-operation simplifications, suggesting which metrics to compute and how to interpret their scores.",
}

@misc{xu2024reasoningcomparisonllmenhancedsemantic,
      title={Reasoning before Comparison: LLM-Enhanced Semantic Similarity Metrics for Domain Specialized Text Analysis}, 
      author={Shaochen Xu and Zihao Wu and Huaqin Zhao and Peng Shu and Zhengliang Liu and Wenxiong Liao and Sheng Li and Andrea Sikora and Tianming Liu and Xiang Li},
      year={2024},
      eprint={2402.11398},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.11398}, 
}

@inproceedings{snover-etal-2006-study,
    title = "A Study of Translation Edit Rate with Targeted Human Annotation",
    author = "Snover, Matthew  and
      Dorr, Bonnie  and
      Schwartz, Rich  and
      Micciulla, Linnea  and
      Makhoul, John",
    booktitle = "Proceedings of the 7th Conference of the Association for Machine Translation in the Americas: Technical Papers",
    month = aug # " 8-12",
    year = "2006",
    address = "Cambridge, Massachusetts, USA",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2006.amta-papers.25",
    pages = "223--231",
    abstract = "We examine a new, intuitive measure for evaluating machine-translation output that avoids the knowledge intensiveness of more meaning-based approaches, and the labor-intensiveness of human judgments. Translation Edit Rate (TER) measures the amount of editing that a human would have to perform to change a system output so it exactly matches a reference translation. We show that the single-reference variant of TER correlates as well with human judgments of MT quality as the four-reference variant of BLEU. We also define a human-targeted TER (or HTER) and show that it yields higher correlations with human judgments than BLEU{---}even when BLEU is given human-targeted references. Our results indicate that HTER correlates with human judgments better than HMETEOR and that the four-reference variants of TER and HTER correlate with human judgments as well as{---}or better than{---}a second human judgment does.",
}

@inproceedings{banerjee-lavie-2005-meteor,
    title = "{METEOR}: An Automatic Metric for {MT} Evaluation with Improved Correlation with Human Judgments",
    author = "Banerjee, Satanjeev  and
      Lavie, Alon",
    editor = "Goldstein, Jade  and
      Lavie, Alon  and
      Lin, Chin-Yew  and
      Voss, Clare",
    booktitle = "Proceedings of the {ACL} Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization",
    month = jun,
    year = "2005",
    address = "Ann Arbor, Michigan",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W05-0909",
    pages = "65--72",
}

@inproceedings{papineni-etal-2002-bleu,
    title = "{B}leu: a Method for Automatic Evaluation of Machine Translation",
    author = "Papineni, Kishore  and
      Roukos, Salim  and
      Ward, Todd  and
      Zhu, Wei-Jing",
    editor = "Isabelle, Pierre  and
      Charniak, Eugene  and
      Lin, Dekang",
    booktitle = "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2002",
    address = "Philadelphia, Pennsylvania, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P02-1040",
    doi = "10.3115/1073083.1073135",
    pages = "311--318",
}

@inproceedings{jin-gildea-2022-rewarding,
    title = "Rewarding Semantic Similarity under Optimized Alignments for {AMR}-to-Text Generation",
    author = "Jin, Lisa  and
      Gildea, Daniel",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-short.80",
    doi = "10.18653/v1/2022.acl-short.80",
    pages = "710--715",
    abstract = "A common way to combat exposure bias is by applying scores from evaluation metrics as rewards in reinforcement learning (RL). Metrics leveraging contextualized embeddings appear more flexible than their n-gram matching counterparts and thus ideal as training rewards. However, metrics such as BERTScore greedily align candidate and reference tokens, which can allow system outputs to receive excess credit relative to a reference. Furthermore, past approaches featuring semantic similarity rewards suffer from repetitive outputs and overfitting. We address these issues by proposing metrics that replace the greedy alignments in BERTScore with optimized ones. We compute them on a model{'}s trained token embeddings to prevent domain mismatch. Our model optimizing discrete alignment metrics consistently outperforms cross-entropy and BLEU reward baselines on AMR-to-text generation. In addition, we find that this approach enjoys stable training compared to a non-RL setting.",
}

@misc{li2024largelanguagemodelsbiomedical,
      title={Large Language Models for Biomedical Text Simplification: Promising But Not There Yet}, 
      author={Zihao Li and Samuel Belkadi and Nicolo Micheletti and Lifeng Han and Matthew Shardlow and Goran Nenadic},
      year={2024},
      eprint={2408.03871},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2408.03871}, 
}

@article{Tang2024ImprovingBF,
  title={Improving BERTScore for Machine Translation Evaluation Through Contrastive Learning},
  author={Gongbo Tang and Oreen Yousuf and Zeying Jin},
  journal={IEEE Access},
  year={2024},
  volume={12},
  pages={77739-77749},
  url={https://api.semanticscholar.org/CorpusID:270104029}
}

@article{Vetrov2022ANA,
  title={A new approach to calculating BERTScore for automatic assessment of translation quality},
  author={Aleksandr A. Vetrov and Edward Gorn},
  journal={ArXiv},
  year={2022},
  volume={abs/2203.05598},
  url={https://api.semanticscholar.org/CorpusID:247411119}
}

@inproceedings{lin-2004-rouge,
    title = "{ROUGE}: A Package for Automatic Evaluation of Summaries",
    author = "Lin, Chin-Yew",
    booktitle = "Text Summarization Branches Out",
    month = jul,
    year = "2004",
    address = "Barcelona, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W04-1013",
    pages = "74--81",
}

@ARTICLE{Truica2023-au,
  title     = "{SimpLex}: a lexical text simplification architecture",
  author    = "Truic{\u a}, Ciprian-Octavian and Stan, Andrei-Ionu{\c t} and
               Apostol, Elena-Simona",
  journal   = "Neural Comput. Appl.",
  publisher = "Springer Science and Business Media LLC",
  volume    =  35,
  number    =  8,
  pages     = "6265--6280",
  month     =  mar,
  year      =  2023,
  copyright = "https://www.springernature.com/gp/researchers/text-and-data-mining",
  language  = "en"
}


@inproceedings{ke-etal-2022-ctrleval,
    title = "{CTRLE}val: An Unsupervised Reference-Free Metric for Evaluating Controlled Text Generation",
    author = "Ke, Pei  and
      Zhou, Hao  and
      Lin, Yankai  and
      Li, Peng  and
      Zhou, Jie  and
      Zhu, Xiaoyan  and
      Huang, Minlie",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.164",
    doi = "10.18653/v1/2022.acl-long.164",
    pages = "2306--2319",
    abstract = "Existing reference-free metrics have obvious limitations for evaluating controlled text generation models. Unsupervised metrics can only provide a task-agnostic evaluation result which correlates weakly with human judgments, whereas supervised ones may overfit task-specific data with poor generalization ability to other datasets. In this paper, we propose an unsupervised reference-free metric called CTRLEval, which evaluates controlled text generation from different aspects by formulating each aspect into multiple text infilling tasks. On top of these tasks, the metric assembles the generation probabilities from a pre-trained language model without any model training. Experimental results show that our metric has higher correlations with human judgments than other baselines, while obtaining better generalization of evaluating generated texts from different models and with different qualities.",
}

@inproceedings{maddela-etal-2023-lens,
    title = "{LENS}: A Learnable Evaluation Metric for Text Simplification",
    author = "Maddela, Mounica  and
      Dou, Yao  and
      Heineman, David  and
      Xu, Wei",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.905",
    doi = "10.18653/v1/2023.acl-long.905",
    pages = "16383--16408",
    abstract = "Training learnable metrics using modern language models has recently emerged as a promising method for the automatic evaluation of machine translation. However, existing human evaluation datasets for text simplification have limited annotations that are based on unitary or outdated models, making them unsuitable for this approach. To address these issues, we introduce the SimpEval corpus that contains: SimpEval{\_}past, comprising 12K human ratings on 2.4K simplifications of 24 past systems, and SimpEval{\_}2022, a challenging simplification benchmark consisting of over 1K human ratings of 360 simplifications including GPT-3.5 generated text. Training on SimpEval, we present LENS, a Learnable Evaluation Metric for Text Simplification. Extensive empirical results show that LENS correlates much better with human judgment than existing metrics, paving the way for future progress in the evaluation of text simplification. We also introduce Rank {\&} Rate, a human evaluation framework that rates simplifications from several models in a list-wise manner using an interactive interface, which ensures both consistency and accuracy in the evaluation process and is used to create the SimpEval datasets.",
}

@misc{zhang2020bertscoreevaluatingtextgeneration,
      title={BERTScore: Evaluating Text Generation with BERT}, 
      author={Tianyi Zhang and Varsha Kishore and Felix Wu and Kilian Q. Weinberger and Yoav Artzi},
      year={2020},
      eprint={1904.09675},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1904.09675}, 
}

@ARTICLE{Lin2021-gm,
  title     = "Neural Sentence Simplification with Semantic Dependency
               Information",
  author    = "Lin, Zhe and Wan, Xiaojun",
  abstract  = "Most previous works on neural sentence simplification exploit
               seq2seq model to rewrite a sentence without explicitly
               considering the semantic information of the sentence. This may
               lead to the semantic deviation of the simplified sentence. In
               this paper, we leverage semantic dependency graph to aid neural
               sentence simplification system. We propose a new sentence
               simplification model with semantic dependency information,
               called SDISS (as shorthand for Semantic Dependency Information
               guided Sentence Simplification), which incorporates semantic
               dependency graph to guide sentence simplification. We evaluate
               SDISS on three benchmark datasets and it outperforms a number of
               strong baseline models on the SARI and FKGL metrics. Human
               evaluation also shows SDISS can produce simplified sentences
               with better quality.",
  journal   = "Proc. Conf. AAAI Artif. Intell.",
  publisher = "Association for the Advancement of Artificial Intelligence
               (AAAI)",
  volume    =  35,
  number    =  15,
  pages     = "13371--13379",
  month     =  may,
  year      =  2021
}


@article{xu-etal-2016-optimizing,
    title = "Optimizing Statistical Machine Translation for Text Simplification",
    author = "Xu, Wei  and
      Napoles, Courtney  and
      Pavlick, Ellie  and
      Chen, Quanze  and
      Callison-Burch, Chris",
    editor = "Lee, Lillian  and
      Johnson, Mark  and
      Toutanova, Kristina",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "4",
    year = "2016",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/Q16-1029",
    doi = "10.1162/tacl_a_00107",
    pages = "401--415",
    abstract = "Most recent sentence simplification systems use basic machine translation models to learn lexical and syntactic paraphrases from a manually simplified parallel corpus. These methods are limited by the quality and quantity of manually simplified corpora, which are expensive to build. In this paper, we conduct an in-depth adaptation of statistical machine translation to perform text simplification, taking advantage of large-scale paraphrases learned from bilingual texts and a small amount of manual simplifications with multiple references. Our work is the first to design automatic metrics that are effective for tuning and evaluating simplification systems, which will facilitate iterative development for this task.",
}

@UNPUBLISHED{Tuan2023-wc,
  title    = "Using machine learning to improve the readability of hospital
              discharge instructions for heart failure",
  author   = "Tuan, Alyssa W and Cannon, Nathan and Foley, David and Gupta,
              Neha and Park, Christian and Chester-Paul, Kyra and Bhasker,
              Joanna and Pearson, Cara and Amarnani, Avisha and High, Zachary
              and Kraschnewski, Jennifer and Shah, Ravi",
  abstract = "AbstractBackgroundLow health literacy is associated with poor
              health outcomes. Hospital discharge instructions are often
              written at advanced reading levels, limiting patients' with low
              health literacy ability to follow medication instructions or
              complete other necessary care. Previous research demonstrates
              that improving the readability of discharge instructions reduces
              hospital readmissions and decreases healthcare costs. We aimed to
              use artificial intelligence (AI) to improve the readability of
              discharge instructions.Methodology/Principal FindingsWe collected
              a series of discharge instructions for adults hospitalized for
              heart failure (n=423), which were then manually simplified to a
              lower reading level to create two parallel sets of discharge
              instructions. Only 343 sets were then processed via AI-based
              machine learning to create a trained algorithm. We then tested
              the algorithm on the remaining 80 discharge instructions. Output
              was evaluated quantitatively using Simple Measure of Gobbledygook
              (SMOG) and Flesch-Kincaid readability scores and cross-entropy
              analysis and qualitatively. Using this test dataset (n=80), the
              average reading levels were: original discharge instructions
              (SMOG: 10.5669$\pm$1.2634, Flesch-Kincaid: 8.6038$\pm$1.5509),
              human-simplified instructions (SMOG: 9.4406$\pm$1.0791,
              Flesch-Kincaid: 7.2221$\pm$1.3794), and AI-simplified
              instructions (SMOG: 9.3045$\pm$0.9531, Flesch-Kincaid:
              7.0464$\pm$1.1308). AI-simplified instructions were significantly
              different from original instructions
              (pConclusions/SignificanceThe AI-based algorithm learned
              meaningful phrase-level simplifications from the human-simplified
              discharge instructions. The AI simplifications, while not in
              complete agreement with the human simplifications, do appear as
              statistically significant improvements to SMOG and Flesch-Kincaid
              reading levels. The algorithm will likely produce more meaningful
              and concise simplifications among discharge instructions as it is
              trained on more data. This study demonstrates an important
              opportunity for AI integration into healthcare delivery to
              address health disparities related to limited health literacy and
              potentially improve patient health.Author summaryPatient-facing
              materials are often written at too high of a reading level for
              patients, such as hospital discharge instructions. These
              instructions provide critical information on how to control
              health conditions, take medications, and attend follow-up visits.
              Difficulty understanding these instructions could lead to the
              patient returning to the hospital if they do not understand how
              to control their health condition.Improving the readability of
              discharge instructions can reduce hospital readmissions. It may
              improve health outcomes for patients and reduce healthcare costs.
              Artificial intelligence (AI) may be used to improve the reading
              level of patient-facing materials. Our work aims to create a tool
              that can accomplish this goal.We obtained hospital discharge
              instructions for heart failure. Discharge instructions were
              edited by medical experts to improve their readability. This
              created two sets of discharge instructions that were processed
              using AI. We created and tested an AI tool to automatically
              simplify discharge instructions. Although not perfect, we found
              that the tool was successful. This research shows that AI can be
              used to address health literacy needs within health care by
              making patient-facing health materials easier to understand. This
              is important to empower all patients to take action to improve
              their health.",
  journal  = "medRxiv",
  month    =  jun,
  year     =  2023
}

@misc{keskar2019ctrlconditionaltransformerlanguage,
      title={CTRL: A Conditional Transformer Language Model for Controllable Generation}, 
      author={Nitish Shirish Keskar and Bryan McCann and Lav R. Varshney and Caiming Xiong and Richard Socher},
      year={2019},
      eprint={1909.05858},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1909.05858}, 
}

@inproceedings{he-etal-2022-ctrlsum,
    title = "{CTRL}sum: Towards Generic Controllable Text Summarization",
    author = "He, Junxian  and
      Kryscinski, Wojciech  and
      McCann, Bryan  and
      Rajani, Nazneen  and
      Xiong, Caiming",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.396/",
    doi = "10.18653/v1/2022.emnlp-main.396",
    pages = "5879--5915",
    abstract = "Current summarization systems yield generic summaries that are disconnected from users' preferences and expectations. To address this limitation, we present CTRLsum, a generic framework to control generated summaries through a set of keywords. During training keywords are extracted automatically without requiring additional human annotations. At test time CTRLsum features a control function to map control signal to keywords; through engineering the control function, the same trained model is able to be applied to control summaries on various dimensions, while neither affecting the model training process nor the pretrained models. We additionally explore the combination of keywords and text prompts for more control tasks. Experiments demonstrate the effectiveness of CTRLsum on three domains of summarization datasets and five control tasks: (1) entity-centric and (2) length-controllable summarization, (3) contribution summarization on scientific papers, (4) invention purpose summarization on patent filings, and (5) question-guided summarization on news articles. Moreover, when used in a standard, unconstrained summarization setting, CTRLsum is comparable or better than strong pretrained systems."
}

@misc{dathathri2020plugplaylanguagemodels,
      title={Plug and Play Language Models: A Simple Approach to Controlled Text Generation}, 
      author={Sumanth Dathathri and Andrea Madotto and Janice Lan and Jane Hung and Eric Frank and Piero Molino and Jason Yosinski and Rosanne Liu},
      year={2020},
      eprint={1912.02164},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1912.02164}, 
}

@article{10.1145/3617680,
author = {Zhang, Hanqing and Song, Haolin and Li, Shaoyu and Zhou, Ming and Song, Dawei},
title = {A Survey of Controllable Text Generation Using Transformer-based Pre-trained Language Models},
year = {2023},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/3617680},
doi = {10.1145/3617680},
abstract = {Controllable Text Generation (CTG) is an emerging area in the field of natural language generation (NLG). It is regarded as crucial for the development of advanced text generation technologies that better meet the specific constraints in practical applications. In recent years, methods using large-scale pre-trained language models (PLMs), in particular the widely used Transformer-based PLMs, have become a new paradigm of NLG, allowing generation of more diverse and fluent text. However, due to the limited level of interpretability of deep neural networks, the controllability of these methods needs to be guaranteed. To this end, controllable text generation using Transformer-based PLMs has become a rapidly growing yet challenging new research hotspot. A diverse range of approaches have emerged in the past 3 to 4 years, targeting different CTG tasks that require different types of controlled constraints. In this article, we present a systematic critical review on the common tasks, main approaches, and evaluation methods in this area. Finally, we discuss the challenges that the field is facing, and put forward various promising future directions. To the best of our knowledge, this is the first survey article to summarize the state-of-the-art CTG techniques from the perspective of Transformer-based PLMs. We hope it can help researchers and practitioners in the related fields to quickly track the academic and technological frontier, providing them with a landscape of the area and a roadmap for future research.},
journal = {ACM Comput. Surv.},
month = oct,
articleno = {64},
numpages = {37},
keywords = {systematic review, controllability, Transformer, pre-trained language models, Controllable text generation}
}

@misc{liang2024controllabletextgenerationlarge,
      title={Controllable Text Generation for Large Language Models: A Survey}, 
      author={Xun Liang and Hanyu Wang and Yezhaohui Wang and Shichao Song and Jiawei Yang and Simin Niu and Jie Hu and Dan Liu and Shunyu Yao and Feiyu Xiong and Zhiyu Li},
      year={2024},
      eprint={2408.12599},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2408.12599}, 
}

@inproceedings{yamanaka-tokunaga-2024-siera,
    title = "{SIERA}: An Evaluation Metric for Text Simplification using the Ranking Model and Data Augmentation by Edit Operations",
    author = "Yamanaka, Hikaru  and
      Tokunaga, Takenobu",
    editor = "Wilkens, Rodrigo  and
      Cardon, R{\'e}mi  and
      Todirascu, Amalia  and
      Gala, N{\'u}ria",
    booktitle = "Proceedings of the 3rd Workshop on Tools and Resources for People with REAding DIfficulties (READI) @ LREC-COLING 2024",
    month = may,
    year = "2024",
    address = "Torino, Italia",
    publisher = "ELRA and ICCL",
    url = "https://aclanthology.org/2024.readi-1.5",
    pages = "47--58",
    abstract = "Automatic evaluation metrics are indispensable for text simplification (TS) research. The past TS research adopts three evaluation aspects: fluency, meaning preservation and simplicity. However, there is little consensus on a metric to measure simplicity, a unique aspect of TS compared with other text generation tasks. In addition, many of the existing metrics require reference simplified texts for evaluation. Thus, the cost of collecting reference texts is also an issue. This study proposes a new automatic evaluation metric, SIERA, for sentence simplification. SIERA employs a ranking model for the order relation of simplicity, which is trained by pairs of the original and simplified sentences. It does not require reference sentences for either training or evaluation. The sentence pairs for training are further augmented by the proposed method that utlizes edit operations to generate intermediate sentences with the simplicity between the original and simplified sentences. Using three evaluation datasets for text simplification, we compare SIERA with other metrics by calculating the correlations between metric values and human ratings. The results showed SIERA{'}s superiority over other metrics with a reservation that the quality of evaluation sentences is consistent with that of the training data.",
}

@ARTICLE{Varli2023-ma,
  title     = "Evaluation of readability levels of online patient education
               materials for female pelvic floor disorders",
  author    = "Varli, Bulut and Cetindag, Elif Nazli and Koyuncu Demir, Kazibe
               and Coban, Ulas and Islamova, Gunel and Dokmeci, Fulya",
  abstract  = "Most women hesitate to seek help from healthcare providers as
               they find it difficult to share complaints of involuntary
               leakage or vaginal prolapse. Hence, they often refer to the
               websites of national and/or international bodies' patient
               education materials (PEMs), which are considered the most
               reliable sources. The crucial factor that determines their
               usefulness is their readability level, which makes them ``easy''
               or ``difficult'' to read, and is recommended, not to exceed the
               sixth grade level. In this study, we aimed to assess the
               readability levels of Turkish translated PEMs from the websites
               of the International Urogynecological Association and the
               European Association of Urology and the PEMs originally written
               in Turkish from the website of the Society of Urological Surgery
               in Turkey. All the PEMs (n = 52) were analyzed by online
               calculators using the Atesman formula, Flesch-Kincaid grade
               level, and Gunning Fog index. The readability parameters, number
               of sentences, words, letters, syllables, and readability
               intervals of these methods were compared among the groups using
               the Kruskal-Wallis test, or ANOVA test, with post hoc
               comparisons where appropriate. The readability level of all PEMs
               is at least at an ``averagely difficult'' interval, according to
               both assessment methods. No significant differences were found
               among the PEM groups in terms of readability parameters and
               assessment methods (P > .05). Whether original or translated,
               international or national societies' PEMs' readability scores
               were above the recommended level of sixth grade. Thus, the
               development of PEMs needs to be revised accordingly by relevant
               authorities.",
  journal   = "Medicine (Baltimore)",
  publisher = "Ovid Technologies (Wolters Kluwer Health)",
  volume    =  102,
  number    =  52,
  pages     = "e36636",
  month     =  dec,
  year      =  2023,
  language  = "en"
}

@article{Crossley2022,
  title = {A large-scaled corpus for assessing text readability},
  volume = {55},
  ISSN = {1554-3528},
  url = {http://dx.doi.org/10.3758/s13428-022-01802-x},
  DOI = {10.3758/s13428-022-01802-x},
  number = {2},
  journal = {Behavior Research Methods},
  publisher = {Springer Science and Business Media LLC},
  author = {Crossley,  Scott and Heintz,  Aron and Choi,  Joon Suh and Batchelor,  Jordan and Karimi,  Mehrnoush and Malatinszky,  Agnes},
  year = {2022},
  month = mar,
  pages = {491–507}
}

@article{Singh2024,
  title = {Readability Metrics in Patient Education: Where Do We Innovate?},
  volume = {14},
  ISSN = {2039-7283},
  url = {http://dx.doi.org/10.3390/clinpract14060183},
  DOI = {10.3390/clinpract14060183},
  number = {6},
  journal = {Clinics and Practice},
  publisher = {MDPI AG},
  author = {Singh,  Som and Jamal,  Aleena and Qureshi,  Fawad},
  year = {2024},
  month = nov,
  pages = {2341–2349}
}

@article{Swanson2024,
  title = {Biomedical text readability after hypernym substitution with fine-tuned large language models},
  volume = {3},
  ISSN = {2767-3170},
  url = {http://dx.doi.org/10.1371/journal.pdig.0000489},
  DOI = {10.1371/journal.pdig.0000489},
  number = {4},
  journal = {PLOS Digital Health},
  publisher = {Public Library of Science (PLoS)},
  author = {Swanson,  Karl and He,  Shuhan and Calvano,  Josh and Chen,  David and Telvizian,  Talar and Jiang,  Lawrence and Chong,  Paul and Schwell,  Jacob and Mak,  Gin and Lee,  Jarone},
  editor = {Tariq,  Amara},
  year = {2024},
  month = apr,
  pages = {e0000489}
}

@article{WANG2013503,
title = {Assessing readability formula differences with written health information materials: Application, results, and recommendations},
journal = {Research in Social and Administrative Pharmacy},
volume = {9},
number = {5},
pages = {503-516},
year = {2013},
issn = {1551-7411},
doi = {https://doi.org/10.1016/j.sapharm.2012.05.009},
url = {https://www.sciencedirect.com/science/article/pii/S1551741112000770},
author = {Lih-Wern Wang and Michael J. Miller and Michael R. Schmitt and Frances K. Wen},
}

@ARTICLE{Fitzsimmons2010-mq,
  title     = "A readability assessment of online Parkinson's disease
               information",
  author    = "Fitzsimmons, P R and Michael, B D and Hulley, J L and Scott, G O",
  abstract  = "BACKGROUND: Patients increasingly use the internet to access
               health information. Inadequate health literacy is common and
               frequently limits patient comprehension of healthcare
               literature. We aimed to assess the readability of online
               consumer-orientated Parkinson's disease (PD) information using
               two validated measures. METHOD: We identified the 100 highest
               ranked consumer-orientated PD webpages and determined webpage
               readability using the Flesch-Kincaid and Simple Measure Of
               Gobbledygook (SMOG) formulae. RESULTS: None of the webpages
               analysed complied with current readability guidelines.
               Commercial websites were significantly easier to read (p =
               0.035). The Flesch-Kincaid formula significantly underestimated
               reading difficulty (p < 0.0001). Ease of reading correlated
               weakly with search engine ranking (r = 0.35, p = 0.0004).
               CONCLUSIONS: Only 1\% of the top 100 PD information webpages are
               fully comprehensible to the average adult. Simple Measure Of
               Gobbledygook should be the preferred measure of webpage
               readability. Parkinson's disease information websites require
               major text revision to comply with readability guidelines and to
               be comprehensible to the average patient.",
  journal   = "J. R. Coll. Physicians Edinb.",
  publisher = "SAGE Publications",
  volume    =  40,
  number    =  4,
  pages     = "292--296",
  month     =  dec,
  year      =  2010,
  language  = "en"
}


@ARTICLE{Wang2013-hu,
  title     = "Assessing readability formula differences with written health
               information materials: application, results, and recommendations",
  author    = "Wang, Lih-Wern and Miller, Michael J and Schmitt, Michael R and
               Wen, Frances K",
  abstract  = "BACKGROUND: Readability formulas are often used to guide the
               development and evaluation of literacy-sensitive written health
               information. However, readability formula results may vary
               considerably as a result of differences in software processing
               algorithms and how each formula is applied. These variations
               complicate interpretations of reading grade level estimates,
               particularly without a uniform guideline for applying and
               interpreting readability formulas. OBJECTIVES: This research
               sought to (1) identify commonly used readability formulas
               reported in the health care literature, (2) demonstrate the use
               of the most commonly used readability formulas on written health
               information, (3) compare and contrast the differences when
               applying common readability formulas to identical selections of
               written health information, and (4) provide recommendations for
               choosing an appropriate readability formula for written
               health-related materials to optimize their use. METHODS: A
               literature search was conducted to identify the most commonly
               used readability formulas in health care literature. Each of the
               identified formulas was subsequently applied to word samples
               from 15 unique examples of written health information about the
               topic of depression and its treatment. Readability estimates
               from common readability formulas were compared based on text
               sample size, selection, formatting, software type, and/or hand
               calculations. Recommendations for their use were provided.
               RESULTS: The Flesch-Kincaid formula was most commonly used
               (57.42\%). Readability formulas demonstrated variability up to 5
               reading grade levels on the same text. The Simple Measure of
               Gobbledygook (SMOG) readability formula performed most
               consistently. Depending on the text sample size, selection,
               formatting, software, and/or hand calculations, the individual
               readability formula estimated up to 6 reading grade levels of
               variability. CONCLUSIONS: The SMOG formula appears best suited
               for health care applications because of its consistency of
               results, higher level of expected comprehension, use of more
               recent validation criteria for determining reading grade level
               estimates, and simplicity of use. To improve interpretation of
               readability results, reporting reading grade level estimates
               from any formula should be accompanied with information about
               word sample size, location of word sampling in the text,
               formatting, and method of calculation.",
  journal   = "Res. Social Adm. Pharm.",
  publisher = "Elsevier BV",
  volume    =  9,
  number    =  5,
  pages     = "503--516",
  month     =  sep,
  year      =  2013,
  keywords  = "Health literacy; Readability; Readability formula",
  language  = "en"
}

@ARTICLE{Hanci2024-wv,
  title     = "Assessment of the readability of the online patient education
               materials of intensive and Critical Care societies",
  author    = "Hanci, Volkan and Otlu, B{\"u}{\c s}ra and Biyiko{\u g}lu, Ali
               Salih",
  abstract  = "OBJECTIVES: This study aimed to evaluate the readability of
               patient education materials (PEMs) on websites of intensive and
               critical care societies. DATA SOURCES: Websites of intensive and
               critical care societies, which are members of The World
               Federation of Intensive and Critical Care and The European
               Society of Intensive Care Medicine. SETTING: Cross-sectional
               observational, internet-based, website, PEMs, readability study.
               STUDY SELECTION: The readability of the PEMs available on
               societies' sites was evaluated. DATA EXTRACTION: The readability
               formulas used were the Flesch Reading Ease Score (FRES),
               Flesch-Kincaid Grade Level (FKGL), Simple Measure of
               Gobbledygook (SMOG), and Gunning Fog (GFOG). DATA SYNTHESIS: One
               hundred twenty-seven PEM from 11 different societies were
               included in our study. In the readability analysis of PEM, the
               FRES was 58.10 (48.85-63.77) (difficult), the mean FKGL and SMOG
               were 10.19 (8.93-11.72) and 11.10 (10.11-11.87) years,
               respectively, and the mean GFOG score was 12.73 (11.37-14.15)
               (very difficult). All readability formula results were
               significantly higher than the recommended sixth-grade level ( p
               < 0.001). All PEMs were above the sixth-grade level when the
               societies were evaluated individually according to all
               readability results ( p < 0.05). CONCLUSIONS: Compared with the
               sixth-grade level recommended by the American Medical
               Association and the National Institutes of Health, the
               readability of PEMs in intensive and critical care societies is
               relatively high. PEMs in intensive and critical care societies
               should be prepared with attention to recommendations on
               readability.",
  journal   = "Crit. Care Med.",
  publisher = "Ovid Technologies (Wolters Kluwer Health)",
  volume    =  52,
  number    =  2,
  pages     = "e47--e57",
  month     =  feb,
  year      =  2024,
  language  = "en"
}

@ARTICLE{Wu2023-fl,
  title     = "{ChatGPT}: is it good for our glaucoma patients?",
  author    = "Wu, Gloria and Lee, David A and Zhao, Weichen and Wong, Adrial
               and Sidhu, Sahej",
  abstract  = "Purpose: Our study investigates ChatGPT and its ability to
               communicate with glaucoma patients. Methods: We inputted eight
               glaucoma-related questions/topics found on the American Academy
               of Ophthalmology (AAO)'s website into ChatGPT. We used the
               Flesch-Kincaid test, Gunning Fog Index, SMOG Index, and
               Dale-Chall readability formula to evaluate the comprehensibility
               of its responses for patients. ChatGPT's answers were compared
               with those found on the AAO's website. Results: ChatGPT's
               responses required reading comprehension of a higher grade level
               (average = grade 12.5 $\pm$ 1.6) than that of the text on the
               AAO's website (average = 9.4 grade $\pm$ 3.5), (0.0384). For the
               eight responses, the key ophthalmic terms appeared 34 out of 86
               times in the ChatGPT responses vs. 86 out of 86 times in the
               text on the AAO's website. The term ``eye doctor'' appeared once
               in the ChatGPT text, but the formal term ``ophthalmologist'' did
               not appear. The term ``ophthalmologist'' appears 26 times on the
               AAO's website. The word counts of the answers produced by
               ChatGPT and those on the AAO's website were similar (p = 0.571),
               with phrases of a homogenous length. Conclusion: ChatGPT trains
               on the texts, phrases, and algorithms inputted by software
               engineers. As ophthalmologists, through our websites and
               journals, we should consider encoding the phrase ``see an
               ophthalmologist''. Our medical assistants should sit with
               patients during their appointments to ensure that the text is
               accurate and that they fully comprehend its meaning. ChatGPT is
               effective for providing general information such as definitions
               or potential treatment options for glaucoma. However, ChatGPT
               has a tendency toward repetitive answers and, due to their
               elevated readability scores, these could be too difficult for a
               patient to read.",
  journal   = "Front. Ophthalmol. (Lausanne)",
  publisher = "Frontiers Media SA",
  volume    =  3,
  pages     = "1260415",
  month     =  nov,
  year      =  2023,
  keywords  = "ChatGPT; artificial intelligence; glaucoma; ophthalmology;
               patient education",
  copyright = "https://creativecommons.org/licenses/by/4.0/",
  language  = "en"
}

@ARTICLE{Lucy2023-zi,
  title    = "Readability of patient education materials for bariatric surgery",
  author   = "Lucy, Adam Timothy and Rakestraw, Stephanie L and Stringer,
              Courtney and Chu, Daniel and Grams, Jayleen and Stahl, Richard
              and Mustian, Margaux N",
  abstract = "INTRODUCTION: Bariatric surgery is a successful treatment for
              obesity, but barriers to surgery exist, including low health
              literacy. National organizations recommend patient education
              materials (PEM) not exceed a sixth-grade reading level. Difficult
              to comprehend PEM can exacerbate barriers to bariatric surgery,
              especially in the Deep South where high obesity and low literacy
              rates exist. This study aimed to assess and compare the
              readability of webpages and electronic medical record (EMR)
              bariatric surgery PEM from one institution. METHODS: Readability
              of online bariatric surgery and standardized perioperative EMR
              PEM were analyzed and compared. Text readability was assessed by
              validated instruments: Flesch Reading Ease Formula (FRE), Flesch
              Kincaid Grade Level (FKGL), Gunning Fog (GF), Coleman-Liau Index
              (CL), Simple Measure of Gobbledygook (SMOG), Automated
              Readability Index (ARI), and Linsear Write Formula (LWF). Mean
              readability scores were calculated with standard deviations and
              compared using unpaired t-tests. RESULTS: 32 webpages and seven
              EMR education documents were analyzed. Webpages were overall
              ``difficult to read'' compared to ``standard/average''
              readability EMR materials (mean FRE 50.5 $\pm$ 18.3 vs. 67.4
              $\pm$ 4.2, p = 0.023). All webpages were at or above high school
              reading level: mean FKGL 11.8 $\pm$ 4.4, GF 14.0 $\pm$ 3.9, CL
              9.5 $\pm$ 3.2, SMOG 11.0 $\pm$ 3.2, ARI 11.7 $\pm$ 5.1, and LWF
              14.9 $\pm$ 6.6. Webpages with highest reading levels were
              nutrition information and lowest were patient testimonials. EMR
              materials were sixth to ninth grade reading level: FKGL 6.2 $\pm$
              0.8, GF 9.3 $\pm$ 1.4, CL 9.7 $\pm$ 0.9, SMOG 7.1 $\pm$ 0.8, ARI
              6.1 $\pm$ 1.0, and LWF 5.9 $\pm$ 0.8. CONCLUSION: Surgeon curated
              bariatric surgery webpages have advanced reading levels above
              recommended thresholds compared to standardized PEM from an EMR.
              This readability gap may unintentionally contribute to barriers
              to surgery and affect postoperative outcomes. Streamlined efforts
              are needed to create materials that are easier to read and comply
              with recommendations.",
  journal  = "Surg. Endosc.",
  volume   =  37,
  number   =  8,
  pages    = "6519--6525",
  month    =  aug,
  year     =  2023,
  keywords = "Bariatric surgery; Health literacy; Obesity; Readability",
  language = "en"
}

@article{ZHANG2024121364,
title = {ROUGE-SEM: Better evaluation of summarization using ROUGE combined with semantics},
journal = {Expert Systems with Applications},
volume = {237},
pages = {121364},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.121364},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423018663},
author = {Ming Zhang and Chengzhang Li and Meilin Wan and Xuejun Zhang and Qingwei Zhao},
keywords = {Automatic summarization evaluation, Semantic similarity, Lexical similarity, Contrastive learning, Back-translation},
abstract = {With the development of pre-trained language models and large-scale datasets, automatic text summarization has attracted much attention from the community of natural language processing, but the progress of automatic summarization evaluation has stagnated. Although there have been efforts to improve automatic summarization evaluation, ROUGE has remained one of the most popular metrics for nearly 20 years due to its competitive evaluation performance. However, ROUGE is not perfect, there are studies have shown that it is suffering from inaccurate evaluation of abstractive summarization and limited diversity of generated summaries, both caused by lexical bias. To avoid the bias of lexical similarity, more and more meaningful embedding-based metrics have been proposed to evaluate summaries by measuring semantic similarity. Due to the challenge of accurately measuring semantic similarity, none of them can fully replace ROUGE as the default automatic evaluation toolkit for text summarization. To address the aforementioned problems, we propose a compromise evaluation framework (ROUGE-SEM) for improving ROUGE with semantic information, which compensates for the lack of semantic awareness through a semantic similarity module. According to the differences in semantic similarity and lexical similarity, summaries are classified into four categories for the first time, including good-summary, pearl-summary, glass-summary, and bad-summary. In particular, the back-translation technique is adopted to rewrite pearl-summary and glass-summary that are inaccurately evaluated by ROUGE to alleviate lexical bias. Through this pipeline framework, summaries are first classified by candidate summary classifier, then rewritten by categorized summary rewriter, and finally scored by rewritten summary scorer, which are efficiently evaluated in a manner consistent with human behavior. When measured using Pearson, Spearman, and Kendall rank coefficients, our proposal achieves comparable or higher correlations with human judgments than several state-of-the-art automatic summarization evaluation metrics in dimensions of coherence, consistency, fluency, and relevance. This also suggests that improving ROUGE with semantics is a promising direction for automatic summarization evaluation.}
}

@misc{ganesan2018rouge20updatedimproved,
      title={ROUGE 2.0: Updated and Improved Measures for Evaluation of Summarization Tasks}, 
      author={Kavita Ganesan},
      year={2018},
      eprint={1803.01937},
      archivePrefix={arXiv},
      primaryClass={cs.IR},
      url={https://arxiv.org/abs/1803.01937}, 
}

@ARTICLE{Fung2024-uh,
  title    = "Internet health resources on nocturnal enuresis: A readability,
              quality, and accuracy analysis",
  author   = "Fung, Adrian C H and Lee, Matthew H L and Leung, Jessie L and
              Chan, Ivy H Y and Wong, Kenneth K Y",
  abstract = "INTRODUCTION: Nocturnal enuresis is a common yet
              quality-of-life-limiting pediatric condition. There is an
              increasing trend for parents to obtain information on the
              disease's nature and treatment options via the internet. However,
              the quality of health-related information on the internet varies
              greatly and is largely uncontrolled and unregulated. With this
              study, a readability, quality, and accuracy evaluation of the
              health information regarding nocturnal enuresis is carried out.
              MATERIALS AND METHODS: A questionnaire was administered to
              parents and patients with nocturnal enuresis to determine their
              use of the internet to research their condition. The most common
              search terms were determined, and the first 30 websites returned
              by the most popular search engines were used to assess the
              quality of information about nocturnal enuresis. Each site was
              categorized by type and assessed for readability using the
              Gunning fog score, Simple Measure of Gobbledygook (SMOG) index,
              and Dale-Chall score; for quality using the DISCERN score; and
              for accuracy by comparison to the International Children's
              Continence Society guidelines by three experienced pediatric
              urologists and nephrologists. RESULTS: A total of 30 websites
              were assessed and classified into five categories: professional
              (n = 13), nonprofit (n = 8), commercial (n = 4), government (n =
              3), and other (n = 2). The information was considered difficult
              for the public to comprehend, with mean Gunning fog, SMOG index,
              and Dale-Chall scores of 12.1 $\pm$ 4.3, 14.1 $\pm$ 4.3, and 8.1
              $\pm$ 1.3, respectively. The mean summed DISCERN score was 41
              $\pm$ 11.6 out of 75. Only seven (23\%) websites were considered
              of good quality (DISCERN score > 50). The mean accuracy score of
              the websites was 3.2 $\pm$ 0.6 out of 5. Commercial websites were
              of the poorest quality and accuracy. Websites generally scored
              well in providing their aims and identifying treatment benefits
              and options, while they lacked references and information
              regarding treatment risks and mechanisms. CONCLUSION: Online
              information about nocturnal enuresis exists for parents; however,
              most websites are of suboptimal quality, readability, and
              accuracy. Pediatric surgeons should be aware of parents'
              health-information-seeking behavior and be proactive in guiding
              parents to identify high-quality resources.",
  journal  = "Eur. J. Pediatr. Surg.",
  volume   =  34,
  number   =  1,
  pages    = "84--90",
  month    =  feb,
  year     =  2024,
  language = "en"
}
