@article{atherton2023readability,
  title={Readability of online health information pertaining to migraine and headache in the UK},
  author={Atherton, Kate and Forshaw, Mark J and Kidd, Tara M},
  journal={British Journal of Pain},
  volume={17},
  number={2},
  pages={117--125},
  year={2023},
  publisher={},
  url={https://doi.org/10.1177/20494637221134461},
  pmid={37057254},
  pmcid={PMC10088424},
  issn={2049-4637},
  month=apr,
}

@inproceedings{tanprasert-kauchak-2021-flesch,
    title = "Flesch-Kincaid is Not a Text Simplification Evaluation Metric",
    author = "Tanprasert, Teerapaun  and
      Kauchak, David",
    editor = "Bosselut, Antoine  and
      Durmus, Esin  and
      Gangal, Varun Prashant  and
      Gehrmann, Sebastian  and
      Jernite, Yacine  and
      Perez-Beltrachini, Laura  and
      Shaikh, Samira  and
      Xu, Wei",
    booktitle = "Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.gem-1.1",
    doi = "10.18653/v1/2021.gem-1.1",
    pages = "1--14",
    abstract = "Sentence-level text simplification is currently evaluated using both automated metrics and human evaluation. For automatic evaluation, a combination of metrics is usually employed to evaluate different aspects of the simplification. Flesch-Kincaid Grade Level (FKGL) is one metric that has been regularly used to measure the readability of system output. In this paper, we argue that FKGL should not be used to evaluate text simplification systems. We provide experimental analyses on recent system output showing that the FKGL score can easily be manipulated to improve the score dramatically with only minor impact on other automated metrics (BLEU and SARI). Instead of using FKGL, we suggest that the component statistics, along with others, be used for posthoc analysis to understand system behavior.",
}

@ARTICLE{Ko2024-dd,
  title     = "Evaluation of the quality and readability of web-based
               information regarding foreign bodies of the ear, nose, and
               throat: Qualitative content analysis",
  author    = "Ko, Tsz Ki and Tan, Denise Jia Yun and Fan, Ka Siu",
  abstract  = "BACKGROUND: Foreign body (FB) inhalation, ingestion, and
               insertion account for 11\% of emergency admissions for ear,
               nose, and throat conditions. Children are disproportionately
               affected, and urgent intervention may be needed to maintain
               airway patency and prevent blood vessel occlusion. High-quality,
               readable online information could help reduce poor outcomes from
               FBs. OBJECTIVE: We aim to evaluate the quality and readability
               of available online health information relating to FBs. METHODS:
               In total, 6 search phrases were queried using the Google search
               engine. For each search term, the first 30 results were
               captured. Websites in the English language and displaying health
               information were included. The provider and country of origin
               were recorded. The modified 36-item Ensuring Quality Information
               for Patients tool was used to assess information quality.
               Readability was assessed using a combination of tools: Flesch
               Reading Ease score, Flesch-Kincaid Grade Level, Gunning-Fog
               Index, and Simple Measure of Gobbledygook. RESULTS: After the
               removal of duplicates, 73 websites were assessed, with the
               majority originating from the United States (n=46, 63\%).
               Overall, the quality of the content was of moderate quality,
               with a median Ensuring Quality Information for Patients score of
               21 (IQR 18-25, maximum 29) out of a maximum possible score of
               36. Precautionary measures were not mentioned on 41\% (n=30) of
               websites and 30\% (n=22) did not identify disk batteries as a
               risky FB. Red flags necessitating urgent care were identified on
               95\% (n=69) of websites, with 89\% (n=65) advising patients to
               seek medical attention and 38\% (n=28) advising on safe FB
               removal. Readability scores (Flesch Reading Ease score=12.4,
               Flesch-Kincaid Grade Level=6.2, Gunning-Fog Index=6.5, and
               Simple Measure of Gobbledygook=5.9 years) showed most websites
               (56\%) were below the recommended sixth-grade level.
               CONCLUSIONS: The current quality and readability of information
               regarding FBs is inadequate. More than half of the websites were
               above the recommended sixth-grade reading level, and important
               information regarding high-risk FBs such as disk batteries and
               magnets was frequently excluded. Strategies should be developed
               to improve access to high-quality information that informs
               patients and parents about risks and when to seek medical help.
               Strategies to promote high-quality websites in search results
               also have the potential to improve outcomes.",
  journal   = "JMIR Form. Res.",
  publisher = "JMIR Publications Inc.",
  volume    =  8,
  pages     = "e55535",
  month     =  aug,
  year      =  2024,
  keywords  = "EQIP; Ensuring Quality Information for Patients; evaluation;
               evaluations; foreign body; grade level; health information;
               information resource; information resources; medical
               informatics; online information; quality; quality of internet
               information; readability; readability of internet information;
               readable; reading level; website; websites",
  language  = "en"
}

@misc{lyu2024scigispynovelmetricbiomedical,
      title={SciGisPy: a Novel Metric for Biomedical Text Simplification via Gist Inference Score}, 
      author={Chen Lyu and Gabriele Pergola},
      year={2024},
      eprint={2410.09632},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.09632}, 
}

@inproceedings{huang-kochmar-2024-referee,
    title = "{REF}e{REE}: A {RE}ference-{FREE} Model-Based Metric for Text Simplification",
    author = "Huang, Yichen  and
      Kochmar, Ekaterina",
    editor = "Calzolari, Nicoletta  and
      Kan, Min-Yen  and
      Hoste, Veronique  and
      Lenci, Alessandro  and
      Sakti, Sakriani  and
      Xue, Nianwen",
    booktitle = "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)",
    month = may,
    year = "2024",
    address = "Torino, Italia",
    publisher = "ELRA and ICCL",
    url = "https://aclanthology.org/2024.lrec-main.1200",
    pages = "13740--13753",
    abstract = "Text simplification lacks a universal standard of quality, and annotated reference simplifications are scarce and costly. We propose to alleviate such limitations by introducing REFeREE, a reference-free model-based metric with a 3-stage curriculum. REFeREE leverages an arbitrarily scalable pretraining stage and can be applied to any quality standard as long as a small number of human annotations are available. Our experiments show that our metric outperforms existing reference-based metrics in predicting overall ratings and reaches competitive and consistent performance in predicting specific ratings while requiring no reference simplifications at inference time.",
}

@misc{sottana2023evaluationmetricseragpt4,
      title={Evaluation Metrics in the Era of GPT-4: Reliably Evaluating Large Language Models on Sequence to Sequence Tasks}, 
      author={Andrea Sottana and Bin Liang and Kai Zou and Zheng Yuan},
      year={2023},
      eprint={2310.13800},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.13800}, 
}

@inproceedings{heineman-etal-2023-dancing,
    title = "Dancing Between Success and Failure: Edit-level Simplification Evaluation using {SALSA}",
    author = "Heineman, David  and
      Dou, Yao  and
      Maddela, Mounica  and
      Xu, Wei",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.211",
    doi = "10.18653/v1/2023.emnlp-main.211",
    pages = "3466--3495",
    abstract = "Large language models (e.g., GPT-4) are uniquely capable of producing highly rated text simplification, yet current human evaluation methods fail to provide a clear understanding of systems{'} specific strengths and weaknesses. To address this limitation, we introduce SALSA, an edit-based human annotation framework that enables holistic and fine-grained text simplification evaluation. We develop twenty one linguistically grounded edit types, covering the full spectrum of success and failure across dimensions of conceptual, syntactic and lexical simplicity. Using SALSA, we collect 19K edit annotations on 840 simplifications, revealing discrepancies in the distribution of simplification strategies performed by fine-tuned models, prompted LLMs and humans, and find GPT-3.5 performs more quality edits than humans, but still exhibits frequent errors. Using our fine-grained annotations, we develop LENS-SALSA, a reference-free automatic simplification metric, trained to predict sentence- and word-level quality simultaneously. Additionally, we introduce word-level quality estimation for simplification and report promising baseline results. Our data, new metric, and annotation toolkit are available at https://salsa-eval.com.",
}

@inproceedings{sulem-etal-2018-semantic,
    title = "Semantic Structural Evaluation for Text Simplification",
    author = "Sulem, Elior  and
      Abend, Omri  and
      Rappoport, Ari",
    editor = "Walker, Marilyn  and
      Ji, Heng  and
      Stent, Amanda",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-1063",
    doi = "10.18653/v1/N18-1063",
    pages = "685--696",
    abstract = "Current measures for evaluating text simplification systems focus on evaluating lexical text aspects, neglecting its structural aspects. In this paper we propose the first measure to address structural aspects of text simplification, called SAMSA. It leverages recent advances in semantic parsing to assess simplification quality by decomposing the input based on its semantic structure and comparing it to the output. SAMSA provides a reference-less automatic evaluation procedure, avoiding the problems that reference-based methods face due to the vast space of valid simplifications for a given sentence. Our human evaluation experiments show both SAMSA{'}s substantial correlation with human judgments, as well as the deficiency of existing reference-based measures in evaluating structural simplification.",
}

@inproceedings{maddela-etal-2023-lens,
    title = "{LENS}: A Learnable Evaluation Metric for Text Simplification",
    author = "Maddela, Mounica  and
      Dou, Yao  and
      Heineman, David  and
      Xu, Wei",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.905",
    doi = "10.18653/v1/2023.acl-long.905",
    pages = "16383--16408",
    abstract = "Training learnable metrics using modern language models has recently emerged as a promising method for the automatic evaluation of machine translation. However, existing human evaluation datasets for text simplification have limited annotations that are based on unitary or outdated models, making them unsuitable for this approach. To address these issues, we introduce the SimpEval corpus that contains: SimpEval{\_}past, comprising 12K human ratings on 2.4K simplifications of 24 past systems, and SimpEval{\_}2022, a challenging simplification benchmark consisting of over 1K human ratings of 360 simplifications including GPT-3.5 generated text. Training on SimpEval, we present LENS, a Learnable Evaluation Metric for Text Simplification. Extensive empirical results show that LENS correlates much better with human judgment than existing metrics, paving the way for future progress in the evaluation of text simplification. We also introduce Rank {\&} Rate, a human evaluation framework that rates simplifications from several models in a list-wise manner using an interactive interface, which ensures both consistency and accuracy in the evaluation process and is used to create the SimpEval datasets.",
}

@inproceedings{deutsch-etal-2022-limitations,
    title = "On the Limitations of Reference-Free Evaluations of Generated Text",
    author = "Deutsch, Daniel  and
      Dror, Rotem  and
      Roth, Dan",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.753",
    doi = "10.18653/v1/2022.emnlp-main.753",
    pages = "10960--10977",
    abstract = "There is significant interest in developing evaluation metrics which accurately estimate the quality of generated text without the aid of a human-written reference text, which can be time consuming and expensive to collect or entirely unavailable in online applications. However, in this work, we demonstrate that these reference-free metrics are inherently biased and limited in their ability to evaluate generated text, and we argue that they should not be used to measure progress on tasks like machine translation or summarization. We show how reference-free metrics are equivalent to using one generation model to evaluate another, which has several limitations: (1) the metrics can be optimized at test time to find the approximate best-possible output, (2) they are inherently biased toward models which are more similar to their own, and (3) they can be biased against higher-quality outputs, including those written by humans. Therefore, we recommend that reference-free metrics should be used as diagnostic tools for analyzing and understanding model behavior instead of measures of how well models perform a task, in which the goal is to achieve as high of a score as possible.",
}

@inproceedings{94205144dd7945cc99b5a6544451b668,
title = "From humans to machines: can ChatGPT-like LLMs effectively replace human annotators in NLP tasks",
author = "Surendrabikram Thapa and Usman Naseem and Mehwish Nasim",
year = "2023",
month = jun,
doi = "10.36190/2023.15",
language = "English",
booktitle = "Workshop Proceedings of the 17th International AAAI Conference on Web and Social Media",
publisher = "Association for the Advancement of Artificial Intelligence (AAAI)",
note = "17th International AAAI Conference on Web and Social Media, ICWSM 2023 ; Conference date: 05-06-2023 Through 08-06-2023",
}

@article{alva-manchego-etal-2021-un,
    title = "The (Un)Suitability of Automatic Evaluation Metrics for Text Simplification",
    author = "Alva-Manchego, Fernando  and
      Scarton, Carolina  and
      Specia, Lucia",
    journal = "Computational Linguistics",
    volume = "47",
    number = "4",
    month = dec,
    year = "2021",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2021.cl-4.28",
    doi = "10.1162/coli_a_00418",
    pages = "861--889",
    abstract = "In order to simplify sentences, several rewriting operations can be performed, such as replacing complex words per simpler synonyms, deleting unnecessary information, and splitting long sentences. Despite this multi-operation nature, evaluation of automatic simplification systems relies on metrics that moderately correlate with human judgments on the simplicity achieved by executing specific operations (e.g., simplicity gain based on lexical replacements). In this article, we investigate how well existing metrics can assess sentence-level simplifications where multiple operations may have been applied and which, therefore, require more general simplicity judgments. For that, we first collect a new and more reliable data set for evaluating the correlation of metrics and human judgments of overall simplicity. Second, we conduct the first meta-evaluation of automatic metrics in Text Simplification, using our new data set (and other existing data) to analyze the variation of the correlation between metrics{'} scores and human judgments across three dimensions: the perceived simplicity level, the system type, and the set of references used for computation. We show that these three aspects affect the correlations and, in particular, highlight the limitations of commonly used operation-specific metrics. Finally, based on our findings, we propose a set of recommendations for automatic evaluation of multi-operation simplifications, suggesting which metrics to compute and how to interpret their scores.",
}

@misc{xu2024reasoningcomparisonllmenhancedsemantic,
      title={Reasoning before Comparison: LLM-Enhanced Semantic Similarity Metrics for Domain Specialized Text Analysis}, 
      author={Shaochen Xu and Zihao Wu and Huaqin Zhao and Peng Shu and Zhengliang Liu and Wenxiong Liao and Sheng Li and Andrea Sikora and Tianming Liu and Xiang Li},
      year={2024},
      eprint={2402.11398},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.11398}, 
}

@inproceedings{snover-etal-2006-study,
    title = "A Study of Translation Edit Rate with Targeted Human Annotation",
    author = "Snover, Matthew  and
      Dorr, Bonnie  and
      Schwartz, Rich  and
      Micciulla, Linnea  and
      Makhoul, John",
    booktitle = "Proceedings of the 7th Conference of the Association for Machine Translation in the Americas: Technical Papers",
    month = aug # " 8-12",
    year = "2006",
    address = "Cambridge, Massachusetts, USA",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2006.amta-papers.25",
    pages = "223--231",
    abstract = "We examine a new, intuitive measure for evaluating machine-translation output that avoids the knowledge intensiveness of more meaning-based approaches, and the labor-intensiveness of human judgments. Translation Edit Rate (TER) measures the amount of editing that a human would have to perform to change a system output so it exactly matches a reference translation. We show that the single-reference variant of TER correlates as well with human judgments of MT quality as the four-reference variant of BLEU. We also define a human-targeted TER (or HTER) and show that it yields higher correlations with human judgments than BLEU{---}even when BLEU is given human-targeted references. Our results indicate that HTER correlates with human judgments better than HMETEOR and that the four-reference variants of TER and HTER correlate with human judgments as well as{---}or better than{---}a second human judgment does.",
}

@inproceedings{banerjee-lavie-2005-meteor,
    title = "{METEOR}: An Automatic Metric for {MT} Evaluation with Improved Correlation with Human Judgments",
    author = "Banerjee, Satanjeev  and
      Lavie, Alon",
    editor = "Goldstein, Jade  and
      Lavie, Alon  and
      Lin, Chin-Yew  and
      Voss, Clare",
    booktitle = "Proceedings of the {ACL} Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization",
    month = jun,
    year = "2005",
    address = "Ann Arbor, Michigan",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W05-0909",
    pages = "65--72",
}

@inproceedings{papineni-etal-2002-bleu,
    title = "{B}leu: a Method for Automatic Evaluation of Machine Translation",
    author = "Papineni, Kishore  and
      Roukos, Salim  and
      Ward, Todd  and
      Zhu, Wei-Jing",
    editor = "Isabelle, Pierre  and
      Charniak, Eugene  and
      Lin, Dekang",
    booktitle = "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2002",
    address = "Philadelphia, Pennsylvania, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P02-1040",
    doi = "10.3115/1073083.1073135",
    pages = "311--318",
}

@inproceedings{jin-gildea-2022-rewarding,
    title = "Rewarding Semantic Similarity under Optimized Alignments for {AMR}-to-Text Generation",
    author = "Jin, Lisa  and
      Gildea, Daniel",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-short.80",
    doi = "10.18653/v1/2022.acl-short.80",
    pages = "710--715",
    abstract = "A common way to combat exposure bias is by applying scores from evaluation metrics as rewards in reinforcement learning (RL). Metrics leveraging contextualized embeddings appear more flexible than their n-gram matching counterparts and thus ideal as training rewards. However, metrics such as BERTScore greedily align candidate and reference tokens, which can allow system outputs to receive excess credit relative to a reference. Furthermore, past approaches featuring semantic similarity rewards suffer from repetitive outputs and overfitting. We address these issues by proposing metrics that replace the greedy alignments in BERTScore with optimized ones. We compute them on a model{'}s trained token embeddings to prevent domain mismatch. Our model optimizing discrete alignment metrics consistently outperforms cross-entropy and BLEU reward baselines on AMR-to-text generation. In addition, we find that this approach enjoys stable training compared to a non-RL setting.",
}

@misc{li2024largelanguagemodelsbiomedical,
      title={Large Language Models for Biomedical Text Simplification: Promising But Not There Yet}, 
      author={Zihao Li and Samuel Belkadi and Nicolo Micheletti and Lifeng Han and Matthew Shardlow and Goran Nenadic},
      year={2024},
      eprint={2408.03871},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2408.03871}, 
}

@article{Tang2024ImprovingBF,
  title={Improving BERTScore for Machine Translation Evaluation Through Contrastive Learning},
  author={Gongbo Tang and Oreen Yousuf and Zeying Jin},
  journal={IEEE Access},
  year={2024},
  volume={12},
  pages={77739-77749},
  url={https://api.semanticscholar.org/CorpusID:270104029}
}

@article{Vetrov2022ANA,
  title={A new approach to calculating BERTScore for automatic assessment of translation quality},
  author={Aleksandr A. Vetrov and Edward Gorn},
  journal={ArXiv},
  year={2022},
  volume={abs/2203.05598},
  url={https://api.semanticscholar.org/CorpusID:247411119}
}

@inproceedings{lin-2004-rouge,
    title = "{ROUGE}: A Package for Automatic Evaluation of Summaries",
    author = "Lin, Chin-Yew",
    booktitle = "Text Summarization Branches Out",
    month = jul,
    year = "2004",
    address = "Barcelona, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W04-1013",
    pages = "74--81",
}

@ARTICLE{Truica2023-au,
  title     = "{SimpLex}: a lexical text simplification architecture",
  author    = "Truic{\u a}, Ciprian-Octavian and Stan, Andrei-Ionu{\c t} and
               Apostol, Elena-Simona",
  journal   = "Neural Comput. Appl.",
  publisher = "Springer Science and Business Media LLC",
  volume    =  35,
  number    =  8,
  pages     = "6265--6280",
  month     =  mar,
  year      =  2023,
  copyright = "https://www.springernature.com/gp/researchers/text-and-data-mining",
  language  = "en"
}


@inproceedings{ke-etal-2022-ctrleval,
    title = "{CTRLE}val: An Unsupervised Reference-Free Metric for Evaluating Controlled Text Generation",
    author = "Ke, Pei  and
      Zhou, Hao  and
      Lin, Yankai  and
      Li, Peng  and
      Zhou, Jie  and
      Zhu, Xiaoyan  and
      Huang, Minlie",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.164",
    doi = "10.18653/v1/2022.acl-long.164",
    pages = "2306--2319",
    abstract = "Existing reference-free metrics have obvious limitations for evaluating controlled text generation models. Unsupervised metrics can only provide a task-agnostic evaluation result which correlates weakly with human judgments, whereas supervised ones may overfit task-specific data with poor generalization ability to other datasets. In this paper, we propose an unsupervised reference-free metric called CTRLEval, which evaluates controlled text generation from different aspects by formulating each aspect into multiple text infilling tasks. On top of these tasks, the metric assembles the generation probabilities from a pre-trained language model without any model training. Experimental results show that our metric has higher correlations with human judgments than other baselines, while obtaining better generalization of evaluating generated texts from different models and with different qualities.",
}

@inproceedings{maddela-etal-2023-lens,
    title = "{LENS}: A Learnable Evaluation Metric for Text Simplification",
    author = "Maddela, Mounica  and
      Dou, Yao  and
      Heineman, David  and
      Xu, Wei",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.905",
    doi = "10.18653/v1/2023.acl-long.905",
    pages = "16383--16408",
    abstract = "Training learnable metrics using modern language models has recently emerged as a promising method for the automatic evaluation of machine translation. However, existing human evaluation datasets for text simplification have limited annotations that are based on unitary or outdated models, making them unsuitable for this approach. To address these issues, we introduce the SimpEval corpus that contains: SimpEval{\_}past, comprising 12K human ratings on 2.4K simplifications of 24 past systems, and SimpEval{\_}2022, a challenging simplification benchmark consisting of over 1K human ratings of 360 simplifications including GPT-3.5 generated text. Training on SimpEval, we present LENS, a Learnable Evaluation Metric for Text Simplification. Extensive empirical results show that LENS correlates much better with human judgment than existing metrics, paving the way for future progress in the evaluation of text simplification. We also introduce Rank {\&} Rate, a human evaluation framework that rates simplifications from several models in a list-wise manner using an interactive interface, which ensures both consistency and accuracy in the evaluation process and is used to create the SimpEval datasets.",
}

@misc{zhang2020bertscoreevaluatingtextgeneration,
      title={BERTScore: Evaluating Text Generation with BERT}, 
      author={Tianyi Zhang and Varsha Kishore and Felix Wu and Kilian Q. Weinberger and Yoav Artzi},
      year={2020},
      eprint={1904.09675},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1904.09675}, 
}

@ARTICLE{Lin2021-gm,
  title     = "Neural Sentence Simplification with Semantic Dependency
               Information",
  author    = "Lin, Zhe and Wan, Xiaojun",
  abstract  = "Most previous works on neural sentence simplification exploit
               seq2seq model to rewrite a sentence without explicitly
               considering the semantic information of the sentence. This may
               lead to the semantic deviation of the simplified sentence. In
               this paper, we leverage semantic dependency graph to aid neural
               sentence simplification system. We propose a new sentence
               simplification model with semantic dependency information,
               called SDISS (as shorthand for Semantic Dependency Information
               guided Sentence Simplification), which incorporates semantic
               dependency graph to guide sentence simplification. We evaluate
               SDISS on three benchmark datasets and it outperforms a number of
               strong baseline models on the SARI and FKGL metrics. Human
               evaluation also shows SDISS can produce simplified sentences
               with better quality.",
  journal   = "Proc. Conf. AAAI Artif. Intell.",
  publisher = "Association for the Advancement of Artificial Intelligence
               (AAAI)",
  volume    =  35,
  number    =  15,
  pages     = "13371--13379",
  month     =  may,
  year      =  2021
}


@article{xu-etal-2016-optimizing,
    title = "Optimizing Statistical Machine Translation for Text Simplification",
    author = "Xu, Wei  and
      Napoles, Courtney  and
      Pavlick, Ellie  and
      Chen, Quanze  and
      Callison-Burch, Chris",
    editor = "Lee, Lillian  and
      Johnson, Mark  and
      Toutanova, Kristina",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "4",
    year = "2016",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/Q16-1029",
    doi = "10.1162/tacl_a_00107",
    pages = "401--415",
    abstract = "Most recent sentence simplification systems use basic machine translation models to learn lexical and syntactic paraphrases from a manually simplified parallel corpus. These methods are limited by the quality and quantity of manually simplified corpora, which are expensive to build. In this paper, we conduct an in-depth adaptation of statistical machine translation to perform text simplification, taking advantage of large-scale paraphrases learned from bilingual texts and a small amount of manual simplifications with multiple references. Our work is the first to design automatic metrics that are effective for tuning and evaluating simplification systems, which will facilitate iterative development for this task.",
}

@UNPUBLISHED{Tuan2023-wc,
  title    = "Using machine learning to improve the readability of hospital
              discharge instructions for heart failure",
  author   = "Tuan, Alyssa W and Cannon, Nathan and Foley, David and Gupta,
              Neha and Park, Christian and Chester-Paul, Kyra and Bhasker,
              Joanna and Pearson, Cara and Amarnani, Avisha and High, Zachary
              and Kraschnewski, Jennifer and Shah, Ravi",
  abstract = "AbstractBackgroundLow health literacy is associated with poor
              health outcomes. Hospital discharge instructions are often
              written at advanced reading levels, limiting patients' with low
              health literacy ability to follow medication instructions or
              complete other necessary care. Previous research demonstrates
              that improving the readability of discharge instructions reduces
              hospital readmissions and decreases healthcare costs. We aimed to
              use artificial intelligence (AI) to improve the readability of
              discharge instructions.Methodology/Principal FindingsWe collected
              a series of discharge instructions for adults hospitalized for
              heart failure (n=423), which were then manually simplified to a
              lower reading level to create two parallel sets of discharge
              instructions. Only 343 sets were then processed via AI-based
              machine learning to create a trained algorithm. We then tested
              the algorithm on the remaining 80 discharge instructions. Output
              was evaluated quantitatively using Simple Measure of Gobbledygook
              (SMOG) and Flesch-Kincaid readability scores and cross-entropy
              analysis and qualitatively. Using this test dataset (n=80), the
              average reading levels were: original discharge instructions
              (SMOG: 10.5669$\pm$1.2634, Flesch-Kincaid: 8.6038$\pm$1.5509),
              human-simplified instructions (SMOG: 9.4406$\pm$1.0791,
              Flesch-Kincaid: 7.2221$\pm$1.3794), and AI-simplified
              instructions (SMOG: 9.3045$\pm$0.9531, Flesch-Kincaid:
              7.0464$\pm$1.1308). AI-simplified instructions were significantly
              different from original instructions
              (pConclusions/SignificanceThe AI-based algorithm learned
              meaningful phrase-level simplifications from the human-simplified
              discharge instructions. The AI simplifications, while not in
              complete agreement with the human simplifications, do appear as
              statistically significant improvements to SMOG and Flesch-Kincaid
              reading levels. The algorithm will likely produce more meaningful
              and concise simplifications among discharge instructions as it is
              trained on more data. This study demonstrates an important
              opportunity for AI integration into healthcare delivery to
              address health disparities related to limited health literacy and
              potentially improve patient health.Author summaryPatient-facing
              materials are often written at too high of a reading level for
              patients, such as hospital discharge instructions. These
              instructions provide critical information on how to control
              health conditions, take medications, and attend follow-up visits.
              Difficulty understanding these instructions could lead to the
              patient returning to the hospital if they do not understand how
              to control their health condition.Improving the readability of
              discharge instructions can reduce hospital readmissions. It may
              improve health outcomes for patients and reduce healthcare costs.
              Artificial intelligence (AI) may be used to improve the reading
              level of patient-facing materials. Our work aims to create a tool
              that can accomplish this goal.We obtained hospital discharge
              instructions for heart failure. Discharge instructions were
              edited by medical experts to improve their readability. This
              created two sets of discharge instructions that were processed
              using AI. We created and tested an AI tool to automatically
              simplify discharge instructions. Although not perfect, we found
              that the tool was successful. This research shows that AI can be
              used to address health literacy needs within health care by
              making patient-facing health materials easier to understand. This
              is important to empower all patients to take action to improve
              their health.",
  journal  = "medRxiv",
  month    =  jun,
  year     =  2023
}

@misc{keskar2019ctrlconditionaltransformerlanguage,
      title={CTRL: A Conditional Transformer Language Model for Controllable Generation}, 
      author={Nitish Shirish Keskar and Bryan McCann and Lav R. Varshney and Caiming Xiong and Richard Socher},
      year={2019},
      eprint={1909.05858},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1909.05858}, 
}

@inproceedings{he-etal-2022-ctrlsum,
    title = "{CTRL}sum: Towards Generic Controllable Text Summarization",
    author = "He, Junxian  and
      Kryscinski, Wojciech  and
      McCann, Bryan  and
      Rajani, Nazneen  and
      Xiong, Caiming",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.396/",
    doi = "10.18653/v1/2022.emnlp-main.396",
    pages = "5879--5915",
    abstract = "Current summarization systems yield generic summaries that are disconnected from users' preferences and expectations. To address this limitation, we present CTRLsum, a generic framework to control generated summaries through a set of keywords. During training keywords are extracted automatically without requiring additional human annotations. At test time CTRLsum features a control function to map control signal to keywords; through engineering the control function, the same trained model is able to be applied to control summaries on various dimensions, while neither affecting the model training process nor the pretrained models. We additionally explore the combination of keywords and text prompts for more control tasks. Experiments demonstrate the effectiveness of CTRLsum on three domains of summarization datasets and five control tasks: (1) entity-centric and (2) length-controllable summarization, (3) contribution summarization on scientific papers, (4) invention purpose summarization on patent filings, and (5) question-guided summarization on news articles. Moreover, when used in a standard, unconstrained summarization setting, CTRLsum is comparable or better than strong pretrained systems."
}

@misc{dathathri2020plugplaylanguagemodels,
      title={Plug and Play Language Models: A Simple Approach to Controlled Text Generation}, 
      author={Sumanth Dathathri and Andrea Madotto and Janice Lan and Jane Hung and Eric Frank and Piero Molino and Jason Yosinski and Rosanne Liu},
      year={2020},
      eprint={1912.02164},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1912.02164}, 
}

@misc{liang2024controllabletextgenerationlarge,
      title={Controllable Text Generation for Large Language Models: A Survey}, 
      author={Xun Liang and Hanyu Wang and Yezhaohui Wang and Shichao Song and Jiawei Yang and Simin Niu and Jie Hu and Dan Liu and Shunyu Yao and Feiyu Xiong and Zhiyu Li},
      year={2024},
      eprint={2408.12599},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2408.12599}, 
}

@inproceedings{yamanaka-tokunaga-2024-siera,
    title = "{SIERA}: An Evaluation Metric for Text Simplification using the Ranking Model and Data Augmentation by Edit Operations",
    author = "Yamanaka, Hikaru  and
      Tokunaga, Takenobu",
    editor = "Wilkens, Rodrigo  and
      Cardon, R{\'e}mi  and
      Todirascu, Amalia  and
      Gala, N{\'u}ria",
    booktitle = "Proceedings of the 3rd Workshop on Tools and Resources for People with REAding DIfficulties (READI) @ LREC-COLING 2024",
    month = may,
    year = "2024",
    address = "Torino, Italia",
    publisher = "ELRA and ICCL",
    url = "https://aclanthology.org/2024.readi-1.5",
    pages = "47--58",
    abstract = "Automatic evaluation metrics are indispensable for text simplification (TS) research. The past TS research adopts three evaluation aspects: fluency, meaning preservation and simplicity. However, there is little consensus on a metric to measure simplicity, a unique aspect of TS compared with other text generation tasks. In addition, many of the existing metrics require reference simplified texts for evaluation. Thus, the cost of collecting reference texts is also an issue. This study proposes a new automatic evaluation metric, SIERA, for sentence simplification. SIERA employs a ranking model for the order relation of simplicity, which is trained by pairs of the original and simplified sentences. It does not require reference sentences for either training or evaluation. The sentence pairs for training are further augmented by the proposed method that utlizes edit operations to generate intermediate sentences with the simplicity between the original and simplified sentences. Using three evaluation datasets for text simplification, we compare SIERA with other metrics by calculating the correlations between metric values and human ratings. The results showed SIERA{'}s superiority over other metrics with a reservation that the quality of evaluation sentences is consistent with that of the training data.",
}

@ARTICLE{Varli2023-ma,
  title     = "Evaluation of readability levels of online patient education
               materials for female pelvic floor disorders",
  author    = "Varli, Bulut and Cetindag, Elif Nazli and Koyuncu Demir, Kazibe
               and Coban, Ulas and Islamova, Gunel and Dokmeci, Fulya",
  abstract  = "Most women hesitate to seek help from healthcare providers as
               they find it difficult to share complaints of involuntary
               leakage or vaginal prolapse. Hence, they often refer to the
               websites of national and/or international bodies' patient
               education materials (PEMs), which are considered the most
               reliable sources. The crucial factor that determines their
               usefulness is their readability level, which makes them ``easy''
               or ``difficult'' to read, and is recommended, not to exceed the
               sixth grade level. In this study, we aimed to assess the
               readability levels of Turkish translated PEMs from the websites
               of the International Urogynecological Association and the
               European Association of Urology and the PEMs originally written
               in Turkish from the website of the Society of Urological Surgery
               in Turkey. All the PEMs (n = 52) were analyzed by online
               calculators using the Atesman formula, Flesch-Kincaid grade
               level, and Gunning Fog index. The readability parameters, number
               of sentences, words, letters, syllables, and readability
               intervals of these methods were compared among the groups using
               the Kruskal-Wallis test, or ANOVA test, with post hoc
               comparisons where appropriate. The readability level of all PEMs
               is at least at an ``averagely difficult'' interval, according to
               both assessment methods. No significant differences were found
               among the PEM groups in terms of readability parameters and
               assessment methods (P > .05). Whether original or translated,
               international or national societies' PEMs' readability scores
               were above the recommended level of sixth grade. Thus, the
               development of PEMs needs to be revised accordingly by relevant
               authorities.",
  journal   = "Medicine (Baltimore)",
  publisher = "Ovid Technologies (Wolters Kluwer Health)",
  volume    =  102,
  number    =  52,
  pages     = "e36636",
  month     =  dec,
  year      =  2023,
  language  = "en"
}

@article{Crossley2022,
  title = {A large-scaled corpus for assessing text readability},
  volume = {55},
  ISSN = {1554-3528},
  url = {http://dx.doi.org/10.3758/s13428-022-01802-x},
  DOI = {10.3758/s13428-022-01802-x},
  number = {2},
  journal = {Behavior Research Methods},
  publisher = {Springer Science and Business Media LLC},
  author = {Crossley,  Scott and Heintz,  Aron and Choi,  Joon Suh and Batchelor,  Jordan and Karimi,  Mehrnoush and Malatinszky,  Agnes},
  year = {2022},
  month = mar,
  pages = {491–507}
}

@article{Singh2024,
  title = {Readability Metrics in Patient Education: Where Do We Innovate?},
  volume = {14},
  ISSN = {2039-7283},
  url = {http://dx.doi.org/10.3390/clinpract14060183},
  DOI = {10.3390/clinpract14060183},
  number = {6},
  journal = {Clinics and Practice},
  publisher = {MDPI AG},
  author = {Singh,  Som and Jamal,  Aleena and Qureshi,  Fawad},
  year = {2024},
  month = nov,
  pages = {2341–2349}
}

@article{Swanson2024,
  title = {Biomedical text readability after hypernym substitution with fine-tuned large language models},
  volume = {3},
  ISSN = {2767-3170},
  url = {http://dx.doi.org/10.1371/journal.pdig.0000489},
  DOI = {10.1371/journal.pdig.0000489},
  number = {4},
  journal = {PLOS Digital Health},
  publisher = {Public Library of Science (PLoS)},
  author = {Swanson,  Karl and He,  Shuhan and Calvano,  Josh and Chen,  David and Telvizian,  Talar and Jiang,  Lawrence and Chong,  Paul and Schwell,  Jacob and Mak,  Gin and Lee,  Jarone},
  editor = {Tariq,  Amara},
  year = {2024},
  month = apr,
  pages = {e0000489}
}

@article{WANG2013503,
title = {Assessing readability formula differences with written health information materials: Application, results, and recommendations},
journal = {Research in Social and Administrative Pharmacy},
volume = {9},
number = {5},
pages = {503-516},
year = {2013},
issn = {1551-7411},
doi = {https://doi.org/10.1016/j.sapharm.2012.05.009},
url = {https://www.sciencedirect.com/science/article/pii/S1551741112000770},
author = {Lih-Wern Wang and Michael J. Miller and Michael R. Schmitt and Frances K. Wen},
}

@ARTICLE{Fitzsimmons2010-mq,
  title     = "A readability assessment of online Parkinson's disease
               information",
  author    = "Fitzsimmons, P R and Michael, B D and Hulley, J L and Scott, G O",
  abstract  = "BACKGROUND: Patients increasingly use the internet to access
               health information. Inadequate health literacy is common and
               frequently limits patient comprehension of healthcare
               literature. We aimed to assess the readability of online
               consumer-orientated Parkinson's disease (PD) information using
               two validated measures. METHOD: We identified the 100 highest
               ranked consumer-orientated PD webpages and determined webpage
               readability using the Flesch-Kincaid and Simple Measure Of
               Gobbledygook (SMOG) formulae. RESULTS: None of the webpages
               analysed complied with current readability guidelines.
               Commercial websites were significantly easier to read (p =
               0.035). The Flesch-Kincaid formula significantly underestimated
               reading difficulty (p < 0.0001). Ease of reading correlated
               weakly with search engine ranking (r = 0.35, p = 0.0004).
               CONCLUSIONS: Only 1\% of the top 100 PD information webpages are
               fully comprehensible to the average adult. Simple Measure Of
               Gobbledygook should be the preferred measure of webpage
               readability. Parkinson's disease information websites require
               major text revision to comply with readability guidelines and to
               be comprehensible to the average patient.",
  journal   = "J. R. Coll. Physicians Edinb.",
  publisher = "SAGE Publications",
  volume    =  40,
  number    =  4,
  pages     = "292--296",
  month     =  dec,
  year      =  2010,
  language  = "en"
}


@ARTICLE{Wang2013-hu,
  title     = "Assessing readability formula differences with written health
               information materials: application, results, and recommendations",
  author    = "Wang, Lih-Wern and Miller, Michael J and Schmitt, Michael R and
               Wen, Frances K",
  abstract  = "BACKGROUND: Readability formulas are often used to guide the
               development and evaluation of literacy-sensitive written health
               information. However, readability formula results may vary
               considerably as a result of differences in software processing
               algorithms and how each formula is applied. These variations
               complicate interpretations of reading grade level estimates,
               particularly without a uniform guideline for applying and
               interpreting readability formulas. OBJECTIVES: This research
               sought to (1) identify commonly used readability formulas
               reported in the health care literature, (2) demonstrate the use
               of the most commonly used readability formulas on written health
               information, (3) compare and contrast the differences when
               applying common readability formulas to identical selections of
               written health information, and (4) provide recommendations for
               choosing an appropriate readability formula for written
               health-related materials to optimize their use. METHODS: A
               literature search was conducted to identify the most commonly
               used readability formulas in health care literature. Each of the
               identified formulas was subsequently applied to word samples
               from 15 unique examples of written health information about the
               topic of depression and its treatment. Readability estimates
               from common readability formulas were compared based on text
               sample size, selection, formatting, software type, and/or hand
               calculations. Recommendations for their use were provided.
               RESULTS: The Flesch-Kincaid formula was most commonly used
               (57.42\%). Readability formulas demonstrated variability up to 5
               reading grade levels on the same text. The Simple Measure of
               Gobbledygook (SMOG) readability formula performed most
               consistently. Depending on the text sample size, selection,
               formatting, software, and/or hand calculations, the individual
               readability formula estimated up to 6 reading grade levels of
               variability. CONCLUSIONS: The SMOG formula appears best suited
               for health care applications because of its consistency of
               results, higher level of expected comprehension, use of more
               recent validation criteria for determining reading grade level
               estimates, and simplicity of use. To improve interpretation of
               readability results, reporting reading grade level estimates
               from any formula should be accompanied with information about
               word sample size, location of word sampling in the text,
               formatting, and method of calculation.",
  journal   = "Res. Social Adm. Pharm.",
  publisher = "Elsevier BV",
  volume    =  9,
  number    =  5,
  pages     = "503--516",
  month     =  sep,
  year      =  2013,
  keywords  = "Health literacy; Readability; Readability formula",
  language  = "en"
}

@ARTICLE{Hanci2024-wv,
  title     = "Assessment of the readability of the online patient education
               materials of intensive and Critical Care societies",
  author    = "Hanci, Volkan and Otlu, B{\"u}{\c s}ra and Biyiko{\u g}lu, Ali
               Salih",
  abstract  = "OBJECTIVES: This study aimed to evaluate the readability of
               patient education materials (PEMs) on websites of intensive and
               critical care societies. DATA SOURCES: Websites of intensive and
               critical care societies, which are members of The World
               Federation of Intensive and Critical Care and The European
               Society of Intensive Care Medicine. SETTING: Cross-sectional
               observational, internet-based, website, PEMs, readability study.
               STUDY SELECTION: The readability of the PEMs available on
               societies' sites was evaluated. DATA EXTRACTION: The readability
               formulas used were the Flesch Reading Ease Score (FRES),
               Flesch-Kincaid Grade Level (FKGL), Simple Measure of
               Gobbledygook (SMOG), and Gunning Fog (GFOG). DATA SYNTHESIS: One
               hundred twenty-seven PEM from 11 different societies were
               included in our study. In the readability analysis of PEM, the
               FRES was 58.10 (48.85-63.77) (difficult), the mean FKGL and SMOG
               were 10.19 (8.93-11.72) and 11.10 (10.11-11.87) years,
               respectively, and the mean GFOG score was 12.73 (11.37-14.15)
               (very difficult). All readability formula results were
               significantly higher than the recommended sixth-grade level ( p
               < 0.001). All PEMs were above the sixth-grade level when the
               societies were evaluated individually according to all
               readability results ( p < 0.05). CONCLUSIONS: Compared with the
               sixth-grade level recommended by the American Medical
               Association and the National Institutes of Health, the
               readability of PEMs in intensive and critical care societies is
               relatively high. PEMs in intensive and critical care societies
               should be prepared with attention to recommendations on
               readability.",
  journal   = "Crit. Care Med.",
  publisher = "Ovid Technologies (Wolters Kluwer Health)",
  volume    =  52,
  number    =  2,
  pages     = "e47--e57",
  month     =  feb,
  year      =  2024,
  language  = "en"
}

@ARTICLE{Wu2023-fl,
  title     = "{ChatGPT}: is it good for our glaucoma patients?",
  author    = "Wu, Gloria and Lee, David A and Zhao, Weichen and Wong, Adrial
               and Sidhu, Sahej",
  abstract  = "Purpose: Our study investigates ChatGPT and its ability to
               communicate with glaucoma patients. Methods: We inputted eight
               glaucoma-related questions/topics found on the American Academy
               of Ophthalmology (AAO)'s website into ChatGPT. We used the
               Flesch-Kincaid test, Gunning Fog Index, SMOG Index, and
               Dale-Chall readability formula to evaluate the comprehensibility
               of its responses for patients. ChatGPT's answers were compared
               with those found on the AAO's website. Results: ChatGPT's
               responses required reading comprehension of a higher grade level
               (average = grade 12.5 $\pm$ 1.6) than that of the text on the
               AAO's website (average = 9.4 grade $\pm$ 3.5), (0.0384). For the
               eight responses, the key ophthalmic terms appeared 34 out of 86
               times in the ChatGPT responses vs. 86 out of 86 times in the
               text on the AAO's website. The term ``eye doctor'' appeared once
               in the ChatGPT text, but the formal term ``ophthalmologist'' did
               not appear. The term ``ophthalmologist'' appears 26 times on the
               AAO's website. The word counts of the answers produced by
               ChatGPT and those on the AAO's website were similar (p = 0.571),
               with phrases of a homogenous length. Conclusion: ChatGPT trains
               on the texts, phrases, and algorithms inputted by software
               engineers. As ophthalmologists, through our websites and
               journals, we should consider encoding the phrase ``see an
               ophthalmologist''. Our medical assistants should sit with
               patients during their appointments to ensure that the text is
               accurate and that they fully comprehend its meaning. ChatGPT is
               effective for providing general information such as definitions
               or potential treatment options for glaucoma. However, ChatGPT
               has a tendency toward repetitive answers and, due to their
               elevated readability scores, these could be too difficult for a
               patient to read.",
  journal   = "Front. Ophthalmol. (Lausanne)",
  publisher = "Frontiers Media SA",
  volume    =  3,
  pages     = "1260415",
  month     =  nov,
  year      =  2023,
  keywords  = "ChatGPT; artificial intelligence; glaucoma; ophthalmology;
               patient education",
  copyright = "https://creativecommons.org/licenses/by/4.0/",
  language  = "en"
}

@ARTICLE{Lucy2023-zi,
  title    = "Readability of patient education materials for bariatric surgery",
  author   = "Lucy, Adam Timothy and Rakestraw, Stephanie L and Stringer,
              Courtney and Chu, Daniel and Grams, Jayleen and Stahl, Richard
              and Mustian, Margaux N",
  abstract = "INTRODUCTION: Bariatric surgery is a successful treatment for
              obesity, but barriers to surgery exist, including low health
              literacy. National organizations recommend patient education
              materials (PEM) not exceed a sixth-grade reading level. Difficult
              to comprehend PEM can exacerbate barriers to bariatric surgery,
              especially in the Deep South where high obesity and low literacy
              rates exist. This study aimed to assess and compare the
              readability of webpages and electronic medical record (EMR)
              bariatric surgery PEM from one institution. METHODS: Readability
              of online bariatric surgery and standardized perioperative EMR
              PEM were analyzed and compared. Text readability was assessed by
              validated instruments: Flesch Reading Ease Formula (FRE), Flesch
              Kincaid Grade Level (FKGL), Gunning Fog (GF), Coleman-Liau Index
              (CL), Simple Measure of Gobbledygook (SMOG), Automated
              Readability Index (ARI), and Linsear Write Formula (LWF). Mean
              readability scores were calculated with standard deviations and
              compared using unpaired t-tests. RESULTS: 32 webpages and seven
              EMR education documents were analyzed. Webpages were overall
              ``difficult to read'' compared to ``standard/average''
              readability EMR materials (mean FRE 50.5 $\pm$ 18.3 vs. 67.4
              $\pm$ 4.2, p = 0.023). All webpages were at or above high school
              reading level: mean FKGL 11.8 $\pm$ 4.4, GF 14.0 $\pm$ 3.9, CL
              9.5 $\pm$ 3.2, SMOG 11.0 $\pm$ 3.2, ARI 11.7 $\pm$ 5.1, and LWF
              14.9 $\pm$ 6.6. Webpages with highest reading levels were
              nutrition information and lowest were patient testimonials. EMR
              materials were sixth to ninth grade reading level: FKGL 6.2 $\pm$
              0.8, GF 9.3 $\pm$ 1.4, CL 9.7 $\pm$ 0.9, SMOG 7.1 $\pm$ 0.8, ARI
              6.1 $\pm$ 1.0, and LWF 5.9 $\pm$ 0.8. CONCLUSION: Surgeon curated
              bariatric surgery webpages have advanced reading levels above
              recommended thresholds compared to standardized PEM from an EMR.
              This readability gap may unintentionally contribute to barriers
              to surgery and affect postoperative outcomes. Streamlined efforts
              are needed to create materials that are easier to read and comply
              with recommendations.",
  journal  = "Surg. Endosc.",
  volume   =  37,
  number   =  8,
  pages    = "6519--6525",
  month    =  aug,
  year     =  2023,
  keywords = "Bariatric surgery; Health literacy; Obesity; Readability",
  language = "en"
}

@article{ZHANG2024121364,
title = {ROUGE-SEM: Better evaluation of summarization using ROUGE combined with semantics},
journal = {Expert Systems with Applications},
volume = {237},
pages = {121364},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.121364},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423018663},
author = {Ming Zhang and Chengzhang Li and Meilin Wan and Xuejun Zhang and Qingwei Zhao},
keywords = {Automatic summarization evaluation, Semantic similarity, Lexical similarity, Contrastive learning, Back-translation},
abstract = {With the development of pre-trained language models and large-scale datasets, automatic text summarization has attracted much attention from the community of natural language processing, but the progress of automatic summarization evaluation has stagnated. Although there have been efforts to improve automatic summarization evaluation, ROUGE has remained one of the most popular metrics for nearly 20 years due to its competitive evaluation performance. However, ROUGE is not perfect, there are studies have shown that it is suffering from inaccurate evaluation of abstractive summarization and limited diversity of generated summaries, both caused by lexical bias. To avoid the bias of lexical similarity, more and more meaningful embedding-based metrics have been proposed to evaluate summaries by measuring semantic similarity. Due to the challenge of accurately measuring semantic similarity, none of them can fully replace ROUGE as the default automatic evaluation toolkit for text summarization. To address the aforementioned problems, we propose a compromise evaluation framework (ROUGE-SEM) for improving ROUGE with semantic information, which compensates for the lack of semantic awareness through a semantic similarity module. According to the differences in semantic similarity and lexical similarity, summaries are classified into four categories for the first time, including good-summary, pearl-summary, glass-summary, and bad-summary. In particular, the back-translation technique is adopted to rewrite pearl-summary and glass-summary that are inaccurately evaluated by ROUGE to alleviate lexical bias. Through this pipeline framework, summaries are first classified by candidate summary classifier, then rewritten by categorized summary rewriter, and finally scored by rewritten summary scorer, which are efficiently evaluated in a manner consistent with human behavior. When measured using Pearson, Spearman, and Kendall rank coefficients, our proposal achieves comparable or higher correlations with human judgments than several state-of-the-art automatic summarization evaluation metrics in dimensions of coherence, consistency, fluency, and relevance. This also suggests that improving ROUGE with semantics is a promising direction for automatic summarization evaluation.}
}

@misc{ganesan2018rouge20updatedimproved,
      title={ROUGE 2.0: Updated and Improved Measures for Evaluation of Summarization Tasks}, 
      author={Kavita Ganesan},
      year={2018},
      eprint={1803.01937},
      archivePrefix={arXiv},
      primaryClass={cs.IR},
      url={https://arxiv.org/abs/1803.01937}, 
}

@ARTICLE{Fung2024-uh,
  title    = "Internet health resources on nocturnal enuresis: A readability,
              quality, and accuracy analysis",
  author   = "Fung, Adrian C H and Lee, Matthew H L and Leung, Jessie L and
              Chan, Ivy H Y and Wong, Kenneth K Y",
  abstract = "INTRODUCTION: Nocturnal enuresis is a common yet
              quality-of-life-limiting pediatric condition. There is an
              increasing trend for parents to obtain information on the
              disease's nature and treatment options via the internet. However,
              the quality of health-related information on the internet varies
              greatly and is largely uncontrolled and unregulated. With this
              study, a readability, quality, and accuracy evaluation of the
              health information regarding nocturnal enuresis is carried out.
              MATERIALS AND METHODS: A questionnaire was administered to
              parents and patients with nocturnal enuresis to determine their
              use of the internet to research their condition. The most common
              search terms were determined, and the first 30 websites returned
              by the most popular search engines were used to assess the
              quality of information about nocturnal enuresis. Each site was
              categorized by type and assessed for readability using the
              Gunning fog score, Simple Measure of Gobbledygook (SMOG) index,
              and Dale-Chall score; for quality using the DISCERN score; and
              for accuracy by comparison to the International Children's
              Continence Society guidelines by three experienced pediatric
              urologists and nephrologists. RESULTS: A total of 30 websites
              were assessed and classified into five categories: professional
              (n = 13), nonprofit (n = 8), commercial (n = 4), government (n =
              3), and other (n = 2). The information was considered difficult
              for the public to comprehend, with mean Gunning fog, SMOG index,
              and Dale-Chall scores of 12.1 $\pm$ 4.3, 14.1 $\pm$ 4.3, and 8.1
              $\pm$ 1.3, respectively. The mean summed DISCERN score was 41
              $\pm$ 11.6 out of 75. Only seven (23\%) websites were considered
              of good quality (DISCERN score > 50). The mean accuracy score of
              the websites was 3.2 $\pm$ 0.6 out of 5. Commercial websites were
              of the poorest quality and accuracy. Websites generally scored
              well in providing their aims and identifying treatment benefits
              and options, while they lacked references and information
              regarding treatment risks and mechanisms. CONCLUSION: Online
              information about nocturnal enuresis exists for parents; however,
              most websites are of suboptimal quality, readability, and
              accuracy. Pediatric surgeons should be aware of parents'
              health-information-seeking behavior and be proactive in guiding
              parents to identify high-quality resources.",
  journal  = "Eur. J. Pediatr. Surg.",
  volume   =  34,
  number   =  1,
  pages    = "84--90",
  month    =  feb,
  year     =  2024,
  language = "en"
}
