Give me the complete methodology for the given paper in clear language.

The following is an example of what you must do. Follow the style and structure of the sample. If necessary you can change the subsections. You can remove sections if they are not relevant to this specific paper, or add subsections if it makes sense.
Here it is,

"""
### Methodology

#### Datasets

The models are trained and evaluated on two key biomedical datasets, **eLife** and **Public Library of Science (PLOS)**, which provide full-text biomedical articles along with their corresponding lay summaries. These datasets focus on creating summaries for laypeople. The **eLife** dataset features lay summaries crafted by expert editors, while **PLOS** includes lay summaries written by the article authors. Both datasets cover a broad range of topics within life sciences and medicine. The average token lengths are 13,000 for eLife articles and 9,000 for PLOS articles, respectively​.

#### Evaluation Metrics

The models performance was evaluated using various automatic metrics related to relevance, readability, and factuality. Relevance is measured through **Recall-Oriented Understudy for Gisting Evaluation or ROUGE (1, 2, and L)** and **BERTScore**. Readability metrics include the **FleschKincaid Grade Level (FKGL)**, **Dale-Chall Readability Score (DCRS)**, **Coleman-Liau Index (CLI)**, and **Learnable Evaluation Metric for Simplification (LENS)**. Notably, lower FKGL, DCRS, and CLI scores signify improved readability. Factuality evaluation incorporates **AlignScore** and **SummaC**.
#### Preprocessing

Before training the models, preprocessing steps were applied to reduce the length of input articles. This involved two key methods:

1. **Section Reordering**: Articles were restructured based on relevance to summary content, with sections ordered as abstract, background, conclusions, results, and methods. This was based on cosine similarity between sections and lay summaries​.
2. **Unsupervised Extractive Summarization**: Two extractive summarization approaches were used to condense articles:
    - **TextRank**: A graph-based ranking algorithm was used to identify key sentences within the text.
    - **BERT-based Clustering**: PubMedBERT embeddings were applied to cluster sentences by similarity, selecting those closest to the cluster centroids as the most relevant​.

#### Model 1: Longformer Encoder-Decoder (LED)

The **LED model** is a transformer-based architecture designed for long document summarization. Given the article length, the authors used the **LED-base model** with a maximum input length of 8,192 tokens, fine-tuning it on reordered article sections and the extracted summaries. For knowledge augmentation, **dense retrieval (DPR)** was employed to pull relevant passages from a Wikipedia corpus, which were then combined with the extracted summaries to enrich the input for summarization​. The model was optimized using **cross-entropy loss** to enhance its performance.
#### Model 2: GPT-3.5

The **GPT-3.5 model** was fine-tuned using an extract-then-summarize approach. The fine-tuning process involved feeding **TextRank-extracted sentences** into GPT-3.5 to generate summaries. Due to memory constraints, only 40 sentences from each article were included in the input. The model was fine-tuned separately for each dataset, with the number of training examples ranging from 100 to 400 articles​. The fine-tuning utilized a **contrastive loss** function, which helped improve the model’s ability to align extracted sentences with lay summaries.

#### Postprocessing

To improve factual accuracy, **GPT-4** was used to refine the generated lay summaries. This involved prompting GPT-4 with both the article’s abstract and the generated summary, instructing it to enhance factual consistency by aligning the summary more closely with the abstract.

Despite this attempt, the results showed minimal improvement in factuality (measured by **AlignScore**) and no significant gains in readability or relevance. In fact, readability scores slightly declined after using GPT-4 for postprocessing.

#### Training and Technology

Both models were trained using a **single NVIDIA Tesla V100 GPU** with 32 GB memory. The LED model was trained for **one epoch** with a batch size of 4, using **Adam optimization**. The **GPT-3.5** fine-tuning process leveraged the **OpenAI API** for model adjustments​.
"""

Do not hallucinate, do not add information from the sample.

The sections "Datasets", "Evaluation Metrics", "Models", "Preprocessing", and "Training & Technology" are required. You should only create subsections if it makes sense. Also, if the methodology employed for one model is different than another, you should create sections "Models 1: xxx", "Model 2: yyy" instead of the "Models" section. If the models are trained the same way, if the pipeline is the same a single "Models" section is enough. Postprocessing should be included if the papers mentions about any postprocessing.
